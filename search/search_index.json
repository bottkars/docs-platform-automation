{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Platform Automation for PCF provides the building blocks to create a repeatable and reusable automated pipeline(s) for upgrading and installing PCF foundations. We also provide instructions on how to use these building blocks for various workflows. In this introduction, we'll provide a high-level overview of Platform Automation - to dive-deeper, check out the references section. About Uses om , (and by extension, the Ops Manager API) to enable command-line interaction with Ops Manager ( Understanding the Ops Manager Interface ) Includes a documented reference pipeline showing one possible configuration to use tasks. When automating your platform, there are some manual steps that you will need to take to optimize for automation - we will call these steps out so that these are clear to you. Comes bundled with Concourse tasks that demonstrate how to use these tasks in a containerized Continuous Integration (CI) system. Platform Automation for PCF tasks are: Legible: They use human-readable YAML config files which can be edited and managed Modular: Each task has defined inputs and outputs that perform granular actions Built for Automation: Tasks are idempotent, so re-running them in a CI won't break builds Not Comprehensive: Workflows that use Platform Automation for PCF may also contain om commands, custom tasks, and even interactions with the Ops Manager user interface. Platform Automation for PCF is a set of tools to use alongside other tools, rather than a comprehensive solution. Assumes you have the following before using our tasks: a paved IaaS, a deployed Concourse, a git compliant source code control system, credential manager, and a S3-compliant object store. For more information on how to implement these, check out the Before you Begin section. The Task Reference topic discusses these example tasks further. Transitioning from PCF Pipelines Platform Automation takes a different approach than PCF Pipelines, one example is that Platform Automation allows you to perform installs and upgrades in the same pipeline. We recommend trying out Platform Automation to get a sense of the features and how they differ to understand the best transition method for your environment and needs. Platform Automation and Upgrading PCF Successful platform engineering teams know that a platform team that\u2019s always up to date is critical for their business. If they don\u2019t stay up to date, they miss out on the latest platform features and the services that Pivotal delivers, which means their development teams miss out too. By not keeping up to date, platforms could encounter security risks or even application failures. Pivotal offers regular updates for PCF, which ensures our customers have access to the latest security patches and new features. For example, Pivotal releases security patches every six days on average. So how can a platform engineering team simplify the platform upgrade process? Small and Continuous Upgrades Adopting the best practice of small and constant platform updates is one of the best ways to simplify the platform upgrade process. This behavior can significantly reduce risk, increase stability with faster troubleshooting, and overall reduce the effort of upgrading. This also creates a culture of continuous iteration and improves feedback loops with the platform teams and the developers - building trust across the organization. A good place to start is by consuming every patch. How Platform Automation for PCF can help with small and continuous upgrades With Platform Automation for PCF, platform teams have the tools to create an automated perpetual upgrade machine that can continuously take the latest updates when new software is available - including PAS, PKS, Ops Manager, stemcells, products and services. In addition, Platform Automation for PCF helps with: Externalized configuration: helps manage multiple foundations and reduces configuration drift by tracking changes through source control The ability to create pipelines that handle installs and upgrades: helps streamline workflows Platform Automation and Ops Manager The following table compares how Ops Manager and Platform Automation for PCF might run a typical sequence of PCF operations: Ops Manager Platform Automation for PCF When to Use First install and minor upgrades Config changes and patch upgrades 1. Create Ops Manager VM Manually prepare IaaS and create Ops Manager VM create-vm 2. Configure Who Can Run Ops Manually configure internal UAA or external identity provider configure-authentication or configure-saml-authentication 3. Configure BOSH Manually configure BOSH Director configure-director with settings saved from BOSH Director with same version 4. Add Products Click Import a Product to upload file, then + to add tile to Installation Dashboard upload-and-stage-product 5. Configure Products Manually configure products configure-product with settings saved from tiles with same version 6. Deploy Products Click Apply Changes apply-changes 7. Upgrade Manually export existing Ops Manager settings, power off the VM, then create a new, updated Ops Manager VM export-installation then upgrade-opsman","title":"Introduction"},{"location":"index.html#about","text":"Uses om , (and by extension, the Ops Manager API) to enable command-line interaction with Ops Manager ( Understanding the Ops Manager Interface ) Includes a documented reference pipeline showing one possible configuration to use tasks. When automating your platform, there are some manual steps that you will need to take to optimize for automation - we will call these steps out so that these are clear to you. Comes bundled with Concourse tasks that demonstrate how to use these tasks in a containerized Continuous Integration (CI) system. Platform Automation for PCF tasks are: Legible: They use human-readable YAML config files which can be edited and managed Modular: Each task has defined inputs and outputs that perform granular actions Built for Automation: Tasks are idempotent, so re-running them in a CI won't break builds Not Comprehensive: Workflows that use Platform Automation for PCF may also contain om commands, custom tasks, and even interactions with the Ops Manager user interface. Platform Automation for PCF is a set of tools to use alongside other tools, rather than a comprehensive solution. Assumes you have the following before using our tasks: a paved IaaS, a deployed Concourse, a git compliant source code control system, credential manager, and a S3-compliant object store. For more information on how to implement these, check out the Before you Begin section. The Task Reference topic discusses these example tasks further. Transitioning from PCF Pipelines Platform Automation takes a different approach than PCF Pipelines, one example is that Platform Automation allows you to perform installs and upgrades in the same pipeline. We recommend trying out Platform Automation to get a sense of the features and how they differ to understand the best transition method for your environment and needs.","title":"About"},{"location":"index.html#platform-automation-and-upgrading-pcf","text":"Successful platform engineering teams know that a platform team that\u2019s always up to date is critical for their business. If they don\u2019t stay up to date, they miss out on the latest platform features and the services that Pivotal delivers, which means their development teams miss out too. By not keeping up to date, platforms could encounter security risks or even application failures. Pivotal offers regular updates for PCF, which ensures our customers have access to the latest security patches and new features. For example, Pivotal releases security patches every six days on average. So how can a platform engineering team simplify the platform upgrade process? Small and Continuous Upgrades Adopting the best practice of small and constant platform updates is one of the best ways to simplify the platform upgrade process. This behavior can significantly reduce risk, increase stability with faster troubleshooting, and overall reduce the effort of upgrading. This also creates a culture of continuous iteration and improves feedback loops with the platform teams and the developers - building trust across the organization. A good place to start is by consuming every patch. How Platform Automation for PCF can help with small and continuous upgrades With Platform Automation for PCF, platform teams have the tools to create an automated perpetual upgrade machine that can continuously take the latest updates when new software is available - including PAS, PKS, Ops Manager, stemcells, products and services. In addition, Platform Automation for PCF helps with: Externalized configuration: helps manage multiple foundations and reduces configuration drift by tracking changes through source control The ability to create pipelines that handle installs and upgrades: helps streamline workflows","title":"Platform Automation and Upgrading PCF"},{"location":"index.html#platform-automation-and-ops-manager","text":"The following table compares how Ops Manager and Platform Automation for PCF might run a typical sequence of PCF operations: Ops Manager Platform Automation for PCF When to Use First install and minor upgrades Config changes and patch upgrades 1. Create Ops Manager VM Manually prepare IaaS and create Ops Manager VM create-vm 2. Configure Who Can Run Ops Manually configure internal UAA or external identity provider configure-authentication or configure-saml-authentication 3. Configure BOSH Manually configure BOSH Director configure-director with settings saved from BOSH Director with same version 4. Add Products Click Import a Product to upload file, then + to add tile to Installation Dashboard upload-and-stage-product 5. Configure Products Manually configure products configure-product with settings saved from tiles with same version 6. Deploy Products Click Apply Changes apply-changes 7. Upgrade Manually export existing Ops Manager settings, power off the VM, then create a new, updated Ops Manager VM export-installation then upgrade-opsman","title":"Platform Automation and Ops Manager"},{"location":"compatibility-and-versioning.html","text":"This topic describes Platform Automation for PCF dependencies and semantic versioning. External Dependencies We have tested Platform Automation with these dependencies. Platform Automation Concourse Ops Manager Pivnet Resource v3.0.0 v3.14.1+ v2.3+ v0.31.15 v2.2.0-beta v3.14.1+ v2.1-v2.5 v0.31.15 v2.0-beta - v2.1-beta v3.14.1+ v2.1-v2.5* v0.31.15 Ops Manager 2.5 The filename for the artifact downloaded from Ops Manager is changed! If your resources or pipelines have a regex for the Ops Manager filename, you may be affected. (Please see Ops Manager's official notice for more information) Semantic Versioning This product uses semantic versioning 2.0.0 to describe the impact of changes to our concourse tasks. In order to take advantage of semantic versioning, we must declare an API. The following are considered part of our API: Our concourse tasks': inputs and outputs (including the format/required information in config files) specified parameters intended and specified functionality These are all documented for each task within the task files themselves. Additionally, the minimum compatible version of Concourse and Ops Manager are part of the API, and are specified below . The following are NOT covered: the om command line tool the p-automator command line tool the dependencies on the image intended to be used with our tasks non-specified parameters (for instance, any env var used by a CLI we call but not specified as a parameter on the task) properties specific to particular product or ops manager versions in config files (which are governed by the product being configured, not our tooling) In general, if we make any change that we anticipate could not be consumed automatically, without manual changes, by all users of our Concourse tasks, we consider it a breaking change, and increment the major version accordingly. This assumes that the required image can be made automatically available; each version of our tasks is designed for and tested with only the version of the image that shipped with it. If we accidentally violate our semver, we will publish an additional version addressing the problem. In some cases, that may mean releasing ths same software with a corrected version, and shipping a new patch version identical to the version prior to the violation. In others, it may mean releasing an additional patch version which reverts an unintentional breaking change. This should make it safe to automatically consume our release. Patch releases should be very safe to automatically update to. Minor versions should be safe, but it can be more difficult to anticipate the effect of new features, so this is slightly riskier. Major versions should be expected to break for at least some users when consumed automatically. Automatic consumption of major versions should be limited to test/staging environments intended to endure and detect such breakage.","title":"Compatibility and Versioning"},{"location":"compatibility-and-versioning.html#external-dependencies","text":"We have tested Platform Automation with these dependencies. Platform Automation Concourse Ops Manager Pivnet Resource v3.0.0 v3.14.1+ v2.3+ v0.31.15 v2.2.0-beta v3.14.1+ v2.1-v2.5 v0.31.15 v2.0-beta - v2.1-beta v3.14.1+ v2.1-v2.5* v0.31.15 Ops Manager 2.5 The filename for the artifact downloaded from Ops Manager is changed! If your resources or pipelines have a regex for the Ops Manager filename, you may be affected. (Please see Ops Manager's official notice for more information)","title":"External Dependencies"},{"location":"compatibility-and-versioning.html#semantic-versioning","text":"This product uses semantic versioning 2.0.0 to describe the impact of changes to our concourse tasks. In order to take advantage of semantic versioning, we must declare an API. The following are considered part of our API: Our concourse tasks': inputs and outputs (including the format/required information in config files) specified parameters intended and specified functionality These are all documented for each task within the task files themselves. Additionally, the minimum compatible version of Concourse and Ops Manager are part of the API, and are specified below . The following are NOT covered: the om command line tool the p-automator command line tool the dependencies on the image intended to be used with our tasks non-specified parameters (for instance, any env var used by a CLI we call but not specified as a parameter on the task) properties specific to particular product or ops manager versions in config files (which are governed by the product being configured, not our tooling) In general, if we make any change that we anticipate could not be consumed automatically, without manual changes, by all users of our Concourse tasks, we consider it a breaking change, and increment the major version accordingly. This assumes that the required image can be made automatically available; each version of our tasks is designed for and tested with only the version of the image that shipped with it. If we accidentally violate our semver, we will publish an additional version addressing the problem. In some cases, that may mean releasing ths same software with a corrected version, and shipping a new patch version identical to the version prior to the violation. In others, it may mean releasing an additional patch version which reverts an unintentional breaking change. This should make it safe to automatically consume our release. Patch releases should be very safe to automatically update to. Minor versions should be safe, but it can be more difficult to anticipate the effect of new features, so this is slightly riskier. Major versions should be expected to break for at least some users when consumed automatically. Automatic consumption of major versions should be limited to test/staging environments intended to endure and detect such breakage.","title":"Semantic Versioning"},{"location":"downloading-and-testing.html","text":"The following describes the procedure for downloading, installing and testing the setup of Platform Automation for PCF. Prerequisites You'll need the following in order to setup Platform Automation for PCF. Deployed Concourse . Platform Automation for PCF is based on Concourse CI. We recommend that you have some familiarity with Concourse before getting started. If you are new to Concourse, Concourse CI Tutorials would be a good place to start. Persisted datastore that can be accessed by Concourse resource (e.g. s3, gcs, minio) Pivnet access to Platform Automation for PCF Download Download the latest Platform Automation for PCF from Pivnet. This includes: Concourse Tasks Docker Image for Concourse Tasks Store the platform-automation-image-*.tgz in a blobstore that can be accessed via a Concourse pipeline. Store the platform-automation-tasks-*.zip in a blobstore that can be accessed via a Concourse pipeline. Testing Setup Next we'll create a test pipeline to see if the assets can be accessed correctly. This pipeline runs a test task, which ensures that all the parts work correctly. Info The pipeline can use any blobstore. We choose S3 because the resource natively supported by Concourse. The S3 Concourse resource also supports S3-compatible blobstores (e.g. minio). See S3 Resource for more information. If you want to use other blobstore, you need to provide a custom resource type . In order to test the setup, fill in the S3 resource credentials and set the below pipeline on your Concourse instance. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 resources : - name : platform-automation-tasks-s3 type : s3 source : access_key_id : ((access_key_id)) secret_access_key : ((secret_access_key)) region_name : ((region)) bucket : ((bucket)) regexp : platform-automation-tasks-(.*).zip - name : platform-automation-image-s3 type : s3 source : access_key_id : ((access_key_id)) secret_access_key : ((secret_access_key)) region_name : ((region)) bucket : ((bucket)) regexp : platform-automation-image-(.*).tgz jobs : - name : test-resources plan : - aggregate : - get : platform-automation-tasks-s3 params : unpack : true - get : platform-automation-image-s3 params : unpack : true - task : test-resources image : platform-automation-image-s3 file : platform-automation-tasks-s3/tasks/test.yml","title":"Downloading and Testing"},{"location":"downloading-and-testing.html#prerequisites","text":"You'll need the following in order to setup Platform Automation for PCF. Deployed Concourse . Platform Automation for PCF is based on Concourse CI. We recommend that you have some familiarity with Concourse before getting started. If you are new to Concourse, Concourse CI Tutorials would be a good place to start. Persisted datastore that can be accessed by Concourse resource (e.g. s3, gcs, minio) Pivnet access to Platform Automation for PCF","title":"Prerequisites"},{"location":"downloading-and-testing.html#download","text":"Download the latest Platform Automation for PCF from Pivnet. This includes: Concourse Tasks Docker Image for Concourse Tasks Store the platform-automation-image-*.tgz in a blobstore that can be accessed via a Concourse pipeline. Store the platform-automation-tasks-*.zip in a blobstore that can be accessed via a Concourse pipeline.","title":"Download"},{"location":"downloading-and-testing.html#testing-setup","text":"Next we'll create a test pipeline to see if the assets can be accessed correctly. This pipeline runs a test task, which ensures that all the parts work correctly. Info The pipeline can use any blobstore. We choose S3 because the resource natively supported by Concourse. The S3 Concourse resource also supports S3-compatible blobstores (e.g. minio). See S3 Resource for more information. If you want to use other blobstore, you need to provide a custom resource type . In order to test the setup, fill in the S3 resource credentials and set the below pipeline on your Concourse instance. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 resources : - name : platform-automation-tasks-s3 type : s3 source : access_key_id : ((access_key_id)) secret_access_key : ((secret_access_key)) region_name : ((region)) bucket : ((bucket)) regexp : platform-automation-tasks-(.*).zip - name : platform-automation-image-s3 type : s3 source : access_key_id : ((access_key_id)) secret_access_key : ((secret_access_key)) region_name : ((region)) bucket : ((bucket)) regexp : platform-automation-image-(.*).tgz jobs : - name : test-resources plan : - aggregate : - get : platform-automation-tasks-s3 params : unpack : true - get : platform-automation-image-s3 params : unpack : true - task : test-resources image : platform-automation-image-s3 file : platform-automation-tasks-s3/tasks/test.yml","title":"Testing Setup"},{"location":"release-notes.html","text":"Ops Manager 2.5 The filename for the artifact downloaded from Ops Manager is changed! If your resources or pipelines have a regex for the Ops Manager filename, you may be affected. (Please see Ops Manager's official notice for more information) Azure Updating to 2.5 Ops Manager will be removing the necessity to provide availability zones for azure. If your director.yml (see staged-director-config ) has a block like the following in the networks section: 1 2 availability_zone_names : - \"null\" your deployment will have the following error: 1 { \"errors\" :[ \"Availability zones cannot find availability zone with name null\" ]} To fix this error, please remove the availability_zone_names section from your azure config, or re-run staged-director-config to update your director.yml . v3.0.1 Release Date Something sometime Bug Fixes upgrade-opsman would incorrectly parse meta information from the download-product prefixing if using S3. This lead to some unexpected upgrading behavior. Now, the task will correctly upgrade if the semver is higher. v3.0.0 Release Date SomeDayOfTheWeek, Month, Day, Year Breaking Changes om will now follow conventional Semantic Versioning, with breaking changes in major bumps, non-breaking changes for minor bumps, and bug fixes for patches. The credhub-interpolate task can have multiple interpolation paths. The INTERPOLATION_PATH param is now plural: INTERPOLATION_PATHS . IF you are using a custom INTERPOLATION_PATH for credhub-interpolate , you will need to update your pipeline.yml to this new param. As an example, if your credhub-interpolate job is defined as so: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # OLD pipeline.yml PRIOR TO 3.0.0 RELEASE - name : example-credhub-interpolate plan : - get : platform-automation-tasks - get : platform-automation-image - get : config - task : credhub-interpolate image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml input_mapping : files : config params : # all required CREDHUB_CA_CERT : ((credhub_ca_cert)) CREDHUB_CLIENT : ((credhub_client)) CREDHUB_SECRET : ((credhub_secret)) CREDHUB_SERVER : ((credhub_server)) PREFIX : /private-foundation INTERPOLATION_PATH : foundation/config-path SKIP_MISSING : true it should now look like 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # NEW pipeline.yml FOR 3.0.0 RELEASE - name : example-credhub-interpolate plan : - get : platform-automation-tasks - get : platform-automation-image - get : config - task : credhub-interpolate image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml input_mapping : files : config params : # all required CREDHUB_CA_CERT : ((credhub_ca_cert)) CREDHUB_CLIENT : ((credhub_client)) CREDHUB_SECRET : ((credhub_secret)) CREDHUB_SERVER : ((credhub_server)) PREFIX : /private-foundation INTERPOLATION_PATHS : foundation/config-path SKIP_MISSING : true the upload-product option --sha256 has been changed to --shasum . IF you are using the --config flag in upload-product , your config file will need to update from: 1 2 3 # OLD upload-product-config.yml PRIOR TO 3.0.0 RELEASE product-version : 1.2.3-build.4 sha256 : 6daededd8fb4c341d0cd437a to: 1 2 3 # NEW upload-product-config.yml FOR 3.0.0 RELEASE product-version : 1.2.3-build.4 shasum : 6daededd8fb4c341d0cd437a # NOTE the name of this value is changed This change was added to future-proof the param name for when sha256 is no longer the de facto way of defining shasums. What's New The new command assign-multi-stemcell assigns multiple stemcells to a provided product. This feature is only available in OpsMan 2.6+. download-product ensures sha sum checking when downloading the file from Pivotal Network. download-product can now disable ssl validation when connecting to Pivotal Network. This helps with environments with SSL and proxying issues. Add pivnet-disable-ssl: true in your download-product-config to use this feature. On GCP , if you did not assign a public IP, Google would assign one for you. This has been changed to only assign a public IP if defined in your opsman.yml . On Azure , if you did not assign a public IP, Azure would assign one for you. This has been changed to only assign a public IP if defined in your opsman.yml . om interpolate (example in the test task ) now supports the ability to accept partial vars files. This is added support for users who may also be using credhub-interpolate or who want to mix interpolation methods. To make use of this feature, include the --skip-missing flag. credhub-interpolate now supports the SKIP_MISSING parameter. For more information on how to use this feature and if it fits for your foundation(s), see the Secrets Handling section. the reference pipeline has been updated to give an example of credhub-interpolate in practice. For more information about credhub, see Secrets Handling om now has support for config-template (a Platform Automation encouraged replacement of tile-config-generator ). This is a experimental command that can only be run currently using docker run . For more information and instruction on how to use config-template , please see Creating a Product Config File . upload-stemcell now supports the ability to include a config file. This allows you to define an expected shasum that will validate the calculated shasum of the provided stemcell uploaded in the task. This was added to give feature parity with upload-product Azure now allows NSG(network security group) to be optional. This change was made because NSGs can be assigned at the subnet level rather than just the VM level. This param is also not required by the Azure CLI . Platform Automation now reflects this. staged-director-config now supports returning multiple IaaS configurations. iaas-configurations is a top level key returned in Ops Manager 2.2+. If using an Ops Manager 2.1 or earlier, iaas_configuration will continue to be a key nested under properties-configuration . configure-director now supports setting multiple IaaS configurations. If using this feature, be sure to use the top-level iaas-configurations key, rather than the nested properties-configuration.iaas_configuration key. If using a single IaaS, properties-configuration.iaas_configuration is still supported, but the new iaas_configurations top-level key is recommended. 1 2 3 4 5 6 7 8 9 # Configuration for 2.2+ iaas-configurations : - additional_cloud_properties : {} name : ((iaas-configurations_0_name)) - additional_cloud_properties : {} name : ((iaas-configurations_1_name)) ... networks-configuration : ... properties-configuration : ... 1 2 3 4 5 6 7 8 9 # Configuration 2.1 and earlier networks-configuration : ... properties-configuration : director_configuration : ... iaas_configuration : additional_cloud_properties : {} name : ((iaas-configurations_0_name)) ... security_configuration : ... Bug Fixes OpenStack would sometimes be unable to associate the public IP when creating the VM, because it was waiting for the VM to come up. The --wait flag has been added to validate that the VM creation is complete before more work is done to the VM. credhub-interpolate now accepts multiple files for the INTERPOLATION_PATHS . CVE update to container image. Resolves USN-3911-1 (related to vulnerabilities with libmagic1 . While none of our code directly used these, they are present on the image.) Improved error messaging for vSphere VM creation if neither ssh-password or ssh-public-key are set. One or the other is required to create a VM.","title":"Release Notes"},{"location":"release-notes.html#v301","text":"Release Date Something sometime","title":"v3.0.1"},{"location":"release-notes.html#bug-fixes","text":"upgrade-opsman would incorrectly parse meta information from the download-product prefixing if using S3. This lead to some unexpected upgrading behavior. Now, the task will correctly upgrade if the semver is higher.","title":"Bug Fixes"},{"location":"release-notes.html#v300","text":"Release Date SomeDayOfTheWeek, Month, Day, Year","title":"v3.0.0"},{"location":"release-notes.html#breaking-changes","text":"om will now follow conventional Semantic Versioning, with breaking changes in major bumps, non-breaking changes for minor bumps, and bug fixes for patches. The credhub-interpolate task can have multiple interpolation paths. The INTERPOLATION_PATH param is now plural: INTERPOLATION_PATHS . IF you are using a custom INTERPOLATION_PATH for credhub-interpolate , you will need to update your pipeline.yml to this new param. As an example, if your credhub-interpolate job is defined as so: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # OLD pipeline.yml PRIOR TO 3.0.0 RELEASE - name : example-credhub-interpolate plan : - get : platform-automation-tasks - get : platform-automation-image - get : config - task : credhub-interpolate image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml input_mapping : files : config params : # all required CREDHUB_CA_CERT : ((credhub_ca_cert)) CREDHUB_CLIENT : ((credhub_client)) CREDHUB_SECRET : ((credhub_secret)) CREDHUB_SERVER : ((credhub_server)) PREFIX : /private-foundation INTERPOLATION_PATH : foundation/config-path SKIP_MISSING : true it should now look like 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # NEW pipeline.yml FOR 3.0.0 RELEASE - name : example-credhub-interpolate plan : - get : platform-automation-tasks - get : platform-automation-image - get : config - task : credhub-interpolate image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml input_mapping : files : config params : # all required CREDHUB_CA_CERT : ((credhub_ca_cert)) CREDHUB_CLIENT : ((credhub_client)) CREDHUB_SECRET : ((credhub_secret)) CREDHUB_SERVER : ((credhub_server)) PREFIX : /private-foundation INTERPOLATION_PATHS : foundation/config-path SKIP_MISSING : true the upload-product option --sha256 has been changed to --shasum . IF you are using the --config flag in upload-product , your config file will need to update from: 1 2 3 # OLD upload-product-config.yml PRIOR TO 3.0.0 RELEASE product-version : 1.2.3-build.4 sha256 : 6daededd8fb4c341d0cd437a to: 1 2 3 # NEW upload-product-config.yml FOR 3.0.0 RELEASE product-version : 1.2.3-build.4 shasum : 6daededd8fb4c341d0cd437a # NOTE the name of this value is changed This change was added to future-proof the param name for when sha256 is no longer the de facto way of defining shasums.","title":"Breaking Changes"},{"location":"release-notes.html#whats-new","text":"The new command assign-multi-stemcell assigns multiple stemcells to a provided product. This feature is only available in OpsMan 2.6+. download-product ensures sha sum checking when downloading the file from Pivotal Network. download-product can now disable ssl validation when connecting to Pivotal Network. This helps with environments with SSL and proxying issues. Add pivnet-disable-ssl: true in your download-product-config to use this feature. On GCP , if you did not assign a public IP, Google would assign one for you. This has been changed to only assign a public IP if defined in your opsman.yml . On Azure , if you did not assign a public IP, Azure would assign one for you. This has been changed to only assign a public IP if defined in your opsman.yml . om interpolate (example in the test task ) now supports the ability to accept partial vars files. This is added support for users who may also be using credhub-interpolate or who want to mix interpolation methods. To make use of this feature, include the --skip-missing flag. credhub-interpolate now supports the SKIP_MISSING parameter. For more information on how to use this feature and if it fits for your foundation(s), see the Secrets Handling section. the reference pipeline has been updated to give an example of credhub-interpolate in practice. For more information about credhub, see Secrets Handling om now has support for config-template (a Platform Automation encouraged replacement of tile-config-generator ). This is a experimental command that can only be run currently using docker run . For more information and instruction on how to use config-template , please see Creating a Product Config File . upload-stemcell now supports the ability to include a config file. This allows you to define an expected shasum that will validate the calculated shasum of the provided stemcell uploaded in the task. This was added to give feature parity with upload-product Azure now allows NSG(network security group) to be optional. This change was made because NSGs can be assigned at the subnet level rather than just the VM level. This param is also not required by the Azure CLI . Platform Automation now reflects this. staged-director-config now supports returning multiple IaaS configurations. iaas-configurations is a top level key returned in Ops Manager 2.2+. If using an Ops Manager 2.1 or earlier, iaas_configuration will continue to be a key nested under properties-configuration . configure-director now supports setting multiple IaaS configurations. If using this feature, be sure to use the top-level iaas-configurations key, rather than the nested properties-configuration.iaas_configuration key. If using a single IaaS, properties-configuration.iaas_configuration is still supported, but the new iaas_configurations top-level key is recommended. 1 2 3 4 5 6 7 8 9 # Configuration for 2.2+ iaas-configurations : - additional_cloud_properties : {} name : ((iaas-configurations_0_name)) - additional_cloud_properties : {} name : ((iaas-configurations_1_name)) ... networks-configuration : ... properties-configuration : ... 1 2 3 4 5 6 7 8 9 # Configuration 2.1 and earlier networks-configuration : ... properties-configuration : director_configuration : ... iaas_configuration : additional_cloud_properties : {} name : ((iaas-configurations_0_name)) ... security_configuration : ...","title":"What's New"},{"location":"release-notes.html#bug-fixes_1","text":"OpenStack would sometimes be unable to associate the public IP when creating the VM, because it was waiting for the VM to come up. The --wait flag has been added to validate that the VM creation is complete before more work is done to the VM. credhub-interpolate now accepts multiple files for the INTERPOLATION_PATHS . CVE update to container image. Resolves USN-3911-1 (related to vulnerabilities with libmagic1 . While none of our code directly used these, they are present on the image.) Improved error messaging for vSphere VM creation if neither ssh-password or ssh-public-key are set. One or the other is required to create a VM.","title":"Bug Fixes"},{"location":"report-an-issue.html","text":"To report an issue, reach out to your primary Pivotal contact or Pivotal Support .","title":"Report an Issue"},{"location":"upgrade.html","text":"This topic provides a high level overview of upgrading an Ops Manager using Platform Automation, including command requirements and common version check and IaaS CLI errors. It's important to note when upgrading your Ops Manager: always perform an export installation persist that exported installation installation is separate from upgrade an initial installation is done, which maintains state Warning It is recommended to persist the zip file exported from export-installation to an external file store (eg S3) on a regular basis. The exported installation can restore the Ops Manager to a working state if it is non-functional. Upgrade Flowchart The upgrade-opsman task follows the flow based on state of an Ops Manager VM. This flowchart gives a high level overview of how the task makes decisions for an upgrade. graph TD; versionChk[Is the existing Ops Manager version less than the Ops Manager image?]; versionError[Version Check Error]; delete[Delete the existing Ops Manager VM]; create[Create a new Ops Manager VM]; iaasErr[IAAS CLI error]; import[Import the provided Installation]; iaasErr2[IAAS CLI error]; versionChk -- No --> versionError; versionChk -- Yes --> delete; delete -- Success --> create; delete -- Failure --> iaasErr; create -- Success --> import ; create -- Failure --> iaasErr2; On successive invocations of the task, it will offer different behaviour of the previous run. This aids in recovering from failures (ie: from an IAAS) that occur. Command Requirements The upgrade-opsman task will delete the previous VM, create a new VM, and import a previous installation. It requires the following to perform this operations: a valid state file from the currently deployed Ops Manager a valid image file for the new Ops Manager to install a [configuration file][opsman-configuration] for IAAS specific details an exported installation from a currently deployed Ops Manager the auth file for a currently deployed Ops Manager Troubleshooting When you are upgrading your Ops Manager you may get version check or IaaS CLI errors. For information about troubleshooting these errors, see Version Check Errors and IaaS CLI Errors below. Version Check Errors 1) Downgrading is not supported by Ops Manager (Manual Intervention Required) Ops Manager does not support downgrading to a lower version. SOLUTION: Try the upgrade again with a newer version of Ops Manager. 2) Could not authenticate with Ops Manager (Manual Intervention Required) Credentials provided in the auth file do not match the credentials of an already deployed Ops Manager. SOLUTION: To change the credentials when upgrading an Ops Manager, you must update the password in your Account Settings. Then, you will need to update the following two files with the changes: auth.yml env.yml 3) The Ops Manager API is inaccessible (Recoverable) The task could not communicate with Ops Manager. SOLUTION: Rerun the upgrade-opsman task. The task will assume that the Ops Manager VM is not created, and will run the create-vm and import-installation tasks. IAAS CLI Errors 1) When the CLI for a supported IAAS fails for any reason (i.e., bad network, outage, etc) we treat this as an IAAS CLI error. The following tasks can return an error from the IAAS's CLI: delete-vm , create-vm SOLUTION: The specific error will be returned as output, but most errors can simply be fixed by re-running the upgrade-opsman task.","title":"About Upgrading Ops Manager"},{"location":"upgrade.html#upgrade-flowchart","text":"The upgrade-opsman task follows the flow based on state of an Ops Manager VM. This flowchart gives a high level overview of how the task makes decisions for an upgrade. graph TD; versionChk[Is the existing Ops Manager version less than the Ops Manager image?]; versionError[Version Check Error]; delete[Delete the existing Ops Manager VM]; create[Create a new Ops Manager VM]; iaasErr[IAAS CLI error]; import[Import the provided Installation]; iaasErr2[IAAS CLI error]; versionChk -- No --> versionError; versionChk -- Yes --> delete; delete -- Success --> create; delete -- Failure --> iaasErr; create -- Success --> import ; create -- Failure --> iaasErr2; On successive invocations of the task, it will offer different behaviour of the previous run. This aids in recovering from failures (ie: from an IAAS) that occur.","title":"Upgrade Flowchart"},{"location":"upgrade.html#command-requirements","text":"The upgrade-opsman task will delete the previous VM, create a new VM, and import a previous installation. It requires the following to perform this operations: a valid state file from the currently deployed Ops Manager a valid image file for the new Ops Manager to install a [configuration file][opsman-configuration] for IAAS specific details an exported installation from a currently deployed Ops Manager the auth file for a currently deployed Ops Manager","title":"Command Requirements"},{"location":"upgrade.html#troubleshooting","text":"When you are upgrading your Ops Manager you may get version check or IaaS CLI errors. For information about troubleshooting these errors, see Version Check Errors and IaaS CLI Errors below.","title":"Troubleshooting"},{"location":"upgrade.html#version-check-errors","text":"1) Downgrading is not supported by Ops Manager (Manual Intervention Required) Ops Manager does not support downgrading to a lower version. SOLUTION: Try the upgrade again with a newer version of Ops Manager. 2) Could not authenticate with Ops Manager (Manual Intervention Required) Credentials provided in the auth file do not match the credentials of an already deployed Ops Manager. SOLUTION: To change the credentials when upgrading an Ops Manager, you must update the password in your Account Settings. Then, you will need to update the following two files with the changes: auth.yml env.yml 3) The Ops Manager API is inaccessible (Recoverable) The task could not communicate with Ops Manager. SOLUTION: Rerun the upgrade-opsman task. The task will assume that the Ops Manager VM is not created, and will run the create-vm and import-installation tasks.","title":"Version Check Errors"},{"location":"upgrade.html#iaas-cli-errors","text":"1) When the CLI for a supported IAAS fails for any reason (i.e., bad network, outage, etc) we treat this as an IAAS CLI error. The following tasks can return an error from the IAAS's CLI: delete-vm , create-vm SOLUTION: The specific error will be returned as output, but most errors can simply be fixed by re-running the upgrade-opsman task.","title":"IAAS CLI Errors"},{"location":"before-you-begin/git-repo-layout.html","text":"Why use Git and GitHub? GitHub is a system that provides Git remotes, essentially, an internet accessible backup to the git repositories on your computer. Using a remote will enable a pipeline to access and update the state and configuration files. Git is a commonly used version control tool. It can be used to track code changes made to files within a repository (or \"repo\"). Changes can then be \"pushed\" to or \"pulled\" from remote copies of that repository. GitHub alternatives There are many alternatives to GitHub including Gitlabs, Google Cloud Source Repositories, etc. Any remote Git client will work with Platform Automation and Concourse. Refer to the Concourse Git resource documentation for details. To learn more about Git and Github, you can read this short git handbook . Creating a Git Repository To create a new, local Git repo: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # start a new repository git init my-platform-automation # navigate to the new directory it creates cd my-platform-automation # create a new directory for your config mkdir config # create remaining directories mkdir env state vars # create config files for director and opsman touch config/director.yml touch config/opsman.yml # create env file touch env/env.yml # create state file touch state/state.yml # Optional: # create vars files for parameters corresponding to configs touch vars/director-vars.yml touch vars/opsman-vars.yml # commit the file to the repository git commit -m \"add initial files\" Creating a GitHub Repository Next, navigate to GitHub and create a new remote repository. Under your profile, select \"Repositories\" Select \"New\" Name your new repository and follow the prompts Do not select to add any default files when prompted Copy the URL of your new GitHub repository Now, we can set the local Git repo's remote to the new GitHub repo: 1 2 3 4 5 # enter the path for the new GitHub repo git remote add origin https://github.com/YOUR-USERNAME/YOUR-REPOSITORY.git # push your changes to the master branch git push --set-upstream origin master You should now see your GitHub repo populated with the directories and empty files. Using GitHub with SSH A GitHub repository may be referenced as a remote repo by HTTPS or by SSH. In general, SSH keys are more secure. The Concourse Git resource supports using SSH keys to pull from a repository. For more information on using SSH keys with GitHub, refer to this SSH documentation. Recommended File Structure You now have both a local Git repo and a remote on GitHub. The above commands give you the recommended structure for a Platform Automation configuration repo: 1 2 3 4 5 \u251c\u2500\u2500 my - platform - automation \u2502 \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 env \u2502 \u251c\u2500\u2500 state \u2502 \u2514\u2500\u2500 vars config Holds config files for the products installed on your foundation. If using Credhub and/or vars files, these config files should have your ((parametrized)) values present in them env Holds env.yml , the environment file used by tasks that interact with Ops Manager. vars Holds product-specific vars files. The fields in these files get used to fill in ((parameters)) during interpolation steps. state Holds state.yml , which contains the VM ID for the Ops Manager VM. For further details regarding the contents of these files, please refer to the Inputs and Outputs documentation. Never commit secrets to Git It is a best practice to not commit secrets, including passwords, keys, and sensitive information, to Git or GitHub. Instead, use ((parameters)) . For more information on a recommended way to do this, using Credhub or vars files, review the handling secrets documentation. Multi-foundation The above is just one example of how to structure your configuration repository. You may instead decide to have a repo for just config files and separate repos just for vars files. This decouples the config parameter names from their values for multi-foundation templating. There are many possibilities for structuring Git repos in these complex situations. For guidance on how to best set up your git's file structure, refer to the Inputs and Outputs documentation and take note of the inputs and outputs of the various Platform Automation tasks . As long as the various input / output mappings correctly correlate to the expected ins and outs of the Platform Automation tasks, any file structure could theoretically work.","title":"Git Repository Layout"},{"location":"before-you-begin/git-repo-layout.html#why-use-git-and-github","text":"GitHub is a system that provides Git remotes, essentially, an internet accessible backup to the git repositories on your computer. Using a remote will enable a pipeline to access and update the state and configuration files. Git is a commonly used version control tool. It can be used to track code changes made to files within a repository (or \"repo\"). Changes can then be \"pushed\" to or \"pulled\" from remote copies of that repository. GitHub alternatives There are many alternatives to GitHub including Gitlabs, Google Cloud Source Repositories, etc. Any remote Git client will work with Platform Automation and Concourse. Refer to the Concourse Git resource documentation for details. To learn more about Git and Github, you can read this short git handbook .","title":"Why use Git and GitHub?"},{"location":"before-you-begin/git-repo-layout.html#creating-a-git-repository","text":"To create a new, local Git repo: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # start a new repository git init my-platform-automation # navigate to the new directory it creates cd my-platform-automation # create a new directory for your config mkdir config # create remaining directories mkdir env state vars # create config files for director and opsman touch config/director.yml touch config/opsman.yml # create env file touch env/env.yml # create state file touch state/state.yml # Optional: # create vars files for parameters corresponding to configs touch vars/director-vars.yml touch vars/opsman-vars.yml # commit the file to the repository git commit -m \"add initial files\"","title":"Creating a Git Repository"},{"location":"before-you-begin/git-repo-layout.html#creating-a-github-repository","text":"Next, navigate to GitHub and create a new remote repository. Under your profile, select \"Repositories\" Select \"New\" Name your new repository and follow the prompts Do not select to add any default files when prompted Copy the URL of your new GitHub repository Now, we can set the local Git repo's remote to the new GitHub repo: 1 2 3 4 5 # enter the path for the new GitHub repo git remote add origin https://github.com/YOUR-USERNAME/YOUR-REPOSITORY.git # push your changes to the master branch git push --set-upstream origin master You should now see your GitHub repo populated with the directories and empty files. Using GitHub with SSH A GitHub repository may be referenced as a remote repo by HTTPS or by SSH. In general, SSH keys are more secure. The Concourse Git resource supports using SSH keys to pull from a repository. For more information on using SSH keys with GitHub, refer to this SSH documentation.","title":"Creating a GitHub Repository"},{"location":"before-you-begin/git-repo-layout.html#recommended-file-structure","text":"You now have both a local Git repo and a remote on GitHub. The above commands give you the recommended structure for a Platform Automation configuration repo: 1 2 3 4 5 \u251c\u2500\u2500 my - platform - automation \u2502 \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 env \u2502 \u251c\u2500\u2500 state \u2502 \u2514\u2500\u2500 vars config Holds config files for the products installed on your foundation. If using Credhub and/or vars files, these config files should have your ((parametrized)) values present in them env Holds env.yml , the environment file used by tasks that interact with Ops Manager. vars Holds product-specific vars files. The fields in these files get used to fill in ((parameters)) during interpolation steps. state Holds state.yml , which contains the VM ID for the Ops Manager VM. For further details regarding the contents of these files, please refer to the Inputs and Outputs documentation. Never commit secrets to Git It is a best practice to not commit secrets, including passwords, keys, and sensitive information, to Git or GitHub. Instead, use ((parameters)) . For more information on a recommended way to do this, using Credhub or vars files, review the handling secrets documentation.","title":"Recommended File Structure"},{"location":"before-you-begin/git-repo-layout.html#multi-foundation","text":"The above is just one example of how to structure your configuration repository. You may instead decide to have a repo for just config files and separate repos just for vars files. This decouples the config parameter names from their values for multi-foundation templating. There are many possibilities for structuring Git repos in these complex situations. For guidance on how to best set up your git's file structure, refer to the Inputs and Outputs documentation and take note of the inputs and outputs of the various Platform Automation tasks . As long as the various input / output mappings correctly correlate to the expected ins and outs of the Platform Automation tasks, any file structure could theoretically work.","title":"Multi-foundation"},{"location":"before-you-begin/setup-s3-and-resources.html","text":"In this guide, you will learn how to set up an S3 bucket, how bucket permissions work, what we can store in a bucket, and how a pipeline may be set up to retrieve and store objects. Why use S3? Platform Automation for PCF uses and produces file artifacts that are too large to store in git. For example, many .pivotal product files are several gigabytes in size. Exported installation files may also be quite large. For environments that can't access the greater internet. This is a common security practice, but it also means that it's not possible to connect directly to PivNet to access the latest product versions for your upgrades. S3 and Concourse's native S3 integration makes it possible to store large file artifacts and retrieve the latest product versions in offline environments. With S3, we can place product files and new versions of OpsMan into a network whitelisted S3 bucket to be used by Platform Automation tasks. We can even create a Resources Pipeline that gets the latest version of products from PivNet and places them into our S3 bucket automatically. Alternatively, because a foundation's backup may be quite large, it is advantageous to persist it in a blobstore automatically through Concourse. Exported installations can then later be accessed through the blobstore. Because most object stores implement secure, durable solutions, exported installations in buckets are easily restorable and persistent. Prerequisites An Amazon Web Services account (commonly referred to as AWS) with access to S3 S3 blobstore compatibility Many cloud storage options exist including Amazon S3 , Google Storage , Minio , and Azure Blob Storage . However, not all object stores are \"S3 compatible\". Because Amazon defines the S3 API for accessing blobstores, and because the Amazon S3 product has emerged as the dominant blob storage solution, not all \"S3 compatible\" object stores act exactly the same. In general, if a storage solution claims to be \"S3 compatible\", it should work with the Concourse's S3 resource integration . But note that it may behave differently if interacting directly with the S3 API. Defer to the documentation of your preferred blobstore solution when setting up storage. Set up S3. With your AWS account, navigate to the S3 console and sign up for S3. Follow the on screen prompts. Now you are ready for buckets! AWS Root User When you sign up for the S3 service on Amazon, the account with the email and password you use is the AWS account root user. As a best practice, you should not use the root user to access and manipulate services. Instead, use AWS Identity and Access Management (commonly refered to as IAM) to create and manage users. For more info on how this works, check out this guide from Amazon . For simplicity, in the rest of this guide, we will use the AWS root user to show how a bucket may be set up and used with Platform Automation. Your First Bucket S3 stores data as objects within buckets. An object is any file that can be stored on a file system. Buckets are the containers for objects. Buckets can have permissions for who can create, write, delete, and see objects within that bucket. Navigate to the S3 console Click the \"Create bucket\" button Enter a DNS-compliant name for your new bucket This name must be unique across all of AWS S3 buckets and adhere to general URL guidelines. Make it something meaningful and memorable! Enter the \"Region\" you want the bucket to reside in Choose \"Create\" This creates a bucket with the default S3 settings. Bucket permissions and settings can be set during bucket creation or changed afterwards. Bucket settings can even be copied from other buckets you have. For a detailed look at creating buckets and managing initial settings, check out this documentation on creating buckets. Bucket Permissions By default, only the AWS account owner can access S3 resources, including buckets and objects. The resource owner may allow public access, allow specific IAM users permissions, or create a custom access policy. To view bucket permissions, from the S3 console, look at the \"Access\" column. Amazon S3 has the following Access permissions: Public - Everyone has access to one or more of the following: List objects, Write objects, Read and write permissions Objects can be public - The bucket is not public. But anyone with appropriate permissions can grant public access to objects. Buckets and objects not public - The bucket and objects do not have any public access. Only authorized users of this account - Access is isolated to IAM users and roles. In order to change who can access buckets or objects in buckets: Navigate to the S3 console . Choose the name of the bucket you created in the previous step In the top row, choose \"Permissions\" In this tab, you can set the various permissions for an individual bucket. For simplicity, in this guide, we will use public permissions for Concourse to access the files. Under the permissions tab for a bucket, choose \"Public access settings\" Choose \"Edit\" to change the public access settings Uncheck all boxes to allow public access. In general, the credentials being used to access an S3 compatible blobstore through Concourse must have Read and Write permissions. It is possible to use different user roles with different credentials to seperate which user can Read objects from the bucket and which user can Write objects to the bucket. Permissions Amazon S3 provides many permission settings for buckets . Specific IAM users can have access . Objects can have their own permissions . And buckets can even have their own custom Bucket Policies . Refer to your organization's security policy to best set up your S3 bucket. Object Versions By default, an S3 bucket will be unversioned . An unversioned bucket will not allow different versions of the same object. In order to take advantage of using an S3 bucket with Platform Automation, we will want to enable versioning. Enabling versioning is not required, but versioning does make the process easier, and will require less potential manual steps around naming updates to the new file whenever they are changed. Navigate to the S3 console Choose the name of the bucket you created in the previous step Select the \"Properties\" tab Click the \"Versioning\" tile Check the \"Enable Versioning\" Now that versioning is enabled, we can store multiple versions of a file. For example, given the following object: 1 my - exported - installation . zip We can now have multiple versions of this object stored in our S3 bucket: 1 2 my - exported - installation . zip ( version 111111 ) my - exported - installation . zip ( version 121212 ) Storing Files in S3 Any file that can be stored on a computer can be stored on S3. S3 is especially good at storing large files as it is designed to scale with large amounts of data while still being durable and fast. Platform Automation users may want to store the following files in S3: .pivotal product files .tgz stemcell files .ova Ops Manager files .zip foundation exports Platform Automation users will likely NOT want to store the following in S3: .yaml configuration files - Better suited for git secrets.yaml environment and secret files - There are a number of ways to handle these types of files, but they should not be stored in S3. Check out the Secrets Handling page for how to work with these types of files. Structuring your Bucket Like any computer, buckets can have folders and any number of sub-folders. The following is one way to set up your bucket's file structure: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \u251c\u2500\u2500 foundation - 1 \u2502 \u251c\u2500\u2500 products \u2502 \u2502 \u251c\u2500\u2500 healthwatch \u2502 \u2502 \u2502 healthwatch . pivotal \u2502 \u2502 \u251c\u2500\u2500 pas \u2502 \u2502 \u2502 pas . pivotal \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2502 \u2502 \u251c\u2500\u2500 stemcells \u2502 \u2502 \u251c\u2500\u2500 healthwatch - stemcell \u2502 \u2502 \u2502 ubuntu - trusty . tgz \u2502 \u2502 \u251c\u2500\u2500 pas - stemcell \u2502 \u2502 \u2502 ubuntu - xenial . tgz \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2502 \u2502 \u251c\u2500\u2500 foundation1 - exports \u2502 foundation1 - installation . zip When viewing a bucket in the AWS S3 console, simple select \"Create Folder\". To create a sub-folder, when viewing a specific folder, select \"Create Folder\" again. When attempting to access a specific object in a folder, simply include the folder structure before the object name: 1 foundation1 / products / healthwatch / my - healthwatch - product . pivotal Using a Bucket When using the Concourse S3 Resource , several configuration properties are available for retreiving objects. The bucket name is required. On networking and accessing a bucket In order for your Concourse to have access to your Amazon S3 bucket, ensure that you have the appropriate firewall and networking settings for your Concourse instance to make requests to your bucket. Concourse uses various \"outside\" resources to perform certain jobs. Ensure that Concourse can \"talk\" to your Amazon S3 bucket. Reference Resources Pipeline The resources pipeline may be used to download dependencies from Pivnet and place them into a trusted S3 bucket. The various resources_types use the Concourse S3 Resource type and several Platform Automation tasks to accomplish this. The following is an S3-specific breakdown of these components and where to find more information. The download-product task The download-product task lets you download products from PivNet. If S3 properties are set in the download config , these files can be placed into an S3 bucket. If S3 configurations are set, this task will perform a specific filename operation that will prepend meta data to the filename. If downloading the product Example Product version 2.2.1 from PivNet where the product slug is example-product and the version is 2.2.1 , when directly downloaded from PivNet, the file may appear as: 1 product - 2 . 2 - build99 . pivotal Because PivNet file names do not always have the necessary metadata required by Platform Automation, the download product task will prepend the necessary information to the filename before it is placed into the S3 bucket: 1 [ example - product , 2 . 2 . 1 - build99 ] product - 2 . 2 - build99 . pivotal For complete information on this task and how it works, refer to the download-product task reference. Changing S3 file names Do not change the meta information prepended by download-product . This information is required by the download-product-s3 task to properly parse product versions. If placing a product file into an S3 bucket manually, ensure that it has the proper file name format; opening bracket, the product slug, a single comma, the product's version, and finally, closing bracket. There should be no spaces between the two brackets. For example, for a product with slug of product-slug and version of 1.1.1 : 1 [ product - slug , 1 . 1 . 1 ] original - filename . pivotal The download-product-s3 task The download-product-s3 task lets you download products from an S3 bucket. The prefixed metadata added by download-product is used to find the appropriate file. This task uses the same download-product config file as download-product to ensure consistency across what is put in S3 and what is being accessed latter by download-product-s3 . download-product and download-product-s3 are designed to be used together. The download product config should be different between the two tasks. For complete information on this task and how it works, refer to the download-product-s3 task reference.","title":"Set Up S3 for File Storage"},{"location":"before-you-begin/setup-s3-and-resources.html#why-use-s3","text":"Platform Automation for PCF uses and produces file artifacts that are too large to store in git. For example, many .pivotal product files are several gigabytes in size. Exported installation files may also be quite large. For environments that can't access the greater internet. This is a common security practice, but it also means that it's not possible to connect directly to PivNet to access the latest product versions for your upgrades. S3 and Concourse's native S3 integration makes it possible to store large file artifacts and retrieve the latest product versions in offline environments. With S3, we can place product files and new versions of OpsMan into a network whitelisted S3 bucket to be used by Platform Automation tasks. We can even create a Resources Pipeline that gets the latest version of products from PivNet and places them into our S3 bucket automatically. Alternatively, because a foundation's backup may be quite large, it is advantageous to persist it in a blobstore automatically through Concourse. Exported installations can then later be accessed through the blobstore. Because most object stores implement secure, durable solutions, exported installations in buckets are easily restorable and persistent.","title":"Why use S3?"},{"location":"before-you-begin/setup-s3-and-resources.html#prerequisites","text":"An Amazon Web Services account (commonly referred to as AWS) with access to S3 S3 blobstore compatibility Many cloud storage options exist including Amazon S3 , Google Storage , Minio , and Azure Blob Storage . However, not all object stores are \"S3 compatible\". Because Amazon defines the S3 API for accessing blobstores, and because the Amazon S3 product has emerged as the dominant blob storage solution, not all \"S3 compatible\" object stores act exactly the same. In general, if a storage solution claims to be \"S3 compatible\", it should work with the Concourse's S3 resource integration . But note that it may behave differently if interacting directly with the S3 API. Defer to the documentation of your preferred blobstore solution when setting up storage. Set up S3. With your AWS account, navigate to the S3 console and sign up for S3. Follow the on screen prompts. Now you are ready for buckets! AWS Root User When you sign up for the S3 service on Amazon, the account with the email and password you use is the AWS account root user. As a best practice, you should not use the root user to access and manipulate services. Instead, use AWS Identity and Access Management (commonly refered to as IAM) to create and manage users. For more info on how this works, check out this guide from Amazon . For simplicity, in the rest of this guide, we will use the AWS root user to show how a bucket may be set up and used with Platform Automation.","title":"Prerequisites"},{"location":"before-you-begin/setup-s3-and-resources.html#your-first-bucket","text":"S3 stores data as objects within buckets. An object is any file that can be stored on a file system. Buckets are the containers for objects. Buckets can have permissions for who can create, write, delete, and see objects within that bucket. Navigate to the S3 console Click the \"Create bucket\" button Enter a DNS-compliant name for your new bucket This name must be unique across all of AWS S3 buckets and adhere to general URL guidelines. Make it something meaningful and memorable! Enter the \"Region\" you want the bucket to reside in Choose \"Create\" This creates a bucket with the default S3 settings. Bucket permissions and settings can be set during bucket creation or changed afterwards. Bucket settings can even be copied from other buckets you have. For a detailed look at creating buckets and managing initial settings, check out this documentation on creating buckets.","title":"Your First Bucket"},{"location":"before-you-begin/setup-s3-and-resources.html#bucket-permissions","text":"By default, only the AWS account owner can access S3 resources, including buckets and objects. The resource owner may allow public access, allow specific IAM users permissions, or create a custom access policy. To view bucket permissions, from the S3 console, look at the \"Access\" column. Amazon S3 has the following Access permissions: Public - Everyone has access to one or more of the following: List objects, Write objects, Read and write permissions Objects can be public - The bucket is not public. But anyone with appropriate permissions can grant public access to objects. Buckets and objects not public - The bucket and objects do not have any public access. Only authorized users of this account - Access is isolated to IAM users and roles. In order to change who can access buckets or objects in buckets: Navigate to the S3 console . Choose the name of the bucket you created in the previous step In the top row, choose \"Permissions\" In this tab, you can set the various permissions for an individual bucket. For simplicity, in this guide, we will use public permissions for Concourse to access the files. Under the permissions tab for a bucket, choose \"Public access settings\" Choose \"Edit\" to change the public access settings Uncheck all boxes to allow public access. In general, the credentials being used to access an S3 compatible blobstore through Concourse must have Read and Write permissions. It is possible to use different user roles with different credentials to seperate which user can Read objects from the bucket and which user can Write objects to the bucket. Permissions Amazon S3 provides many permission settings for buckets . Specific IAM users can have access . Objects can have their own permissions . And buckets can even have their own custom Bucket Policies . Refer to your organization's security policy to best set up your S3 bucket.","title":"Bucket Permissions"},{"location":"before-you-begin/setup-s3-and-resources.html#object-versions","text":"By default, an S3 bucket will be unversioned . An unversioned bucket will not allow different versions of the same object. In order to take advantage of using an S3 bucket with Platform Automation, we will want to enable versioning. Enabling versioning is not required, but versioning does make the process easier, and will require less potential manual steps around naming updates to the new file whenever they are changed. Navigate to the S3 console Choose the name of the bucket you created in the previous step Select the \"Properties\" tab Click the \"Versioning\" tile Check the \"Enable Versioning\" Now that versioning is enabled, we can store multiple versions of a file. For example, given the following object: 1 my - exported - installation . zip We can now have multiple versions of this object stored in our S3 bucket: 1 2 my - exported - installation . zip ( version 111111 ) my - exported - installation . zip ( version 121212 )","title":"Object Versions"},{"location":"before-you-begin/setup-s3-and-resources.html#storing-files-in-s3","text":"Any file that can be stored on a computer can be stored on S3. S3 is especially good at storing large files as it is designed to scale with large amounts of data while still being durable and fast. Platform Automation users may want to store the following files in S3: .pivotal product files .tgz stemcell files .ova Ops Manager files .zip foundation exports Platform Automation users will likely NOT want to store the following in S3: .yaml configuration files - Better suited for git secrets.yaml environment and secret files - There are a number of ways to handle these types of files, but they should not be stored in S3. Check out the Secrets Handling page for how to work with these types of files.","title":"Storing Files in S3"},{"location":"before-you-begin/setup-s3-and-resources.html#structuring-your-bucket","text":"Like any computer, buckets can have folders and any number of sub-folders. The following is one way to set up your bucket's file structure: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \u251c\u2500\u2500 foundation - 1 \u2502 \u251c\u2500\u2500 products \u2502 \u2502 \u251c\u2500\u2500 healthwatch \u2502 \u2502 \u2502 healthwatch . pivotal \u2502 \u2502 \u251c\u2500\u2500 pas \u2502 \u2502 \u2502 pas . pivotal \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2502 \u2502 \u251c\u2500\u2500 stemcells \u2502 \u2502 \u251c\u2500\u2500 healthwatch - stemcell \u2502 \u2502 \u2502 ubuntu - trusty . tgz \u2502 \u2502 \u251c\u2500\u2500 pas - stemcell \u2502 \u2502 \u2502 ubuntu - xenial . tgz \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2502 \u2502 \u251c\u2500\u2500 foundation1 - exports \u2502 foundation1 - installation . zip When viewing a bucket in the AWS S3 console, simple select \"Create Folder\". To create a sub-folder, when viewing a specific folder, select \"Create Folder\" again. When attempting to access a specific object in a folder, simply include the folder structure before the object name: 1 foundation1 / products / healthwatch / my - healthwatch - product . pivotal","title":"Structuring your Bucket"},{"location":"before-you-begin/setup-s3-and-resources.html#using-a-bucket","text":"When using the Concourse S3 Resource , several configuration properties are available for retreiving objects. The bucket name is required. On networking and accessing a bucket In order for your Concourse to have access to your Amazon S3 bucket, ensure that you have the appropriate firewall and networking settings for your Concourse instance to make requests to your bucket. Concourse uses various \"outside\" resources to perform certain jobs. Ensure that Concourse can \"talk\" to your Amazon S3 bucket.","title":"Using a Bucket"},{"location":"before-you-begin/setup-s3-and-resources.html#reference-resources-pipeline","text":"The resources pipeline may be used to download dependencies from Pivnet and place them into a trusted S3 bucket. The various resources_types use the Concourse S3 Resource type and several Platform Automation tasks to accomplish this. The following is an S3-specific breakdown of these components and where to find more information.","title":"Reference Resources Pipeline"},{"location":"before-you-begin/setup-s3-and-resources.html#the-download-product-task","text":"The download-product task lets you download products from PivNet. If S3 properties are set in the download config , these files can be placed into an S3 bucket. If S3 configurations are set, this task will perform a specific filename operation that will prepend meta data to the filename. If downloading the product Example Product version 2.2.1 from PivNet where the product slug is example-product and the version is 2.2.1 , when directly downloaded from PivNet, the file may appear as: 1 product - 2 . 2 - build99 . pivotal Because PivNet file names do not always have the necessary metadata required by Platform Automation, the download product task will prepend the necessary information to the filename before it is placed into the S3 bucket: 1 [ example - product , 2 . 2 . 1 - build99 ] product - 2 . 2 - build99 . pivotal For complete information on this task and how it works, refer to the download-product task reference. Changing S3 file names Do not change the meta information prepended by download-product . This information is required by the download-product-s3 task to properly parse product versions. If placing a product file into an S3 bucket manually, ensure that it has the proper file name format; opening bracket, the product slug, a single comma, the product's version, and finally, closing bracket. There should be no spaces between the two brackets. For example, for a product with slug of product-slug and version of 1.1.1 : 1 [ product - slug , 1 . 1 . 1 ] original - filename . pivotal","title":"The download-product task"},{"location":"before-you-begin/setup-s3-and-resources.html#the-download-product-s3-task","text":"The download-product-s3 task lets you download products from an S3 bucket. The prefixed metadata added by download-product is used to find the appropriate file. This task uses the same download-product config file as download-product to ensure consistency across what is put in S3 and what is being accessed latter by download-product-s3 . download-product and download-product-s3 are designed to be used together. The download product config should be different between the two tasks. For complete information on this task and how it works, refer to the download-product-s3 task reference.","title":"The download-product-s3 task"},{"location":"configuration-management/configure-auth.html","text":"Generating an Auth File Ops Manager's authentication system can be configured several ways. The format of the configuration file varies according to the authentication method to be used. configure-authentication : 1 2 3 4 --- username : username password : password decryption-passphrase : decryption-passphrase configure-ldap-authentication : 1 2 3 4 5 6 7 8 9 10 11 decryption-passphrase : some-passphrase server-url : ldap://example.com ldap-username : cn=admin,dc=opsmanager,dc=com ldap-password : some-password user-search-base : ou=users,dc=opsmanager,dc=com user-search-filter : cn={0} group-search-base : ou=groups,dc=opsmanager,dc=com group-search-filter : member={0} ldap-rbac-admin-group-name : cn=opsmgradmins,ou=groups,dc=opsmanager,dc=com email-attribute : mail ldap-referrals : follow configure-saml-authentication : 1 2 3 4 5 6 --- decryption-passphrase : decryption-passphrase saml-idp-metadata : https://saml.example.com:8080 saml-bosh-idp-metadata : https://bosh-saml.example.com:8080 saml-rbac-admin-group : opsman.full_control saml-rbac-groups-attribute : myenterprise Managing Configuration, Auth, and State Files To use all these files with the Concourse tasks that require them, you need to make them available as Concourse Resources. They\u2019re all text files. There are many resource types that can work for this. In our examples, we use a git repository. As with the tasks and image, you\u2019ll need to declare a resource in your pipeline for each repo you need.","title":"Configure Auth"},{"location":"configuration-management/configure-auth.html#generating-an-auth-file","text":"Ops Manager's authentication system can be configured several ways. The format of the configuration file varies according to the authentication method to be used.","title":"Generating an Auth File"},{"location":"configuration-management/configure-auth.html#configure-authentication","text":"1 2 3 4 --- username : username password : password decryption-passphrase : decryption-passphrase","title":"configure-authentication:"},{"location":"configuration-management/configure-auth.html#configure-ldap-authentication","text":"1 2 3 4 5 6 7 8 9 10 11 decryption-passphrase : some-passphrase server-url : ldap://example.com ldap-username : cn=admin,dc=opsmanager,dc=com ldap-password : some-password user-search-base : ou=users,dc=opsmanager,dc=com user-search-filter : cn={0} group-search-base : ou=groups,dc=opsmanager,dc=com group-search-filter : member={0} ldap-rbac-admin-group-name : cn=opsmgradmins,ou=groups,dc=opsmanager,dc=com email-attribute : mail ldap-referrals : follow","title":"configure-ldap-authentication:"},{"location":"configuration-management/configure-auth.html#configure-saml-authentication","text":"1 2 3 4 5 6 --- decryption-passphrase : decryption-passphrase saml-idp-metadata : https://saml.example.com:8080 saml-bosh-idp-metadata : https://bosh-saml.example.com:8080 saml-rbac-admin-group : opsman.full_control saml-rbac-groups-attribute : myenterprise","title":"configure-saml-authentication:"},{"location":"configuration-management/configure-auth.html#managing-configuration-auth-and-state-files","text":"To use all these files with the Concourse tasks that require them, you need to make them available as Concourse Resources. They\u2019re all text files. There are many resource types that can work for this. In our examples, we use a git repository. As with the tasks and image, you\u2019ll need to declare a resource in your pipeline for each repo you need.","title":"Managing Configuration, Auth, and State Files"},{"location":"configuration-management/configure-env.html","text":"Generating an Env File Almost all om commands require an env file to describe the Ops Manager Authenticated API endpoint (This is the URL you connect to Ops Manager with). The configuration for authentication has a dependency on either username/password 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- target : https://pcf.example.com connect-timeout : 30 # default 5 request-timeout : 1800 # default 1800 skip-ssl-validation : false # default false username : username password : password # decryption-passphrase is optional, # except for use with `import-installation`. # OpsMan depends on the passphrase # to decrypt the imported installation. # For other commands, providing this key allows # decryption of the OpsMan VM after reboot, # which would otherwise need to be done manually. decryption-passphrase : passphrase or, if using SAML, a client-id and client-secret. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- target : https://pcf.example.com connect-timeout : 30 # default 5 request-timeout : 1800 # default 1800 skip-ssl-validation : false # default false client-id : client_id client-secret : client_secret # decryption-passphrase is optional, # except for use with `import-installation`. # OpsMan depends on the passphrase # to decrypt the imported installation. # For other commands, providing this key allows # decryption of the OpsMan VM after reboot, # which would otherwise need to be done manually. decryption-passphrase : passphrase While decryption-passphrase is nominally optional, if you intend to use a single env.yml for an entire pipeline, it will be necessary to include for use with the import-installation step.","title":"Configuring Env"},{"location":"configuration-management/configure-env.html#generating-an-env-file","text":"Almost all om commands require an env file to describe the Ops Manager Authenticated API endpoint (This is the URL you connect to Ops Manager with). The configuration for authentication has a dependency on either username/password 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- target : https://pcf.example.com connect-timeout : 30 # default 5 request-timeout : 1800 # default 1800 skip-ssl-validation : false # default false username : username password : password # decryption-passphrase is optional, # except for use with `import-installation`. # OpsMan depends on the passphrase # to decrypt the imported installation. # For other commands, providing this key allows # decryption of the OpsMan VM after reboot, # which would otherwise need to be done manually. decryption-passphrase : passphrase or, if using SAML, a client-id and client-secret. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- target : https://pcf.example.com connect-timeout : 30 # default 5 request-timeout : 1800 # default 1800 skip-ssl-validation : false # default false client-id : client_id client-secret : client_secret # decryption-passphrase is optional, # except for use with `import-installation`. # OpsMan depends on the passphrase # to decrypt the imported installation. # For other commands, providing this key allows # decryption of the OpsMan VM after reboot, # which would otherwise need to be done manually. decryption-passphrase : passphrase While decryption-passphrase is nominally optional, if you intend to use a single env.yml for an entire pipeline, it will be necessary to include for use with the import-installation step.","title":"Generating an Env File"},{"location":"configuration-management/creating-a-director-config-file.html","text":"Extracting a director configuration file, an externalized config that lives outside of Ops Manager, can make it easier to manage multiple foundations as well as help with: traceability avoiding configuration drift configuration promotion Prerequisites To extract the configuration for an director, you will first need to do the following: Create the infrastructure for your installation. To make the infrastructure creation easier, consider using terraform to create the resources needed: terraforming-aws terraforming-gcp terraforming-azure terraforming-vsphere terraforming-openstack Install Ops Manager using the create-vm task (Optional) Configure Ops Manager manually within the Ops Manager UI (Instructions for doing so can be found using the Official-PCF-Documentation ) Extracting Configuration om has a command called staged-director-config , which is used to extract the Ops Manager and the BOSH director configuration from the targeted foundation. Info staged-director-config will not be able to grab all sensitive fields in your Ops Manager installation (for example: vcenter_username and vcenter_password if using vsphere). To find these missing fields, please refer to the Ops Manager API Documentation Sample usage: om --env env.yml staged-director-config > director.yml will give you the whole configuration of Ops Manager in a single yml file. It will look more or less the same as the example above. You can check it in to your VCS. The following is an example configuration file for Ops Manager that might return after running this command: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 --- az-configuration : - clusters : - cluster : cluster-name resource_pool : resource-pool-name name : AZ01 properties-configuration : iaas_configuration : vcenter_host : vcenter.example.com vcenter_password : admin vcenter_username : password ...... director_configuration : blobstore_type : local bosh_recreate_on_next_deploy : false custom_ssh_banner : null ...... security_configuration : generate_vm_passwords : true trusted_certificates : syslog_configuration : enabled : false network-assignment : network : name : INFRASTRUCTURE other_availability_zones : [] singleton_availability_zone : name : AZ01 networks-configuration : icmp_checks_enabled : false networks : - name : NETWORK-NAME ...... resource-configuration : compilation : instance_type : id : automatic instances : automatic ...... Configuring Director Using Config File Now you can modify the settings in the configuration file directly instead of operating in the web ui. After you finish editing the file, the configuration file will need to apply back to the Ops Manager instance. The command configure-director will do the job. Sample usage: om --env env.yml configure-director --config director.yml Promoting Ops Manager to Another Foundation The configuration file is the exact state of a given foundation, it contains some environment specific properties. You need to manually edit those properties to reflect the state of the new foundation. Or, when extracting the configuration file from the foundation, you can use the flag --include-placeholders , it will help to parameterize some variables to ease the process of adapt for another foundation.","title":"Creating a Director Config File"},{"location":"configuration-management/creating-a-director-config-file.html#prerequisites","text":"To extract the configuration for an director, you will first need to do the following: Create the infrastructure for your installation. To make the infrastructure creation easier, consider using terraform to create the resources needed: terraforming-aws terraforming-gcp terraforming-azure terraforming-vsphere terraforming-openstack Install Ops Manager using the create-vm task (Optional) Configure Ops Manager manually within the Ops Manager UI (Instructions for doing so can be found using the Official-PCF-Documentation )","title":"Prerequisites"},{"location":"configuration-management/creating-a-director-config-file.html#extracting-configuration","text":"om has a command called staged-director-config , which is used to extract the Ops Manager and the BOSH director configuration from the targeted foundation. Info staged-director-config will not be able to grab all sensitive fields in your Ops Manager installation (for example: vcenter_username and vcenter_password if using vsphere). To find these missing fields, please refer to the Ops Manager API Documentation Sample usage: om --env env.yml staged-director-config > director.yml will give you the whole configuration of Ops Manager in a single yml file. It will look more or less the same as the example above. You can check it in to your VCS. The following is an example configuration file for Ops Manager that might return after running this command: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 --- az-configuration : - clusters : - cluster : cluster-name resource_pool : resource-pool-name name : AZ01 properties-configuration : iaas_configuration : vcenter_host : vcenter.example.com vcenter_password : admin vcenter_username : password ...... director_configuration : blobstore_type : local bosh_recreate_on_next_deploy : false custom_ssh_banner : null ...... security_configuration : generate_vm_passwords : true trusted_certificates : syslog_configuration : enabled : false network-assignment : network : name : INFRASTRUCTURE other_availability_zones : [] singleton_availability_zone : name : AZ01 networks-configuration : icmp_checks_enabled : false networks : - name : NETWORK-NAME ...... resource-configuration : compilation : instance_type : id : automatic instances : automatic ......","title":"Extracting Configuration"},{"location":"configuration-management/creating-a-director-config-file.html#configuring-director-using-config-file","text":"Now you can modify the settings in the configuration file directly instead of operating in the web ui. After you finish editing the file, the configuration file will need to apply back to the Ops Manager instance. The command configure-director will do the job. Sample usage: om --env env.yml configure-director --config director.yml","title":"Configuring Director Using Config File"},{"location":"configuration-management/creating-a-director-config-file.html#promoting-ops-manager-to-another-foundation","text":"The configuration file is the exact state of a given foundation, it contains some environment specific properties. You need to manually edit those properties to reflect the state of the new foundation. Or, when extracting the configuration file from the foundation, you can use the flag --include-placeholders , it will help to parameterize some variables to ease the process of adapt for another foundation.","title":"Promoting Ops Manager to Another Foundation"},{"location":"configuration-management/creating-a-product-config-file.html","text":"Extracting a product configuration file, an externalized config that lives outside of Ops Manager, can make it easier to manage multiple foundations as well as help with: traceability avoiding configuration drift configuration promotion From Pivnet A configuration file can be generated from the tile metadata directly from Pivotal Network. Prerequisites A token for the Pivotal Network API is required. You'll need the Platform Automation Docker Image imported and ready to run . For products that have multiple .pivotal files, you'll need a glob pattern uniquely matching one of them. Workflow Generate the Config Template Directory 1 export PIVNET_API_TOKEN = 'your-pivotal-network-api-token' (Alternatively, you can write the above to a file and source it to avoid credentials in your bash history.) 1 2 3 4 5 6 7 docker run -it -v $HOME /configs:/configs platform-automation-image \\ om config-template \\ --output-directory /configs/ \\ --pivnet-api-token \" ${ PIVNET_API_TOKEN } \" \\ --pivnet-product-slug elastic-runtime \\ --product-version '2.5.0' \\ --product-file-glob 'cf*.pivotal' # Only necessary if the product has multiple .pivotal files This will create or update a directory at $HOME/configs/cf/2.5.0/ . cd into the directory to get started creating your config. Interpolate a Config The directory will contain a product.yml file. This is the template for the product configuration you're about to build. Open it in an editor of your choice. Get familiar with what's in there. The values will be variables intended to be interpolated from other sources, designated with the (()) syntax. You can find the value for any property with a default in the product-default-vars.yml file. This file serves as a good example of a variable source. You'll need to create a vars file of your own for variables without default values. For the base template, you can get a list of required variables by running 1 2 3 4 5 6 docker run -it -v $HOME /configs:/configs platform-automation-image \\ om interpolate \\ --config product.yml \\ -l product-default-vars.yml \\ -l resource-vars.yml \\ -l errand-vars.yml Put all those vars in a file and give them the appropriate values. Once you've included all the variables, the output will be the finished template. For the rest of this guide, we will refer to these vars as required-vars.yml . There may be situations that call for splitting your vars across multiple files. This can be useful if there are vars that need to be interpolated when you apply the configuration, rather than when you create the final template. You might consider creating a seperate vars file for each of the following cases: credentials (these vars can then be persisted separately/securely ) foundation-specific variables when using the same template for multiple foundations You can use the --skip-missing flag when creating your final template using om interpolate to leave such vars to be rendered later. If you're having trouble figuring out what the values should be, here are some approaches you can use: Look in the template where the variable appears for some additional context of its value. Look at the tile's online documentation Upload the tile to an Ops Manager and visit the tile in the Ops Manager UI to see if that provides any hints. If you are still struggling, inspecting the html of the Ops Manager webpage can more accurately map the value names to the associated UI element. When Using The Ops Manager Docs and UI Be aware that the field names in the UI do not necessarily map directly to property names. Optional Features The above process will get you a default installation, with no optional features or variables, that is entirely deployed in a single Availability Zone (AZ). In order to provide non-required variables, use multiple AZs, or make non-default selections for some options, you'll need to use some of the ops files in one of the following four directories: features allow the enabling of selectors for a product. For example, enabling/disabling of an s3 bucket network contains options for enabling 2-3 availability zones for network configuration optional contains optional properties without defaults. For optional values that can be provided more than once, there's an ops file for each param count resource contains configuration that can be applied to resource configuration. For example, BOSH vm extensions To use an ops file, add -o with the path to the ops file you want to use to your interpolate command. So, to enable TCP routing in PAS, you would add -o features/tcp_routing-enable.yml . For the rest of this guide, the vars for this feature are referred to as feature-vars.yml . If you run your complete command, you should again get a list of any newly-required variables. 1 2 3 4 5 6 7 8 9 docker run -it -v $HOME /configs:/configs platform-automation-image \\ om interpolate \\ --config product.yml \\ -l product-default-vars.yml \\ -l resource-vars.yml \\ -l required-vars.yml \\ -o features/tcp_routing-enable.yml \\ -l feature-vars.yml \\ -l errand-vars.yml Finalize Your Configuration Once you've selected your ops files and created your vars files, decide which vars you want in the template and which you want to have interpolated later. Create a final template and write it to a file, using only the vars you want to in the template, and using --skip-missing to allow the rest to remain as variables. 1 2 3 4 5 6 7 8 9 10 11 docker run -it -v $HOME /configs:/configs platform-automation-image \\ om interpolate \\ --config product.yml \\ -l product-default-vars.yml \\ -l resource-vars.yml \\ -l required-vars.yml \\ -o features/tcp_routing-enable.yml \\ -l feature-vars.yml \\ -l errand-vars.yml \\ --skip-missing \\ > pas-config-template.yml You can check-in the resulting configuration to a git repo. For vars that do not include credentials, you can check those vars files in, as well. Handle vars that are secret more carefully . You can then dispose of the config template directory. From A Staged Product A configuration can be generated from a staged product on an already existing Ops Manager. Prerequisites To extract the configuration for a product, you will first need to do the following: Upload and stage your desired product(s) to a fully deployed Ops Manager. For example, let's use PAS on Vsphere Configure your product manually according to the product's install instructions . Workflow om has a command called staged-config , which is used to extract staged product configuration present in the Ops Manager UI of the targeted foundation. Sample usage, using om directly and assuming the PAS product: om --env env.yml staged-config --include-placeholders --product-name cf > tile-config.yml Most products will contain the following high level keys: network-properties product-properties resource-config You can check the file in to git. For convenience, Platform Automation provides you with two ways to use the om staged-config command. The command can be run as a task inside of your pipeline. As an example of how to invoke this for the PAS product in your pipeline.yml(resources not listed): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 jobs : - name : staged-pas-config plan : - aggregate : - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : staged-config image : platform-automation-image file : platform-automation-tasks/tasks/staged-config.yml input_mapping : env : configuration params : PRODUCT_NAME : cf ENV_FILE : ((foundation))/env/env.yml SUBSTITUTE_CREDENTIALS_WITH_PLACEHOLDERS : true - put : configuration params : file : generated-config/cf.yml This task will connect to the Ops Manager defined in your env.yml , download the current staged configuration of your product, and put it into a generated-config folder in the concourse job. The put in concourse allows you to persist this config outside the concourse container. Alternatively, this can be run external to concourse by using docker. An example of how to do this using on the linux/mac command line: 1 2 3 docker import ${ PLATFORM_AUTOMATION_IMAGE_TGZ } platform-automation-image docker run -it --rm -v $PWD :/workspace -w /workspace platform-automation-image \\ om --env ${ ENV_FILE } staged-config --include-placeholders --product-name cf Using Ops Files for Multi-Foundation --include-placeholders in the om command is a vital first step to externalizing your configuration for multiple foundations. This will search the Ops Manager product for fields marked as \"secrets\", and replace those values with ((placeholder_credentials)) . In order to fully support multiple foundations, however, a bit more work is necessary. There are two ways to do this: using secrets management or ops files. This section will explain how to support multiple foundations using ops files. Starting with an incomplete PAS config from vSphere as an example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # base.yml # An incomplete yaml response from om staged-config product-name : cf product-properties : .cloud_controller.apps_domain : value : ((cloud_controller_apps_domain)) .cloud_controller.encrypt_key : value : secret : ((cloud_controller_encrypt_key.secret)) .properties.security_acknowledgement : value : X .properties.cloud_controller_default_stack : value : default network-properties : network : name : DEPLOYMENT other_availability_zones : - name : AZ01 singleton_availability_zone : name : AZ01 resource-config : diego_cell : instances : 5 instance_type : id : automatic uaa : instances : 1 instance_type : id : automatic For a single foundation deploy, leaving values such as \".cloud_controller.apps_domain\" as-is would work fine. For multiple foundations, this value will be different per deployed foundation. Other values, such as .cloud_controller.encrypt_key have a secret that already have a placeholder from om . If different foundations have different load requirements, even the values in resource-config can be edited using ops files . Using the example above, let's try filling in the existing placeholder for cloud_controller.apps_domain in our first foundation. 1 2 3 4 # replace-domain-ops-file.yml - type : replace path : /product-properties/.cloud_controller.apps_domain/value? value : unique.foundation.one.domain To test that the ops file will work in your base.yml , this can be done locally using bosh int : 1 bosh int base.yml -o replace-domain.yml This will output base.yml with the replaced(interpolated) values: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # interpolated-base.yml network-properties : network : name : DEPLOYMENT other_availability_zones : - name : AZ01 singleton_availability_zone : name : AZ01 product-name : cf product-properties : .cloud_controller.apps_domain : unique.foundation.one.domain .cloud_controller.encrypt_key : value : secret : ((cloud_controller_encrypt_key.secret)) .properties.cloud_controller_default_stack : value : default .properties.security_acknowledgement : value : X resource-config : diego_cell : instance_type : id : automatic instances : 5 uaa : instance_type : id : automatic instances : 1 Anything that needs to be different per deployment can be replaced via ops files as long as the path: is correct. Upgrading products to new patch versions: Configuration settings should not differ between successive patch versions within the same minor version line. Underlying properties or property names may change, but the tile's upgrade process automatically translates properties to the new fields and values. Pivotal cannot guarantee the functionality of upgrade scripts in third-party PCF products. Replicating configuration settings from one product to the same product on a different foundation: Because properties and property names can change between patch versions of a product, you can only safely apply configuration settings across products if their versions exactly match.","title":"Creating a Product Config File"},{"location":"configuration-management/creating-a-product-config-file.html#from-pivnet","text":"A configuration file can be generated from the tile metadata directly from Pivotal Network.","title":"From Pivnet"},{"location":"configuration-management/creating-a-product-config-file.html#prerequisites","text":"A token for the Pivotal Network API is required. You'll need the Platform Automation Docker Image imported and ready to run . For products that have multiple .pivotal files, you'll need a glob pattern uniquely matching one of them.","title":"Prerequisites"},{"location":"configuration-management/creating-a-product-config-file.html#workflow","text":"","title":"Workflow"},{"location":"configuration-management/creating-a-product-config-file.html#generate-the-config-template-directory","text":"1 export PIVNET_API_TOKEN = 'your-pivotal-network-api-token' (Alternatively, you can write the above to a file and source it to avoid credentials in your bash history.) 1 2 3 4 5 6 7 docker run -it -v $HOME /configs:/configs platform-automation-image \\ om config-template \\ --output-directory /configs/ \\ --pivnet-api-token \" ${ PIVNET_API_TOKEN } \" \\ --pivnet-product-slug elastic-runtime \\ --product-version '2.5.0' \\ --product-file-glob 'cf*.pivotal' # Only necessary if the product has multiple .pivotal files This will create or update a directory at $HOME/configs/cf/2.5.0/ . cd into the directory to get started creating your config.","title":"Generate the Config Template Directory"},{"location":"configuration-management/creating-a-product-config-file.html#interpolate-a-config","text":"The directory will contain a product.yml file. This is the template for the product configuration you're about to build. Open it in an editor of your choice. Get familiar with what's in there. The values will be variables intended to be interpolated from other sources, designated with the (()) syntax. You can find the value for any property with a default in the product-default-vars.yml file. This file serves as a good example of a variable source. You'll need to create a vars file of your own for variables without default values. For the base template, you can get a list of required variables by running 1 2 3 4 5 6 docker run -it -v $HOME /configs:/configs platform-automation-image \\ om interpolate \\ --config product.yml \\ -l product-default-vars.yml \\ -l resource-vars.yml \\ -l errand-vars.yml Put all those vars in a file and give them the appropriate values. Once you've included all the variables, the output will be the finished template. For the rest of this guide, we will refer to these vars as required-vars.yml . There may be situations that call for splitting your vars across multiple files. This can be useful if there are vars that need to be interpolated when you apply the configuration, rather than when you create the final template. You might consider creating a seperate vars file for each of the following cases: credentials (these vars can then be persisted separately/securely ) foundation-specific variables when using the same template for multiple foundations You can use the --skip-missing flag when creating your final template using om interpolate to leave such vars to be rendered later. If you're having trouble figuring out what the values should be, here are some approaches you can use: Look in the template where the variable appears for some additional context of its value. Look at the tile's online documentation Upload the tile to an Ops Manager and visit the tile in the Ops Manager UI to see if that provides any hints. If you are still struggling, inspecting the html of the Ops Manager webpage can more accurately map the value names to the associated UI element. When Using The Ops Manager Docs and UI Be aware that the field names in the UI do not necessarily map directly to property names.","title":"Interpolate a Config"},{"location":"configuration-management/creating-a-product-config-file.html#optional-features","text":"The above process will get you a default installation, with no optional features or variables, that is entirely deployed in a single Availability Zone (AZ). In order to provide non-required variables, use multiple AZs, or make non-default selections for some options, you'll need to use some of the ops files in one of the following four directories: features allow the enabling of selectors for a product. For example, enabling/disabling of an s3 bucket network contains options for enabling 2-3 availability zones for network configuration optional contains optional properties without defaults. For optional values that can be provided more than once, there's an ops file for each param count resource contains configuration that can be applied to resource configuration. For example, BOSH vm extensions To use an ops file, add -o with the path to the ops file you want to use to your interpolate command. So, to enable TCP routing in PAS, you would add -o features/tcp_routing-enable.yml . For the rest of this guide, the vars for this feature are referred to as feature-vars.yml . If you run your complete command, you should again get a list of any newly-required variables. 1 2 3 4 5 6 7 8 9 docker run -it -v $HOME /configs:/configs platform-automation-image \\ om interpolate \\ --config product.yml \\ -l product-default-vars.yml \\ -l resource-vars.yml \\ -l required-vars.yml \\ -o features/tcp_routing-enable.yml \\ -l feature-vars.yml \\ -l errand-vars.yml","title":"Optional Features"},{"location":"configuration-management/creating-a-product-config-file.html#finalize-your-configuration","text":"Once you've selected your ops files and created your vars files, decide which vars you want in the template and which you want to have interpolated later. Create a final template and write it to a file, using only the vars you want to in the template, and using --skip-missing to allow the rest to remain as variables. 1 2 3 4 5 6 7 8 9 10 11 docker run -it -v $HOME /configs:/configs platform-automation-image \\ om interpolate \\ --config product.yml \\ -l product-default-vars.yml \\ -l resource-vars.yml \\ -l required-vars.yml \\ -o features/tcp_routing-enable.yml \\ -l feature-vars.yml \\ -l errand-vars.yml \\ --skip-missing \\ > pas-config-template.yml You can check-in the resulting configuration to a git repo. For vars that do not include credentials, you can check those vars files in, as well. Handle vars that are secret more carefully . You can then dispose of the config template directory.","title":"Finalize Your Configuration"},{"location":"configuration-management/creating-a-product-config-file.html#from-a-staged-product","text":"A configuration can be generated from a staged product on an already existing Ops Manager.","title":"From A Staged Product"},{"location":"configuration-management/creating-a-product-config-file.html#prerequisites_1","text":"To extract the configuration for a product, you will first need to do the following: Upload and stage your desired product(s) to a fully deployed Ops Manager. For example, let's use PAS on Vsphere Configure your product manually according to the product's install instructions .","title":"Prerequisites"},{"location":"configuration-management/creating-a-product-config-file.html#workflow_1","text":"om has a command called staged-config , which is used to extract staged product configuration present in the Ops Manager UI of the targeted foundation. Sample usage, using om directly and assuming the PAS product: om --env env.yml staged-config --include-placeholders --product-name cf > tile-config.yml Most products will contain the following high level keys: network-properties product-properties resource-config You can check the file in to git. For convenience, Platform Automation provides you with two ways to use the om staged-config command. The command can be run as a task inside of your pipeline. As an example of how to invoke this for the PAS product in your pipeline.yml(resources not listed): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 jobs : - name : staged-pas-config plan : - aggregate : - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : staged-config image : platform-automation-image file : platform-automation-tasks/tasks/staged-config.yml input_mapping : env : configuration params : PRODUCT_NAME : cf ENV_FILE : ((foundation))/env/env.yml SUBSTITUTE_CREDENTIALS_WITH_PLACEHOLDERS : true - put : configuration params : file : generated-config/cf.yml This task will connect to the Ops Manager defined in your env.yml , download the current staged configuration of your product, and put it into a generated-config folder in the concourse job. The put in concourse allows you to persist this config outside the concourse container. Alternatively, this can be run external to concourse by using docker. An example of how to do this using on the linux/mac command line: 1 2 3 docker import ${ PLATFORM_AUTOMATION_IMAGE_TGZ } platform-automation-image docker run -it --rm -v $PWD :/workspace -w /workspace platform-automation-image \\ om --env ${ ENV_FILE } staged-config --include-placeholders --product-name cf","title":"Workflow"},{"location":"configuration-management/creating-a-product-config-file.html#using-ops-files-for-multi-foundation","text":"--include-placeholders in the om command is a vital first step to externalizing your configuration for multiple foundations. This will search the Ops Manager product for fields marked as \"secrets\", and replace those values with ((placeholder_credentials)) . In order to fully support multiple foundations, however, a bit more work is necessary. There are two ways to do this: using secrets management or ops files. This section will explain how to support multiple foundations using ops files. Starting with an incomplete PAS config from vSphere as an example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # base.yml # An incomplete yaml response from om staged-config product-name : cf product-properties : .cloud_controller.apps_domain : value : ((cloud_controller_apps_domain)) .cloud_controller.encrypt_key : value : secret : ((cloud_controller_encrypt_key.secret)) .properties.security_acknowledgement : value : X .properties.cloud_controller_default_stack : value : default network-properties : network : name : DEPLOYMENT other_availability_zones : - name : AZ01 singleton_availability_zone : name : AZ01 resource-config : diego_cell : instances : 5 instance_type : id : automatic uaa : instances : 1 instance_type : id : automatic For a single foundation deploy, leaving values such as \".cloud_controller.apps_domain\" as-is would work fine. For multiple foundations, this value will be different per deployed foundation. Other values, such as .cloud_controller.encrypt_key have a secret that already have a placeholder from om . If different foundations have different load requirements, even the values in resource-config can be edited using ops files . Using the example above, let's try filling in the existing placeholder for cloud_controller.apps_domain in our first foundation. 1 2 3 4 # replace-domain-ops-file.yml - type : replace path : /product-properties/.cloud_controller.apps_domain/value? value : unique.foundation.one.domain To test that the ops file will work in your base.yml , this can be done locally using bosh int : 1 bosh int base.yml -o replace-domain.yml This will output base.yml with the replaced(interpolated) values: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # interpolated-base.yml network-properties : network : name : DEPLOYMENT other_availability_zones : - name : AZ01 singleton_availability_zone : name : AZ01 product-name : cf product-properties : .cloud_controller.apps_domain : unique.foundation.one.domain .cloud_controller.encrypt_key : value : secret : ((cloud_controller_encrypt_key.secret)) .properties.cloud_controller_default_stack : value : default .properties.security_acknowledgement : value : X resource-config : diego_cell : instance_type : id : automatic instances : 5 uaa : instance_type : id : automatic instances : 1 Anything that needs to be different per deployment can be replaced via ops files as long as the path: is correct. Upgrading products to new patch versions: Configuration settings should not differ between successive patch versions within the same minor version line. Underlying properties or property names may change, but the tile's upgrade process automatically translates properties to the new fields and values. Pivotal cannot guarantee the functionality of upgrade scripts in third-party PCF products. Replicating configuration settings from one product to the same product on a different foundation: Because properties and property names can change between patch versions of a product, you can only safely apply configuration settings across products if their versions exactly match.","title":"Using Ops Files for Multi-Foundation"},{"location":"configuration-management/secrets-handling.html","text":"Using Your Credhub Credhub can be used to store secure properties that you don't want committed into a config file. Within your pipeline, the config file can then reference that Credhub value for runtime evaluation. An example workflow would be storing an SSH key. Authenticate with your credhub instance. Generate an ssh key: credhub generate --name=\"/private-foundation/opsman_ssh_key\" --type=ssh Create an [Ops Manager configuration][opsman-configuration] file that references the name of the property. 1 2 3 opsman-configuration : azure : ssh_public_key : ((opsman_ssh_key.public_key)) Configure your pipeline to use the credhub-interpolate task. It takes an input files , which should contain your configuration file from (3). The declaration within a pipeline might look like: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 jobs : - name : example-job plan : - get : platform-automation-tasks - get : platform-automation-image - get : config - task : credhub-interpolate image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml input_mapping : files : config params : # depending on credhub configuration # ether CA cert or secret are required CREDHUB_CA_CERT : ((credhub_ca_cert)) CREDHUB_SECRET : ((credhub_secret)) # all required CREDHUB_CLIENT : ((credhub_client)) CREDHUB_SERVER : ((credhub_server)) PREFIX : /private-foundation SKIP_MISSING : true Notice the PREFIX has been set to /private-foundation , the path prefix defined for your cred in (2). This allows the config file to have values scoped, for example, per foundation. params should be filled in by the credhub created with your Concourse instance. Info You can set the param SKIP_MISSING:false to enforce strict checking of your vars files during intrpolation. This is true by default to support credential management from multiple sources. For more information, see the Multiple Sources section. This task will reach out to the deployed credhub and fill in your entry references and return an output named interpolated-files that can then be read as an input to any following tasks. Our configuration will now look like 1 2 3 opsman-configuration : azure : ssh_public_key : ssh-rsa AAAAB3Nz... Info If using this you need to ensure the concourse worker can talk to credhub so depending on how you deployed credhub and/or worker this may or may not be possible. This inverts control that now workers need to access credhub vs default is atc injects secrets and passes them to the worker. Storing values for Multi-foundation Credhub In the example above, bosh int did not replace the ((placeholder_credential)): ((cloud_controller_encrypt_key.secret)) . For security, values such as secrets and keys should not be saved off in static files (such as an ops file). In order to rectify this, you can use a secret management tool, such as Credhub, to sub in the necessary values to the deployment manifest. Let's assume basic knowledge and understanding of the credhub-interpolate task described in the [Secrets Handling][secrets handling] section of the documentation. For multiple foundations, credhub-interpolate will work the same, but PREFIX param will differ per foundation. This will allow you to keep your base.yml the same for each foundation with the same ((placeholder_credential)) reference. Each foundation will require a separate credhub-interpolate task call with a unique prefix to fill out the missing pieces of the template. Vars Files Vars files can be used for your Secrets Handling . Take the example below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # base.yml # An incomplete yaml response from om staged-config product-name : cf product-properties : .cloud_controller.apps_domain : value : ((cloud_controller_apps_domain)) .cloud_controller.encrypt_key : value : secret : ((cloud_controller_encrypt_key.secret)) .properties.security_acknowledgement : value : X .properties.cloud_controller_default_stack : value : default network-properties : network : name : DEPLOYMENT other_availability_zones : - name : AZ01 singleton_availability_zone : name : AZ01 resource-config : diego_cell : instances : 5 instance_type : id : automatic uaa : instances : 1 instance_type : id : automatic In our first foundation, we have the following vars.yml , optional for the configure-product task. 1 2 3 # vars.yml cloud_controller_encrypt_key.secret : super-secret-encryption-key cloud_controller_apps_domain : cfapps.domain.com The vars.yml could then be passed to configure-product with base.yml as the config file. The task will then sub the ((cloud_controller_encrypt_key.secret)) and ((cloud_controller_apps_domain)) specified in vars.yml and configure the product as normal. An example of how this might look in a pipeline(resources not listed): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 jobs : - name : configure-product plan : - aggregate : - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : configure-product image : platform-automation-image file : platform-automation-tasks/tasks/configure-product.yml input_mapping : config : configuration env : configuration vars : variable params : CONFIG_FILE : base.yml VARS_FILES : vars.yml ENV_FILE : env.yml Multiple Sources Both vars files and credhub may be used to interpolate variables into base.yml . Using the same example from above: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # base.yml # An incomplete yaml response from om staged-config product-name : cf product-properties : .cloud_controller.apps_domain : value : ((cloud_controller_apps_domain)) .cloud_controller.encrypt_key : value : secret : ((cloud_controller_encrypt_key.secret)) .properties.security_acknowledgement : value : X .properties.cloud_controller_default_stack : value : default network-properties : network : name : DEPLOYMENT other_availability_zones : - name : AZ01 singleton_availability_zone : name : AZ01 resource-config : diego_cell : instances : 5 instance_type : id : automatic uaa : instances : 1 instance_type : id : automatic We have one parametrized variable that is secret and might not want to have stored in a plain text vars file, ((cloud_controller_encrypt_key.secret)) , but ((cloud_controller_apps_domain)) is fine in a vars file. In order to support a base.yml with credentials from multiple sources (i.e. credhub and vars files), you will need to SKIP_MISSING: true in the credhub-interpolate task. This is enabled by default by the credhub-interpolate task. The workflow would be the same as Credhub , but when passing the interpolated base.yml as a config into the next task, you would add in a Vars File to fill in the missing variables. An example of how this might look in a pipeline (resources not listed), assuming: The ((base.yml)) above ((cloud_controller_encrypt_key.secret)) is stored in credhub ((cloud_controller_apps_domain)) is stored in director-vars.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 jobs : - name : example-credhub-interpolate plan : - get : platform-automation-tasks - get : platform-automation-image - get : config - task : credhub-interpolate image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml input_mapping : files : config params : # depending on credhub configuration # ether Credhub CA cert or Credhub secret are required CREDHUB_CA_CERT : ((credhub_ca_cert)) CREDHUB_SECRET : ((credhub_secret)) # all required CREDHUB_CLIENT : ((credhub_client)) CREDHUB_SERVER : ((credhub_server)) PREFIX : /private-foundation SKIP_MISSING : true - name : example-configure-director plan : - get : - task : configure-director image : platform-automation-image file : platform-automation-tasks/tasks/configure-director.yml params : VARS_FILES : vars/director-vars.yml ENV_FILE : env/env.yml DIRECTOR_CONFIG_FILE : config/director.yml Multiple Key Lookups in a Single Task When using Credhub in a single foundation or multi-foundation manner, we want to avoid duplicating identical credentials (duplication makes credential rotation harder). In order to have Credhub read in credentials from multiple paths (not relative to your PREFIX ), you must provide the absolute path to any credentials not in your relative path. For example, using an alternative base.yml : 1 2 3 4 5 6 7 8 9 10 11 12 13 # An incomplete yaml response from om staged-config product-name : cf product-properties : .cloud_controller.apps_domain : value : ((cloud_controller_apps_domain)) .cloud_controller.encrypt_key : value : secret : ((/alternate_prefix/cloud_controller_encrypt_key.secret)) .properties.security_acknowledgement : value : X .properties.cloud_controller_default_stack : value : default Let's say in our job , we define the prefix as \"foundation1\". The parameterized values in the example above will be interpolated as follows: ((cloud_controller_apps_domain)) uses a relative path for Credhub. When running credhub-interpolate , the task will prepend the PREFIX . This value is stored in Credhub as /foundation1/cloud_controller_apps_domain . ((/alternate_prefix/cloud_controller_encrypt_key.secret)) (note the leading slash) uses an absolute path for Credhub. When running credhub-interpolate , the task will not prepend the prefix. This value is stored in Credhub at it's absolute path /alternate_prefix/cloud_controller_encrypt_key.secret . Any value with a leading / slash will never use the PREFIX to look up values in Credhub. Therefore, you can have multiple key lookups in a single interpolate task.","title":"Secrets Handling"},{"location":"configuration-management/secrets-handling.html#using-your-credhub","text":"Credhub can be used to store secure properties that you don't want committed into a config file. Within your pipeline, the config file can then reference that Credhub value for runtime evaluation. An example workflow would be storing an SSH key. Authenticate with your credhub instance. Generate an ssh key: credhub generate --name=\"/private-foundation/opsman_ssh_key\" --type=ssh Create an [Ops Manager configuration][opsman-configuration] file that references the name of the property. 1 2 3 opsman-configuration : azure : ssh_public_key : ((opsman_ssh_key.public_key)) Configure your pipeline to use the credhub-interpolate task. It takes an input files , which should contain your configuration file from (3). The declaration within a pipeline might look like: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 jobs : - name : example-job plan : - get : platform-automation-tasks - get : platform-automation-image - get : config - task : credhub-interpolate image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml input_mapping : files : config params : # depending on credhub configuration # ether CA cert or secret are required CREDHUB_CA_CERT : ((credhub_ca_cert)) CREDHUB_SECRET : ((credhub_secret)) # all required CREDHUB_CLIENT : ((credhub_client)) CREDHUB_SERVER : ((credhub_server)) PREFIX : /private-foundation SKIP_MISSING : true Notice the PREFIX has been set to /private-foundation , the path prefix defined for your cred in (2). This allows the config file to have values scoped, for example, per foundation. params should be filled in by the credhub created with your Concourse instance. Info You can set the param SKIP_MISSING:false to enforce strict checking of your vars files during intrpolation. This is true by default to support credential management from multiple sources. For more information, see the Multiple Sources section. This task will reach out to the deployed credhub and fill in your entry references and return an output named interpolated-files that can then be read as an input to any following tasks. Our configuration will now look like 1 2 3 opsman-configuration : azure : ssh_public_key : ssh-rsa AAAAB3Nz... Info If using this you need to ensure the concourse worker can talk to credhub so depending on how you deployed credhub and/or worker this may or may not be possible. This inverts control that now workers need to access credhub vs default is atc injects secrets and passes them to the worker.","title":"Using Your Credhub"},{"location":"configuration-management/secrets-handling.html#storing-values-for-multi-foundation","text":"","title":"Storing values for Multi-foundation"},{"location":"configuration-management/secrets-handling.html#credhub","text":"In the example above, bosh int did not replace the ((placeholder_credential)): ((cloud_controller_encrypt_key.secret)) . For security, values such as secrets and keys should not be saved off in static files (such as an ops file). In order to rectify this, you can use a secret management tool, such as Credhub, to sub in the necessary values to the deployment manifest. Let's assume basic knowledge and understanding of the credhub-interpolate task described in the [Secrets Handling][secrets handling] section of the documentation. For multiple foundations, credhub-interpolate will work the same, but PREFIX param will differ per foundation. This will allow you to keep your base.yml the same for each foundation with the same ((placeholder_credential)) reference. Each foundation will require a separate credhub-interpolate task call with a unique prefix to fill out the missing pieces of the template.","title":"Credhub"},{"location":"configuration-management/secrets-handling.html#vars-files","text":"Vars files can be used for your Secrets Handling . Take the example below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # base.yml # An incomplete yaml response from om staged-config product-name : cf product-properties : .cloud_controller.apps_domain : value : ((cloud_controller_apps_domain)) .cloud_controller.encrypt_key : value : secret : ((cloud_controller_encrypt_key.secret)) .properties.security_acknowledgement : value : X .properties.cloud_controller_default_stack : value : default network-properties : network : name : DEPLOYMENT other_availability_zones : - name : AZ01 singleton_availability_zone : name : AZ01 resource-config : diego_cell : instances : 5 instance_type : id : automatic uaa : instances : 1 instance_type : id : automatic In our first foundation, we have the following vars.yml , optional for the configure-product task. 1 2 3 # vars.yml cloud_controller_encrypt_key.secret : super-secret-encryption-key cloud_controller_apps_domain : cfapps.domain.com The vars.yml could then be passed to configure-product with base.yml as the config file. The task will then sub the ((cloud_controller_encrypt_key.secret)) and ((cloud_controller_apps_domain)) specified in vars.yml and configure the product as normal. An example of how this might look in a pipeline(resources not listed): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 jobs : - name : configure-product plan : - aggregate : - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : configure-product image : platform-automation-image file : platform-automation-tasks/tasks/configure-product.yml input_mapping : config : configuration env : configuration vars : variable params : CONFIG_FILE : base.yml VARS_FILES : vars.yml ENV_FILE : env.yml","title":"Vars Files"},{"location":"configuration-management/secrets-handling.html#multiple-sources","text":"Both vars files and credhub may be used to interpolate variables into base.yml . Using the same example from above: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # base.yml # An incomplete yaml response from om staged-config product-name : cf product-properties : .cloud_controller.apps_domain : value : ((cloud_controller_apps_domain)) .cloud_controller.encrypt_key : value : secret : ((cloud_controller_encrypt_key.secret)) .properties.security_acknowledgement : value : X .properties.cloud_controller_default_stack : value : default network-properties : network : name : DEPLOYMENT other_availability_zones : - name : AZ01 singleton_availability_zone : name : AZ01 resource-config : diego_cell : instances : 5 instance_type : id : automatic uaa : instances : 1 instance_type : id : automatic We have one parametrized variable that is secret and might not want to have stored in a plain text vars file, ((cloud_controller_encrypt_key.secret)) , but ((cloud_controller_apps_domain)) is fine in a vars file. In order to support a base.yml with credentials from multiple sources (i.e. credhub and vars files), you will need to SKIP_MISSING: true in the credhub-interpolate task. This is enabled by default by the credhub-interpolate task. The workflow would be the same as Credhub , but when passing the interpolated base.yml as a config into the next task, you would add in a Vars File to fill in the missing variables. An example of how this might look in a pipeline (resources not listed), assuming: The ((base.yml)) above ((cloud_controller_encrypt_key.secret)) is stored in credhub ((cloud_controller_apps_domain)) is stored in director-vars.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 jobs : - name : example-credhub-interpolate plan : - get : platform-automation-tasks - get : platform-automation-image - get : config - task : credhub-interpolate image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml input_mapping : files : config params : # depending on credhub configuration # ether Credhub CA cert or Credhub secret are required CREDHUB_CA_CERT : ((credhub_ca_cert)) CREDHUB_SECRET : ((credhub_secret)) # all required CREDHUB_CLIENT : ((credhub_client)) CREDHUB_SERVER : ((credhub_server)) PREFIX : /private-foundation SKIP_MISSING : true - name : example-configure-director plan : - get : - task : configure-director image : platform-automation-image file : platform-automation-tasks/tasks/configure-director.yml params : VARS_FILES : vars/director-vars.yml ENV_FILE : env/env.yml DIRECTOR_CONFIG_FILE : config/director.yml","title":"Multiple Sources"},{"location":"configuration-management/secrets-handling.html#multiple-key-lookups-in-a-single-task","text":"When using Credhub in a single foundation or multi-foundation manner, we want to avoid duplicating identical credentials (duplication makes credential rotation harder). In order to have Credhub read in credentials from multiple paths (not relative to your PREFIX ), you must provide the absolute path to any credentials not in your relative path. For example, using an alternative base.yml : 1 2 3 4 5 6 7 8 9 10 11 12 13 # An incomplete yaml response from om staged-config product-name : cf product-properties : .cloud_controller.apps_domain : value : ((cloud_controller_apps_domain)) .cloud_controller.encrypt_key : value : secret : ((/alternate_prefix/cloud_controller_encrypt_key.secret)) .properties.security_acknowledgement : value : X .properties.cloud_controller_default_stack : value : default Let's say in our job , we define the prefix as \"foundation1\". The parameterized values in the example above will be interpolated as follows: ((cloud_controller_apps_domain)) uses a relative path for Credhub. When running credhub-interpolate , the task will prepend the PREFIX . This value is stored in Credhub as /foundation1/cloud_controller_apps_domain . ((/alternate_prefix/cloud_controller_encrypt_key.secret)) (note the leading slash) uses an absolute path for Credhub. When running credhub-interpolate , the task will not prepend the prefix. This value is stored in Credhub at it's absolute path /alternate_prefix/cloud_controller_encrypt_key.secret . Any value with a leading / slash will never use the PREFIX to look up values in Credhub. Therefore, you can have multiple key lookups in a single interpolate task.","title":"Multiple Key Lookups in a Single Task"},{"location":"how-to-guides/brainDump.html","text":"How to: Upgrade an Existing Ops Manager The following is a How To Guide on setting up and using Platform Automation. This guide assumes you already have a foundation that needs to be automated, or you are coming from a different form of automation (such as pcf-pipelines ) Prerequisites In addition to the prerequisites listed in Downloading and Testing , the Platform Automation team recommends the following: Installed Docker CLI There are a couple one-off tasks that can be either saved in your pipeline, or run once from the command line using our Docker image. The preference to do either is your choice, but the How To Guide will be using the Docker CLI. Basic knowledge of Git , and a GitHub account. Several tasks mutate state and configuration files that are best persisted in a remote version control system. For the purposes of this guide, we'll use GitHub. Amazon S3 or Minio While any blobstore may be used, this How To Guide will be using an s3-compatible blobstore. A fully installed foundation (either PAS or PKS) with all relevant tiles similarly configured and installed Warning Upgrading Ops Manager requires that your foundation have no pending apply-changes. The exported installation will not reflect any pending changes, and will not export at all if the foundation has not fully installed at least the Ops Manager BOSH director. Goals and Overview The goal of this How To Guide is to build up a portion of the Reference Pipeline and the Reference Resources Pipeline that is relevant for upgrading Ops Manager for an already existing foundation. This guide will go through the following steps: Retrieving Ops Manager, PAS, Healthwatch, and Platform Automation from Pivnet and storing in an s3 blobstore How to interpolate the configs using credhub, and feeding these interpolated configs into the concourse tasks How to setup a sample github repo Setup recommended file structure Create required files for Upgrade Retrieve the existing config from Ops Manager using docker run Retrieve the existing config from PAS and Healthwatch using docker run Create a pipeline to upgrade Ops Manager TODO: link the above to the headers below (after design review) Retrieving Resources from Pivnet When creating a Concourse pipeline, we will expand the following base structure: 1 2 3 resource_types : resources : jobs : Concourse has many Resource Types built in, but for the purpose of this reference pipeline, we will be utilizing the Pivotal pivnet-resource to directly communicate with and download products from Pivnet. To tell concourse that we will be using the Pivnet resource, we will have to include the following in your resource_types section: 1 2 3 4 5 - name : pivnet type : docker-image source : repository : pivotalcf/pivnet-resource tag : latest-final After listing the \"custom\" resource type, we can then list the resources that our foundation requires. For this guide, our foundation includes: Ops Manager, Pivotal Application Service (PAS), and Healthwatch. The general automated workflow for fetching and storing resources is as follows: TODO: link to the appropriate sections (after design review) Setup your s3 with the appropriate credentials/buckets for your foundation Download product and stemcell (using download-product ) and store in an s3 bucket Download Platform Automation from Pivnet The resources required to accomplish these tasks include: healthwatch-product s3 storage location healthwatch-stemcell s3 storage location ops-manager s3 storage location pas-product s3 storage location pas-stemcell s3 storage location platform-automation-docker-image s3 storage location platform-automation-tasks storage location To reference the storage locations in Concourse, you are required to have the access_key_id and secret_access_key for accessing the appropriate bucket the name of the bucket for storing the product(s) the region the bucket is located in a regex that describes the filename of the product being stored (to prevent the wrong product/version from being stored/accessed later) The following example assumes that all of your products live in the same s3 bucket, thus their regexp are very specific to match the slug/version. To retrieve the platform-automation product, there is no stemcell, and thus the download process is much simpler than with Ops Manager products. Therefore, we can use the Pivnet resource we defined earlier and pull from Pivnet directly. The product can be stored in s3 the same way that the other products can. We can add all of these resources to our pipeline under the resources section: Healthwatch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 - name : healthwatch-product type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : \\[p-healthwatch,(.*)\\]p-healthwatch-.*.pivotal - name : healthwatch-stemcell type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : healthwatch-stemcell/\\[stemcells-ubuntu-xenial,(.*)\\]bosh-stemcell-.*-vsphere.*\\.tgz PAS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 - name : pas-product type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : \\[elastic-runtime,(.*)\\]cf-.*.pivotal - name : pas-stemcell type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : pas-stemcell/\\[stemcells-ubuntu-xenial,(.*)\\]bosh-stemcell-.*-vsphere.*\\.tgz Ops Manager 1 2 3 4 5 6 7 8 - name : opsman-product type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : \\[ops-manager,(.*)\\].*.ova Platform Automation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 - name : platform-automation-pivnet type : pivnet source : api_token : ((pivnet_token)) product_slug : platform-automation product_version : 2\\.(.*) sort_by : semver - name : platform-automation-tasks type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : platform-automation-tasks-(.*).zip - name : platform-automation-image type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : platform-automation-image-(.*).tgz Parametrizing Secrets, and Using Credhub Interpolate This example pipeline will make heavy use of the credhub-interpolate task. For more information on how this works, and how to set it up and use it properly, please see the Secrets Handling page. The config file used in the following section mixes secret and non-secret variables. When choosing which variables to keep in the config file, and which ones to ((parametrize)) , you should consider whether public access to the variable would be a concern. If choosing to parametrize, you will need to first use credhub-interpolate to substitute the Credhub values into the config for the next task to use. Info Parametrized configurations that are interpolated by Credhub return a config file with the formerly parametrized variables with their Credhub values. Concourse VMs are ephemeral, and these full config files are only available in the specific job, and will not be persisted. An example of how to use this in the resources pipeline is shown below. We will be defining this \"task\" external to jobs and resources, so that it can be used in multiple jobs while keeping the yaml clean. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 resource-types : resources : credhub-interpolate : &credhub-interpolate image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : '/pipeline/vsphere' INTERPOLATION_PATHS : \"download-product-configs\" input_mapping : files : config output_mapping : interpolated-files : config jobs : When referencing the above \"task\" we will be calling it with the yaml below. This will expand the anchor *credhub-interpolate with the concourse-readable data we defined in &credhub-interpolate above. 1 2 - task : credhub-interpolate << : *credhub-interpolate Download Product and Product's Stemcell Before downloading a product, you need a config file for download-product . Commented out fields are entirely optional and should only be used if you have a need to do so. Explanations for each field are given below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 --- pivnet-api-token : ((pivnet-api-token)) ## Note that file globs must be quoted if they start with *; ## otherwise they'll be interpreted as a YAML anchor. pivnet-file-glob : \"*.pivotal\" pivnet-product-slug : product-slugs ## Either product-version OR product-version-regex is required # product-version: 1.2.3 ## Note that the regex mustn't be quoted, ## as escape characters for the regex will confuse yaml parsers. product-version-regex : ^1\\.2\\..*$ stemcell-iaas : google ## The following are required only if using download-product-s3. ## Any key marked required above is still required when using S3. ## If s3-bucket is set, ## downloaded product files will have their slug and version prepended. s3-bucket : s3-bucket s3-region-name : us-west-1 # required; sufficient for AWS s3-endpoint : s3.endpoint.com # if not using AWS, this is required ## Required unless `s3-auth-method` is `iam` s3-access-key-id : ((s3-access-key-id)) s3-secret-access-key : ((s3-secret-access-key)) To fetch a product from Pivnet, concourse needs to know: what image it will run the task on ( platform-automation-image ) where the task file will come from ( platform-automation-tasks ) what config file it will read from to get data about pivnet and the tile (this is the download-product-config created above) how to map the output from the task to something you will use later where to put the output resources created in the task These requirements gathered together and executed in a task could look like the snippet below. The snippet involves downloading Healthwatch. However, Healthwatch can be easily replaced by any other tile. The only pieces that would need to change are the task name (if being specific), the name of the stemcell (if mapping), the name of the download-product-config , and the put 's specified after the product and stemcell are downloaded. Healthwatch 1 2 3 4 5 6 7 8 9 10 11 12 13 - task : download-healthwatch-product-and-stemcell image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : download-product-configs/healthwatch.yml output_mapping : { downloaded-stemcell : healthwatch-stemcell } - aggregate : - put : healthwatch-product params : file : downloaded-product/*.pivotal - put : healthwatch-stemcell params : file : healthwatch-stemcell/*.tgz PAS 1 2 3 4 5 6 7 8 9 10 11 12 13 - task : download-pas-product-and-stemcell image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : download-product-configs/pas.yml output_mapping : { downloaded-stemcell : pas-stemcell } - aggregate : - put : pas-product params : file : downloaded-product/*.pivotal - put : pas-stemcell params : file : pas-stemcell/*.tgz Concourse requires tasks to be within jobs. For convenience, and ease of explanation, we have created a separate job for each product. These tasks can easily be combined into a single job. Benefits of doing this could include running credhub-interpolate only once, (instead of once for each job). One downside of structuring your pipeline with many tasks in a given job is that you lose the ability to rerun just a particular section of the job that failed. This means Concourse would run every task again if the job was triggered a second time. To make sure your blobstore always has the most recent version of a pivnet product, you can use the built-in time resource, to tell the fetch jobs how often to run and attempt to download a new version of the product and/or stemcell. To add this functionality to your pipeline, you must include the time resource in your resources section: 1 2 3 4 - name : daily-trigger type : time source : interval : 24h If included, this resource can be referenced in any appropriate job, and you can set the job to trigger on that daily (or custom) interval. Examples of the fetch-{product} job are shown below. The job includes the task we created above, the daily time trigger, and the interpolate created in an earlier step . These jobs should be included under the jobs header in your pipeline. Healthwatch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 - name : fetch-healthwatch plan : - aggregate : - get : daily trigger : true - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : config - task : credhub-interpolate << : *credhub-interpolate - task : download-healthwatch-product-and-stemcell image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : download-product-configs/healthwatch.yml output_mapping : { downloaded-stemcell : healthwatch-stemcell } - aggregate : - put : healthwatch-product params : file : downloaded-product/*.pivotal - put : healthwatch-stemcell params : file : healthwatch-stemcell/*.tgz PAS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 - name : fetch-pas plan : - aggregate : - get : daily trigger : true - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : config - task : credhub-interpolate << : *credhub-interpolate - task : download-pas-product-and-stemcell image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : download-product-configs/pas.yml output_mapping : { downloaded-stemcell : pas-stemcell } - aggregate : - put : pas-product params : file : downloaded-product/*.pivotal - put : pas-stemcell params : file : pas-stemcell/*.tgz Ops Manager 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 - name : fetch-opsman plan : - aggregate : - get : daily trigger : true - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : config - task : credhub-interpolate << : *credhub-interpolate - task : download-opsman-image image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : download-product-configs/opsman.yml - aggregate : - put : opsman-product params : file : downloaded-product/* Download Platform Automation from Pivnet Downloading the Platform Automation toolset does not use download-product . Using download-product for this would require the product have a dependency on itself. Instead, we use the pivnet-resource we defined earlier. The Platform Automation team recommends triggering fetch-platform-automation whenever there is a new version available. The resource definition for platform automation from above is pinned to the major version. Pinning to the major version but letting the minor and patch version increase allows you to get security updates, bug fixes, and new (non-breaking) features. Platform Automation uses semantic versioning to enable this. To download the Platform Automation tasks and the Docker image, and put it into your s3 blobstore, add the following job : 1 2 3 4 5 6 7 8 9 10 11 - name : fetch-platform-automation plan : - get : platform-automation-pivnet trigger : true - aggregate : - put : platform-automation-tasks params : file : platform-automation-pivnet/*tasks*.zip - put : platform-automation-image params : file : platform-automation-pivnet/*image*.tgz A Complete Resources Pipeline Now that we have built up the resources pipeline, you can find this full example on the Reference Pipeline page. This also includes an example of fetching a Windows tile, but if you understand the concepts above, you can use the Windows tile, the mySQL tile, or any other tile you desire for your foundation. Sample Github repository and file structure Now let's dive into using version control to manage state in pipelines built with Platform Automation. We'll set up a git repository on Github, and the recommended directory structure for the repo. Git and Github Git is a commonly used version control tool. It can be used to track code changes made to files within a repository (or \"repo\"). Changes can then be \"pushed\" to or \"pulled\" from remote copies of that repository. Github is a system that provides git remotes. Using a remote will enable the pipeline we are creating to access and update the state and configuration files. To learn more about git and github, you can read this short git handbook . To create our Github repo: Create a new repository Using the \"Example: Start a new repository and publish it to GitHub\" section of the Git handbook (about 3/4 down the page), create a repo, add a file, and push it to Github. Creating Repo Directory Structure You now have both a local git repo and a remote on Github. The recommended structure for a config repo is: 1 2 3 4 5 \u251c\u2500\u2500 foundation \u2502 \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 env \u2502 \u251c\u2500\u2500 state \u2502 \u2514\u2500\u2500 vars These directories are needed for this guide. config Holds config files for the products installed on your foundation. If using Credhub and/or vars files, these config files should have your ((parametrized)) values present in them env Holds env.yml , the environment file used by tasks that interact with Ops Manager. vars Holds product-specific vars files. state Holds state.yml , which contains the VM ID for the Ops Manager VM. Creating the Required Files Several files are required for upgrading an Ops Manager VM. They are used to capture the current state of the Ops Manager VM. Ops Manager VM State We'll need to capture the current Ops Manager VM identifier, so we know what VM we are upgrading. To create a state.yml from your existing foundation, use the following as a template, based on your IaaS: AWS 1 2 3 iaas : aws # Instance ID of the AWS VM vm_id : i-12345678987654321 Azure 1 2 3 iaas : azure # Computer Name of the Azure VM vm_id : vm_name GCP 1 2 3 iaas : gcp # Name of the VM in GCP vm_id : vm_name OpenStack 1 2 3 iaas : openstack # Instance ID from the OpenStack Overview vm_id : 12345678-9876-5432-1abc-defghijklmno vSphere 1 2 3 iaas : vsphere # Path to the VM in vCenter vm_id : /datacenter/vm/folder/vm_name Ops Manager VM Configuration opsman.yml is the configuration file for p-automator , which calls out to specific IaaS CLIs in order to create/update/delete a VM. The properties contained in opsman.yml differ based on your IaaS: AWS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # code_snippet aws-configuration start yaml --- opsman-configuration : aws : region : us-west-2 vm_name : ops-manager-vm # defaults Ops Manager-vm boot_disk_size : 100 # default 200 vpc_subnet_id : subnet-0292bc845215c2cbf security_group_id : sg-0354f804ba7c4bc41 key_pair_name : ops-manager-key iam_instance_profile_name : ops-manager-iam instance_type : m5.large # At least one IP address (public or private) # needs to be assigned to the VM. # It is also permissable to assign both. public_ip : 1.2.3.4 private_ip : 10.0.0.2 # Required if use_instance_profile is false # omit if using Instance Profiles access_key_id : sample-access-id secret_access_key : sample-secret-access-key # If using Instance Profiles (omit if using AWS Credentials) use_instance_profile : true # default false # Optional, necessary if a role is needed to authorize the instance profile assume_role : arn:aws:iam::123456789:role/test # code_snippet aws-configuration end Azure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # code_snippet azure-configuration start yaml --- opsman-configuration : azure : subscription_id : 90f35f10-ea9e-4e80-aac4-d6778b995532 resource_group : res-group tenant_id : 3e52862f-a01e-4b97-98d5-f31a409df682 client_id : 5782deb6-9195-4827-83ae-a13fda90aa0d client_secret : 6Iaue71Lqxfq location : westus container : opsmanagerimage # container for opsman image network_security_group : ops-manager-security-group # Note that there are several environment-specific details in this path vpc_subnet : /subscriptions/<MY_SUBSCRIPTION_ID>/resourceGroups/<MY_RESOURCE_GROUP>/providers/Microsoft.Network/virtualNetworks/<MY_VNET>/subnets/<MY_SUBNET> storage_account : opsman # account name of container # Optional # only needed if your client doesn't have the needed storage permissions storage_key : pEuXDaDK/WWo... ssh_public_key : ssh-rsa AAAAB3NzaC1yc2EAZ... # ssh key to access VM vm_name : ops-manager-vm # default : Ops Manager-vm boot_disk_size : 100 # default : 200 cloud_name : AzureCloud # default : AzureCloud # This flag is only respected by the create-vm & upgrade-opsman commands # set to true if you want to create the new opsman vm with unmanaged disk # delete-vm discovers the disk type from the VM use_unmanaged_disk : false # At least one IP address (public or private) # needs to be assigned to the VM. # It is also permissable to assign both. public_ip : 1.2.3.4 private_ip : 10.0.0.3 # code_snippet azure-configuration end GCP 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # code_snippet gcp-configuration start yaml --- opsman-configuration : gcp : gcp_service_account : | { \"type\": \"service_account\", \"project_id\": \"project-id\", \"private_key_id\": \"af719b1ca48f7b6ac67ca9c5319cb175\", \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n...\\n-----END PRIVATE KEY-----\\n\", \"client_email\": \"user@project-id.iam.gserviceaccount.com\", \"client_id\": \"1234567890\", \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\", \"token_uri\": \"https://accounts.google.com/o/oauth2/token\", \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\", \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/user%40project-id.iam.gserviceaccount.com\" } project : project-id region : us-central1 zone : us-central1-b vm_name : ops-manager-vm # default : Ops Manager-vm # For SharedVPC: projects/[HOST_PROJECT_ID]/regions/[REGION]/subnetworks/[SUBNET] vpc_subnet : infrastructure-subnet tags : ops-manager # This CPU, Memory and disk size demonstrated here # match the defaults, and needn't be included if these are the desired values custom_cpu : 2 custom_memory : 8 boot_disk_size : 100 # At least one IP address (public or private) needs to be assigned to the VM. public_ip : 1.2.3.4. private_ip : 10.0.0.2 # code_snippet gcp-configuration end OpenStack 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # code_snippet openstack-configuration start yaml --- opsman-configuration : openstack : auth_url : http://os.example.com:5000/v2.0 project_name : project net_id : 26a13112-b6c2-11e8-96f8-529269fb1459 username : admin password : password key_pair_name : opsman-keypair security_group_name : opsman-sec-group vm_name : ops-manager-vm # default : Ops Manager-vm # At least one IP address (public or private) needs to be assigned to the VM. public_ip : 1.2.3.4 private_ip : 10.0.0.3 flavor : m1.xlarge # default : m1.xlarge project_domain_name : default user_domain_name : default identity_api_version : 3 # default : 2 insecure : true # default : false availability_zone : zone-01 # code_snippet openstack-configuration end vSphere 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # code_snippet vsphere-configuration start yaml --- opsman-configuration : vsphere : vcenter : url : vcenter.example.com username : admin password : password datastore : exmple-ds-1 ca_cert : certificate host : example-host # vCenter host to deploy Ops Manager in datacenter : example-dc resource_pool : /example-dc/host/example-host/Resources/ResPool # or /<Data Center Name>/host/<Cluster Name> folder : /example-dc/vm/Folder insecure : 1 # default : 0 (secure); 1 (insecure) disk_type : thin # example : thin|thick private_ip : 10.0.0.2 dns : 8.8.8.8 ntp : ntp.example.com # example : ntp.ubuntu.com ssh_password : password ssh_public_key : ssh-rsa ...... # for Ops Manager >= 2.3 hostname : pcf.example.com network : virtual-network # vcenter network to deploy to netmask : 255.255.255.192 gateway : 192.168.10.1 vm_name : Ops_Manager # default : Ops_Manager memory : 8 # default : 8 GB cpu : 1 # default : 1 # code_snippet vsphere-configuration end Ops Manager Environment File env.yml holds authentication and target information for a particular Ops Manager. This file is required by upgrade-opsman because after the VM is recreated, the task will import the provided installation.zip to Ops Manager to finish the process. If your foundation uses authentication other than basic auth, please reference Inputs and Outputs for more detail on UAA-based authentication. An example env.yml is shown below. As mentioned in the comment, decryption-passphrase is required for import-installation , and therefore required for upgrade-opsman . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- target : https://pcf.example.com connect-timeout : 30 # default 5 request-timeout : 1800 # default 1800 skip-ssl-validation : false # default false username : username password : password # decryption-passphrase is optional, # except for use with `import-installation`. # OpsMan depends on the passphrase # to decrypt the imported installation. # For other commands, providing this key allows # decryption of the OpsMan VM after reboot, # which would otherwise need to be done manually. decryption-passphrase : passphrase Vars files If you are using files to store vars be rendered into your configuration templates, these files should be in your git repo under the vars directory. For more information on vars files, see Secrets Handling . Valid exported Ops Manager installation upgrade-opsman will not allow you to execute the task unless the installation provided to the task is a installation provided by Ops Manager itself. In the UI, this is located on the Settings Page of Ops Manager. Platform Automation strongly recommends automatically exporting and persisting the Ops Manager installation on a regular basis. In order to do so, you can set your pipeline to run the export-installation task on a daily trigger. This should be persisted into S3 or a blobstore of your choice. You can start your pipeline by first creating this export-installation task and persisting it in an S3 bucket. Warning It is recommended to persist the zip file exported from export-installation to an external file store (eg S3) on a regular basis. The exported installation can restore the Ops Manager to a working state if it is non-functional. Requirements for this task include: the Platform Automation image the Platform Automation tasks a configuration path for your env file interpolation of the env file with credhub a resource to store the exported installation into Starting our concourse pipeline, we need the following resources: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 - name : platform-automation-tasks type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : .*tasks-(.*).zip - name : platform-automation-image type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : .*image-(.*).tgz - name : configuration type : git source : private_key : ((configuration.private_key)) uri : ((configuration.uri)) branch : master - name : installation type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.installation)) regexp : installation-(.*).zip In our jobs section, we need a job that will trigger daily to pull down the Ops Manager installation and store it in S3. This looks like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 - name : export-installation serial : true plan : - aggregate : - get : daily-trigger trigger : true - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : interpolate-env-creds image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : '/pipeline/vsphere' INTERPOLATION_PATHS : ((foundation))/config SKIP_MISSING : true input_mapping : files : configuration output_mapping : interpolated-files : interpolated-configs - task : export-installation image : platform-automation-image file : platform-automation-tasks/tasks/export-installation.yml input_mapping : env : interpolated-env params : ENV_FILE : ((foundation))/env/env.yml INSTALLATION_FILE : installation-$timestamp.zip - put : installation params : file : installation/installation*.zip Once this resource is persisted, we can safely run upgrade-opsman , knowing that we can never truly lose our foundation. This is also important in case something happens to the VM externally (whether accidentally deleted, or a similar disaster occurs). If something does happen to the original Ops Manager VM, this installation can be imported by any newly created Ops Manager VM. Warning It is recommended to persist the zip file exported from export-installation to an external file store (eg S3) on a regular basis. The exported installation can restore the Ops Manager to a working state if it is non-functional. Retrieving Existing Ops Manager Director Configuration If you would like to automate the configuration of your Ops Manager, you first need to externalize the director configuration. Using Platform Automation, this is done using Docker or by adding a job to the pipeline. Docker To get the currently configured Ops Manager configuration, we have to: Import the image 1 docker import ${ PLATFORM_AUTOMATION_IMAGE_TGZ } platform-automation-image Where ${PLATFORM_AUTOMATION_IMAGE_TGZ} is the image file downloaded from Pivnet. Then, you can use docker run to pass it arbitrary commands. Here, we're running the om CLI to see what commands are available: 1 2 docker run -it --rm -v $PWD :/workspace -w /workspace platform-automation-image \\ om -h Note: that this will have access read and write files in your current working directory. If you need to mount other directories as well, you can add additional -v arguments. The command we will use to extract the current director configuration is called staged-director-config . This is an om command that calls the Ops Manager API to pull down the currently configured director configuration. To run this using Docker, you will need the env file created above as ${ENV_FILE} : 1 2 docker run -it --rm -v $PWD :/workspace -w /workspace platform-automation-image \\ om --env ${ ENV_FILE } staged-director-config --include-placeholders --include-placeholders is an optional flag, but highly recommended if you want a full configuration for your Ops Manager. This flag will replace any fields marked as \"secret\" in your Ops Manager config with ((parametrized)) variables. If you would prefer to not work with ((parametrized)) variables, you can substitute --include-placeholders with --include-credentials . Warning --include-credentials WILL expose passwords and secrets in plain text . Therefore, --include-placeholders is recommended, but not required. Pipeline To add [ staged-director-config ] to your pipeline, you will need the following resources: the Platform Automation image the Platform Automation tasks a configuration path for your env file a resource to store the exported configuration into Starting our Concourse pipeline, we need the following resources : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 - name : platform-automation-tasks type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : .*tasks-(.*).zip - name : platform-automation-image type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : .*image-(.*).tgz - name : configuration type : git source : private_key : ((configuration.private_key)) uri : ((configuration.uri)) branch : master In our jobs section, we need a job that will interpolate the env file, pull down the Ops Manager director config, and store the director config in the configuration directory (this can be the same resource as where the env is located, but will be stored in the config instead of the env directory). In order to persist the director config in your git repo, we first need to make a commit, detailing the change we made, and where in your git repo the change happened. A way to do this is shown below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 - name : staged-director-config plan : - aggregate : - get : platform-automation-tasks params : { unpack : true } - get : platform-automation-image params : { unpack : true } - get : configuration - task : interpolate-env-creds image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : '/pipeline/vsphere' INTERPOLATION_PATHS : ((foundation))/config SKIP_MISSING : true input_mapping : files : configuration output_mapping : interpolated-files : interpolated-configs - task : staged-director-config image : platform-automation-image file : platform-automation-tasks/tasks/staged-director-config.yml input_mapping : env : interpolated-env output_mapping : generated-config : configuration/((foundation))/config params : ENV_FILE : ((foundation))/env/env.yml - task : make-commit image : platform-automation-image file : platform-automation-tasks/tasks/make-git-commit.yml input_mapping : repository : configuration file-source : configuration/((foundation))/config output_mapping : repository-commit : configuration-commit params : FILE_SOURCE_PATH : director.yml FILE_DESTINATION_PATH : config/((foundation))/director.yml GIT_AUTHOR_EMAIL : \"git-author-email@example.com\" GIT_AUTHOR_NAME : \"Git Author\" COMMIT_MESSAGE : \"Update director.yml file\" - put : configuration params : repository : configuration-commit merge : true Retrieving Existing Product Configurations If you would like to automate the configuration of your products, you first need to externalize each product's configuration. Using Platform Automation, this is done using Docker or by adding a job to the pipeline. Docker As an example, we are going to start with the PAS tile. To get the currently configured PAS configuration, we have to: Import the image 1 docker import ${ PLATFORM_AUTOMATION_IMAGE_TGZ } platform-automation-image Where ${PLATFORM_AUTOMATION_IMAGE_TGZ} is the image file downloaded from Pivnet. Then, you can use docker run to pass it arbitrary commands. Here, we're running the om CLI to see what commands are available: 1 2 docker run -it --rm -v $PWD :/workspace -w /workspace platform-automation-image \\ om -h Note: that this will have access read and write files in your current working directory. If you need to mount other directories as well, you can add additional -v arguments. The command we will use to extract the current director configuration is called staged-config . This is an om command that calls the Ops Manager API to pull down the currently configured product configuration given a product slug. To run this using Docker, you will need the env file created above as ${ENV_FILE} . The product slug for the PAS tile, within Ops Manager, is cf . To find the slug of your product, you can run the following docker command: 1 2 docker run -it --rm -v $PWD :/workspace -w /workspace platform-automation-image \\ om --env ${ ENV_FILE } staged-products This will give you a table like the following: 1 2 3 4 5 6 7 + ---------------+-----------------+ | NAME | VERSION | + ---------------+-----------------+ | cf | 2 . x . x | | p - healthwatch | 1 . x . x - build . x | | p - bosh | 2 . x . x - build . x | + ---------------+-----------------+ The values in the NAME column are the slugs of each product you have deployed. For this How to Guide , we only have PAS and Healthwatch. Info p-bosh is the product slug of Ops Manager. However, staged-config cannot be used to extract the director config. To do so, you must use staged-director-config With the appropriate product ${SLUG} , we can run the following docker command to pull down the configuration of the chosen tile: 1 2 docker run -it --rm -v $PWD :/workspace -w /workspace platform-automation-image \\ om --env ${ ENV_FILE } staged-config --product-name ${ SLUG } --include-placeholders --include-placeholders is an optional flag, but highly recommended if you want a full configuration for your tile. This flag will replace any fields marked as \"secret\" by the product in the config with ((parametrized)) variables. If you would prefer to not work with ((parametrized)) variables, you can substitute --include-placeholders with --include-credentials . Warning --include-credentials WILL expose passwords and secrets in plain text . Therefore, --include-placeholders is recommended, but not required. Pipeline To add [ staged-config ] to your pipeline, you will need the following resources: the Platform Automation image the Platform Automation tasks a configuration path for your env file a resource to store the exported configuration into Starting our Concourse pipeline, we need the following resources: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 - name : platform-automation-tasks type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : .*tasks-(.*).zip - name : platform-automation-image type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : .*image-(.*).tgz - name : configuration type : git source : private_key : ((configuration.private_key)) uri : ((configuration.uri)) branch : master In our jobs section, we need a job that will interpolate the env file, pull down the product config, and store the director config in the configuration directory (this can be the same resource as where the env is located, but will be stored in the config instead of the env directory). In order to persist the product config in your git repo, we first need to make a commit, detailing the change we made, and where in your git repo the change happened. A way to do this is shown below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 - name : staged-director-config plan : - aggregate : - get : platform-automation-tasks params : { unpack : true } - get : platform-automation-image params : { unpack : true } - get : configuration - task : interpolate-env-creds image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : '/pipeline/vsphere' INTERPOLATION_PATHS : ((foundation))/config SKIP_MISSING : true input_mapping : files : configuration output_mapping : interpolated-files : interpolated-configs - task : staged-director-config image : platform-automation-image file : platform-automation-tasks/tasks/staged-director-config.yml input_mapping : env : interpolated-env output_mapping : generated-config : configuration/((foundation))/config params : ENV_FILE : ((foundation))/env/env.yml - task : make-commit image : platform-automation-image file : platform-automation-tasks/tasks/make-git-commit.yml input_mapping : repository : configuration file-source : configuration/((foundation))/config output_mapping : repository-commit : configuration-commit params : FILE_SOURCE_PATH : director.yml FILE_DESTINATION_PATH : config/((foundation))/director.yml GIT_AUTHOR_EMAIL : \"git-author-email@example.com\" GIT_AUTHOR_NAME : \"Git Author\" COMMIT_MESSAGE : \"Update director.yml file\" - put : configuration params : repository : configuration-commit merge : true To retrieve the configuration for Healthwatch, we can simply duplicate the steps used for PAS. The ${SLUG} for Healthwatch, as we retrieved from staged-products , is p-healthwatch Creating a Pipeline to Upgrade Ops Manager With the director configuration, product configurations, resources gathered, and config files created, we can finally begin to create a pipeline that will automatically update your Ops Manager. At this point, your file tree should now look something like what is shown below. The following tree structure assumes that you are using a mix of vars files and credhub for each of the products used, for the Ops Manager director, and for the upgrade-opsman config file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u251c\u2500\u2500 foundation \u2502 \u251c\u2500\u2500 config \u2502 \u2502 \u251c\u2500\u2500 cf . yml \u2502 \u2502 \u251c\u2500\u2500 director . yml \u2502 \u2502 \u251c\u2500\u2500 healthwatch . yml \u2502 \u2502 \u2514\u2500\u2500 opsman . yml \u2502 \u251c\u2500\u2500 env \u2502 \u2502 \u2514\u2500\u2500 env . yml \u2502 \u251c\u2500\u2500 state \u2502 \u2502 \u2514\u2500\u2500 state . yml \u2502 \u2514\u2500\u2500 vars \u2502 \u251c\u2500\u2500 cf - vars . yml \u2502 \u251c\u2500\u2500 director - vars . yml \u2502 \u251c\u2500\u2500 healthwatch - vars . yml \u2502 \u2514\u2500\u2500 opsman - vars . yml Let's review the resources that are required by Concourse for upgrading Ops Manager: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 resources : - name : platform-automation-tasks type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : .*tasks-(.*).zip - name : platform-automation-image type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : .*image-(.*).tgz - name : configuration type : git source : private_key : ((configuration.private_key)) uri : ((configuration.uri)) branch : master - name : installation type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.installation)) regexp : installation-(.*).zip - name : opsman-image type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : \\[ops-manager,(.*)\\].*.ova # vsphere ex: ops-manager-(.*).ova # for exporting installation daily - name : daily-trigger type : time source : interval : 24h Before we write the jobs portion of the pipeline, let's take a look at the tasks we should run in order to function smoothly: a passing export installation In order for this job to run, a passing export-installation job must have been completed. Again, by exporting the Ops Manager installation, we can ensure that we have a backup in-case of some error. With a backed up installation, upgrade-opsman can be run over-and-over regardless of which stage the task failed during. This is also important in case something happens to the VM externally (whether accidentally deleted, or a similar disaster occurs). If something does happen to the original Ops Manager VM, this installation can be imported by any newly created Ops Manager VM. Please refer to the Valid exported Ops Manager installation section for details on this job. Warning It is recommended to persist the zip file exported from export-installation to an external file store (eg S3) on a regular basis. The exported installation can restore the Ops Manager to a working state if it is non-functional. interpolated env and config This job uses a mix of secret and non-secret variables that are interpolated from both Credhub and vars files. This can be seen in both the interpolate-config-creds and interpolate-env-creds tasks. These tasks return the interpolated-config and interpolated-env files that are further used by the upgrade-opsman task. For further information on how this works and how you can set it up, see the Parameterizing Secrets section of this guide and the Secrets Handling page. ensure commit The ensure portion of this pipeline is used to ensure that the state file is committed to the repository, whether upgrading succeeded or failed. This way, the repository always has the most up to date state.yml file that reflects the current condition of Ops Manager. This is important so that subsequent runs of this task or other tasks don't attempt to target a Ops Manager VM that is deleted or in a bad state. Info When attempting to trouble-shoot the upgrade-opsman task, it may be necessary to manually remove the vm_id from your state.yml file. If the Ops Manager VM is in an unresponsive state or the state.yml file does not reflect the most up to date VM information, (for example, if the ensure fails for some reason) manually removing the vm_id will allow the upgrade task to start in a fresh state and create the VM during the next run. upgrade ops man task The upgrade-opsman task uses all the inputs provided, including the interpolated files, the Ops Manager image, the state.yml file, the env.yml file, and the opsman.yml file, to run. Upon completion, Ops Manager will be upgraded. The following flowchart gives a high level overview of how the task makes decisions for an upgrade: graph TD; versionChk[Is the existing Ops Manager version less than the Ops Manager image?]; versionError[Version Check Error]; delete[Delete the existing Ops Manager VM]; create[Create a new Ops Manager VM]; iaasErr[IAAS CLI error]; import[Import the provided Installation]; iaasErr2[IAAS CLI error]; versionChk -- No --> versionError; versionChk -- Yes --> delete; delete -- Success --> create; delete -- Failure --> iaasErr; create -- Success --> import ; create -- Failure --> iaasErr2; On successive invocations of the task, it may offer different behaviour than the previous run. This aids in recovering from failures (ie: from an IAAS) that occur. For examples on common errors and troubleshooting, see the Troubleshooting section. apply director changes task Finally, using the interpolated environment file, the apply-director-changes task will apply any remaining upgrade changes to Ops Manager. Upon completion of this task, upgrading Ops Manager is now complete. By placing all of these tasks into a pipeline, you can get something like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 jobs : - name : export-installation serial : true plan : - aggregate : - get : daily-trigger trigger : true - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : interpolate-env-creds image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : '/pipeline/vsphere' INTERPOLATION_PATHS : ((foundation))/config SKIP_MISSING : true input_mapping : files : configuration output_mapping : interpolated-files : interpolated-configs - task : export-installation image : platform-automation-image file : platform-automation-tasks/tasks/export-installation.yml input_mapping : env : interpolated-env params : ENV_FILE : ((foundation))/env/env.yml INSTALLATION_FILE : installation-$timestamp.zip - put : installation params : file : installation/installation*.zip - name : upgrade-opsman plan : - aggregate : - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : opsman-image - get : installation passed : [ export-installation ] - get : configuration - task : interpolate-env-creds image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : '/pipeline/vsphere' INTERPOLATION_PATHS : ((foundation))/env SKIP_MISSING : true input_mapping : files : configuration output_mapping : interpolated-files : interpolated-env - task : interpolate-config-creds image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : '/pipeline/vsphere' INTERPOLATION_PATHS : ((foundation))/config SKIP_MISSING : true input_mapping : files : configuration output_mapping : interpolated-files : interpolated-configs - task : upgrade-opsman image : platform-automation-image file : platform-automation-tasks/tasks/upgrade-opsman.yml input_mapping : image : opsman-image state : configuration config : interpolated-configs env : interpolated-env params : VARS_FILES : vars/((foundation))/vars/opsman-vars.yml ENV_FILE : ((foundation))/env/env.yml OPSMAN_CONFIG_FILE : ((foundation))/config/opsman.yml STATE_FILE : ((foundation))/state/state.yml ensure : do : - task : make-commit image : platform-automation-image file : platform-automation-tasks/tasks/make-git-commit.yml input_mapping : repository : configuration file-source : configuration/((foundation))/config output_mapping : repository-commit : configuration-commit params : FILE_SOURCE_PATH : cf.yml # the filename will be called ${SLUG}.yml FILE_DESTINATION_PATH : config/((foundation))/director.yml GIT_AUTHOR_EMAIL : \"git-author-email@example.com\" GIT_AUTHOR_NAME : \"Git Author\" COMMIT_MESSAGE : \"Update director.yml file\" - put : configuration params : repository : configuration-commit merge : true - put : configuration params : repository : configuration-commit merge : true - task : apply-director-changes image : platform-automation-image file : platform-automation-tasks/tasks/apply-director-changes.yml input_mapping : env : interpolated-env params : ENV_FILE : ((foundation))/env/env.yml With your pipeline completed, you are now ready to trigger export-installation , and get started! Troubleshooting When you are upgrading your Ops Manager you may get version check or IaaS CLI errors. For information about troubleshooting these errors, see Version Check Errors and IaaS CLI Errors below. Version Check Errors 1) Downgrading is not supported by Ops Manager (Manual Intervention Required) Ops Manager does not support downgrading to a lower version. SOLUTION: Try the upgrade again with a newer version of Ops Manager. 2) Could not authenticate with Ops Manager (Manual Intervention Required) Credentials provided in the auth file do not match the credentials of an already deployed Ops Manager. SOLUTION: To change the credentials when upgrading an Ops Manager, you must update the password in your Account Settings. Then, you will need to update the following two files with the changes: auth.yml env.yml 3) The Ops Manager API is inaccessible (Recoverable) The task could not communicate with Ops Manager. SOLUTION: Rerun the upgrade-opsman task. The task will assume that the Ops Manager VM is not created, and will run the create-vm and import-installation tasks. IAAS CLI Errors 1) When the CLI for a supported IAAS fails for any reason (i.e., bad network, outage, etc) we treat this as an IAAS CLI error. The following tasks can return an error from the IAAS's CLI: delete-vm , create-vm SOLUTION: The specific error will be returned as output, but most errors can simply be fixed by re-running the upgrade-opsman task. TODO Defining yaml anchors, triggers, time, etc.","title":"How to: Upgrade an Existing Ops Manager"},{"location":"how-to-guides/brainDump.html#how-to-upgrade-an-existing-ops-manager","text":"The following is a How To Guide on setting up and using Platform Automation. This guide assumes you already have a foundation that needs to be automated, or you are coming from a different form of automation (such as pcf-pipelines )","title":"How to: Upgrade an Existing Ops Manager"},{"location":"how-to-guides/brainDump.html#prerequisites","text":"In addition to the prerequisites listed in Downloading and Testing , the Platform Automation team recommends the following: Installed Docker CLI There are a couple one-off tasks that can be either saved in your pipeline, or run once from the command line using our Docker image. The preference to do either is your choice, but the How To Guide will be using the Docker CLI. Basic knowledge of Git , and a GitHub account. Several tasks mutate state and configuration files that are best persisted in a remote version control system. For the purposes of this guide, we'll use GitHub. Amazon S3 or Minio While any blobstore may be used, this How To Guide will be using an s3-compatible blobstore. A fully installed foundation (either PAS or PKS) with all relevant tiles similarly configured and installed Warning Upgrading Ops Manager requires that your foundation have no pending apply-changes. The exported installation will not reflect any pending changes, and will not export at all if the foundation has not fully installed at least the Ops Manager BOSH director.","title":"Prerequisites"},{"location":"how-to-guides/brainDump.html#goals-and-overview","text":"The goal of this How To Guide is to build up a portion of the Reference Pipeline and the Reference Resources Pipeline that is relevant for upgrading Ops Manager for an already existing foundation. This guide will go through the following steps: Retrieving Ops Manager, PAS, Healthwatch, and Platform Automation from Pivnet and storing in an s3 blobstore How to interpolate the configs using credhub, and feeding these interpolated configs into the concourse tasks How to setup a sample github repo Setup recommended file structure Create required files for Upgrade Retrieve the existing config from Ops Manager using docker run Retrieve the existing config from PAS and Healthwatch using docker run Create a pipeline to upgrade Ops Manager TODO: link the above to the headers below (after design review)","title":"Goals and Overview"},{"location":"how-to-guides/brainDump.html#retrieving-resources-from-pivnet","text":"When creating a Concourse pipeline, we will expand the following base structure: 1 2 3 resource_types : resources : jobs : Concourse has many Resource Types built in, but for the purpose of this reference pipeline, we will be utilizing the Pivotal pivnet-resource to directly communicate with and download products from Pivnet. To tell concourse that we will be using the Pivnet resource, we will have to include the following in your resource_types section: 1 2 3 4 5 - name : pivnet type : docker-image source : repository : pivotalcf/pivnet-resource tag : latest-final After listing the \"custom\" resource type, we can then list the resources that our foundation requires. For this guide, our foundation includes: Ops Manager, Pivotal Application Service (PAS), and Healthwatch. The general automated workflow for fetching and storing resources is as follows: TODO: link to the appropriate sections (after design review) Setup your s3 with the appropriate credentials/buckets for your foundation Download product and stemcell (using download-product ) and store in an s3 bucket Download Platform Automation from Pivnet The resources required to accomplish these tasks include: healthwatch-product s3 storage location healthwatch-stemcell s3 storage location ops-manager s3 storage location pas-product s3 storage location pas-stemcell s3 storage location platform-automation-docker-image s3 storage location platform-automation-tasks storage location To reference the storage locations in Concourse, you are required to have the access_key_id and secret_access_key for accessing the appropriate bucket the name of the bucket for storing the product(s) the region the bucket is located in a regex that describes the filename of the product being stored (to prevent the wrong product/version from being stored/accessed later) The following example assumes that all of your products live in the same s3 bucket, thus their regexp are very specific to match the slug/version. To retrieve the platform-automation product, there is no stemcell, and thus the download process is much simpler than with Ops Manager products. Therefore, we can use the Pivnet resource we defined earlier and pull from Pivnet directly. The product can be stored in s3 the same way that the other products can. We can add all of these resources to our pipeline under the resources section: Healthwatch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 - name : healthwatch-product type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : \\[p-healthwatch,(.*)\\]p-healthwatch-.*.pivotal - name : healthwatch-stemcell type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : healthwatch-stemcell/\\[stemcells-ubuntu-xenial,(.*)\\]bosh-stemcell-.*-vsphere.*\\.tgz PAS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 - name : pas-product type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : \\[elastic-runtime,(.*)\\]cf-.*.pivotal - name : pas-stemcell type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : pas-stemcell/\\[stemcells-ubuntu-xenial,(.*)\\]bosh-stemcell-.*-vsphere.*\\.tgz Ops Manager 1 2 3 4 5 6 7 8 - name : opsman-product type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : \\[ops-manager,(.*)\\].*.ova Platform Automation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 - name : platform-automation-pivnet type : pivnet source : api_token : ((pivnet_token)) product_slug : platform-automation product_version : 2\\.(.*) sort_by : semver - name : platform-automation-tasks type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : platform-automation-tasks-(.*).zip - name : platform-automation-image type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : platform-automation-image-(.*).tgz","title":"Retrieving Resources from Pivnet"},{"location":"how-to-guides/brainDump.html#parametrizing-secrets-and-using-credhub-interpolate","text":"This example pipeline will make heavy use of the credhub-interpolate task. For more information on how this works, and how to set it up and use it properly, please see the Secrets Handling page. The config file used in the following section mixes secret and non-secret variables. When choosing which variables to keep in the config file, and which ones to ((parametrize)) , you should consider whether public access to the variable would be a concern. If choosing to parametrize, you will need to first use credhub-interpolate to substitute the Credhub values into the config for the next task to use. Info Parametrized configurations that are interpolated by Credhub return a config file with the formerly parametrized variables with their Credhub values. Concourse VMs are ephemeral, and these full config files are only available in the specific job, and will not be persisted. An example of how to use this in the resources pipeline is shown below. We will be defining this \"task\" external to jobs and resources, so that it can be used in multiple jobs while keeping the yaml clean. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 resource-types : resources : credhub-interpolate : &credhub-interpolate image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : '/pipeline/vsphere' INTERPOLATION_PATHS : \"download-product-configs\" input_mapping : files : config output_mapping : interpolated-files : config jobs : When referencing the above \"task\" we will be calling it with the yaml below. This will expand the anchor *credhub-interpolate with the concourse-readable data we defined in &credhub-interpolate above. 1 2 - task : credhub-interpolate << : *credhub-interpolate","title":"Parametrizing Secrets, and Using Credhub Interpolate"},{"location":"how-to-guides/brainDump.html#download-product-and-products-stemcell","text":"Before downloading a product, you need a config file for download-product . Commented out fields are entirely optional and should only be used if you have a need to do so. Explanations for each field are given below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 --- pivnet-api-token : ((pivnet-api-token)) ## Note that file globs must be quoted if they start with *; ## otherwise they'll be interpreted as a YAML anchor. pivnet-file-glob : \"*.pivotal\" pivnet-product-slug : product-slugs ## Either product-version OR product-version-regex is required # product-version: 1.2.3 ## Note that the regex mustn't be quoted, ## as escape characters for the regex will confuse yaml parsers. product-version-regex : ^1\\.2\\..*$ stemcell-iaas : google ## The following are required only if using download-product-s3. ## Any key marked required above is still required when using S3. ## If s3-bucket is set, ## downloaded product files will have their slug and version prepended. s3-bucket : s3-bucket s3-region-name : us-west-1 # required; sufficient for AWS s3-endpoint : s3.endpoint.com # if not using AWS, this is required ## Required unless `s3-auth-method` is `iam` s3-access-key-id : ((s3-access-key-id)) s3-secret-access-key : ((s3-secret-access-key)) To fetch a product from Pivnet, concourse needs to know: what image it will run the task on ( platform-automation-image ) where the task file will come from ( platform-automation-tasks ) what config file it will read from to get data about pivnet and the tile (this is the download-product-config created above) how to map the output from the task to something you will use later where to put the output resources created in the task These requirements gathered together and executed in a task could look like the snippet below. The snippet involves downloading Healthwatch. However, Healthwatch can be easily replaced by any other tile. The only pieces that would need to change are the task name (if being specific), the name of the stemcell (if mapping), the name of the download-product-config , and the put 's specified after the product and stemcell are downloaded. Healthwatch 1 2 3 4 5 6 7 8 9 10 11 12 13 - task : download-healthwatch-product-and-stemcell image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : download-product-configs/healthwatch.yml output_mapping : { downloaded-stemcell : healthwatch-stemcell } - aggregate : - put : healthwatch-product params : file : downloaded-product/*.pivotal - put : healthwatch-stemcell params : file : healthwatch-stemcell/*.tgz PAS 1 2 3 4 5 6 7 8 9 10 11 12 13 - task : download-pas-product-and-stemcell image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : download-product-configs/pas.yml output_mapping : { downloaded-stemcell : pas-stemcell } - aggregate : - put : pas-product params : file : downloaded-product/*.pivotal - put : pas-stemcell params : file : pas-stemcell/*.tgz Concourse requires tasks to be within jobs. For convenience, and ease of explanation, we have created a separate job for each product. These tasks can easily be combined into a single job. Benefits of doing this could include running credhub-interpolate only once, (instead of once for each job). One downside of structuring your pipeline with many tasks in a given job is that you lose the ability to rerun just a particular section of the job that failed. This means Concourse would run every task again if the job was triggered a second time. To make sure your blobstore always has the most recent version of a pivnet product, you can use the built-in time resource, to tell the fetch jobs how often to run and attempt to download a new version of the product and/or stemcell. To add this functionality to your pipeline, you must include the time resource in your resources section: 1 2 3 4 - name : daily-trigger type : time source : interval : 24h If included, this resource can be referenced in any appropriate job, and you can set the job to trigger on that daily (or custom) interval. Examples of the fetch-{product} job are shown below. The job includes the task we created above, the daily time trigger, and the interpolate created in an earlier step . These jobs should be included under the jobs header in your pipeline. Healthwatch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 - name : fetch-healthwatch plan : - aggregate : - get : daily trigger : true - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : config - task : credhub-interpolate << : *credhub-interpolate - task : download-healthwatch-product-and-stemcell image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : download-product-configs/healthwatch.yml output_mapping : { downloaded-stemcell : healthwatch-stemcell } - aggregate : - put : healthwatch-product params : file : downloaded-product/*.pivotal - put : healthwatch-stemcell params : file : healthwatch-stemcell/*.tgz PAS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 - name : fetch-pas plan : - aggregate : - get : daily trigger : true - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : config - task : credhub-interpolate << : *credhub-interpolate - task : download-pas-product-and-stemcell image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : download-product-configs/pas.yml output_mapping : { downloaded-stemcell : pas-stemcell } - aggregate : - put : pas-product params : file : downloaded-product/*.pivotal - put : pas-stemcell params : file : pas-stemcell/*.tgz Ops Manager 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 - name : fetch-opsman plan : - aggregate : - get : daily trigger : true - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : config - task : credhub-interpolate << : *credhub-interpolate - task : download-opsman-image image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : download-product-configs/opsman.yml - aggregate : - put : opsman-product params : file : downloaded-product/*","title":"Download Product and Product's Stemcell"},{"location":"how-to-guides/brainDump.html#download-platform-automation-from-pivnet","text":"Downloading the Platform Automation toolset does not use download-product . Using download-product for this would require the product have a dependency on itself. Instead, we use the pivnet-resource we defined earlier. The Platform Automation team recommends triggering fetch-platform-automation whenever there is a new version available. The resource definition for platform automation from above is pinned to the major version. Pinning to the major version but letting the minor and patch version increase allows you to get security updates, bug fixes, and new (non-breaking) features. Platform Automation uses semantic versioning to enable this. To download the Platform Automation tasks and the Docker image, and put it into your s3 blobstore, add the following job : 1 2 3 4 5 6 7 8 9 10 11 - name : fetch-platform-automation plan : - get : platform-automation-pivnet trigger : true - aggregate : - put : platform-automation-tasks params : file : platform-automation-pivnet/*tasks*.zip - put : platform-automation-image params : file : platform-automation-pivnet/*image*.tgz","title":"Download Platform Automation from Pivnet"},{"location":"how-to-guides/brainDump.html#a-complete-resources-pipeline","text":"Now that we have built up the resources pipeline, you can find this full example on the Reference Pipeline page. This also includes an example of fetching a Windows tile, but if you understand the concepts above, you can use the Windows tile, the mySQL tile, or any other tile you desire for your foundation.","title":"A Complete Resources Pipeline"},{"location":"how-to-guides/brainDump.html#sample-github-repository-and-file-structure","text":"Now let's dive into using version control to manage state in pipelines built with Platform Automation. We'll set up a git repository on Github, and the recommended directory structure for the repo.","title":"Sample Github repository and file structure"},{"location":"how-to-guides/brainDump.html#git-and-github","text":"Git is a commonly used version control tool. It can be used to track code changes made to files within a repository (or \"repo\"). Changes can then be \"pushed\" to or \"pulled\" from remote copies of that repository. Github is a system that provides git remotes. Using a remote will enable the pipeline we are creating to access and update the state and configuration files. To learn more about git and github, you can read this short git handbook . To create our Github repo: Create a new repository Using the \"Example: Start a new repository and publish it to GitHub\" section of the Git handbook (about 3/4 down the page), create a repo, add a file, and push it to Github.","title":"Git and Github"},{"location":"how-to-guides/brainDump.html#creating-repo-directory-structure","text":"You now have both a local git repo and a remote on Github. The recommended structure for a config repo is: 1 2 3 4 5 \u251c\u2500\u2500 foundation \u2502 \u251c\u2500\u2500 config \u2502 \u251c\u2500\u2500 env \u2502 \u251c\u2500\u2500 state \u2502 \u2514\u2500\u2500 vars These directories are needed for this guide. config Holds config files for the products installed on your foundation. If using Credhub and/or vars files, these config files should have your ((parametrized)) values present in them env Holds env.yml , the environment file used by tasks that interact with Ops Manager. vars Holds product-specific vars files. state Holds state.yml , which contains the VM ID for the Ops Manager VM.","title":"Creating Repo Directory Structure"},{"location":"how-to-guides/brainDump.html#creating-the-required-files","text":"Several files are required for upgrading an Ops Manager VM. They are used to capture the current state of the Ops Manager VM.","title":"Creating the Required Files"},{"location":"how-to-guides/brainDump.html#ops-manager-vm-state","text":"We'll need to capture the current Ops Manager VM identifier, so we know what VM we are upgrading. To create a state.yml from your existing foundation, use the following as a template, based on your IaaS: AWS 1 2 3 iaas : aws # Instance ID of the AWS VM vm_id : i-12345678987654321 Azure 1 2 3 iaas : azure # Computer Name of the Azure VM vm_id : vm_name GCP 1 2 3 iaas : gcp # Name of the VM in GCP vm_id : vm_name OpenStack 1 2 3 iaas : openstack # Instance ID from the OpenStack Overview vm_id : 12345678-9876-5432-1abc-defghijklmno vSphere 1 2 3 iaas : vsphere # Path to the VM in vCenter vm_id : /datacenter/vm/folder/vm_name","title":"Ops Manager VM State"},{"location":"how-to-guides/brainDump.html#ops-manager-vm-configuration","text":"opsman.yml is the configuration file for p-automator , which calls out to specific IaaS CLIs in order to create/update/delete a VM. The properties contained in opsman.yml differ based on your IaaS: AWS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # code_snippet aws-configuration start yaml --- opsman-configuration : aws : region : us-west-2 vm_name : ops-manager-vm # defaults Ops Manager-vm boot_disk_size : 100 # default 200 vpc_subnet_id : subnet-0292bc845215c2cbf security_group_id : sg-0354f804ba7c4bc41 key_pair_name : ops-manager-key iam_instance_profile_name : ops-manager-iam instance_type : m5.large # At least one IP address (public or private) # needs to be assigned to the VM. # It is also permissable to assign both. public_ip : 1.2.3.4 private_ip : 10.0.0.2 # Required if use_instance_profile is false # omit if using Instance Profiles access_key_id : sample-access-id secret_access_key : sample-secret-access-key # If using Instance Profiles (omit if using AWS Credentials) use_instance_profile : true # default false # Optional, necessary if a role is needed to authorize the instance profile assume_role : arn:aws:iam::123456789:role/test # code_snippet aws-configuration end Azure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # code_snippet azure-configuration start yaml --- opsman-configuration : azure : subscription_id : 90f35f10-ea9e-4e80-aac4-d6778b995532 resource_group : res-group tenant_id : 3e52862f-a01e-4b97-98d5-f31a409df682 client_id : 5782deb6-9195-4827-83ae-a13fda90aa0d client_secret : 6Iaue71Lqxfq location : westus container : opsmanagerimage # container for opsman image network_security_group : ops-manager-security-group # Note that there are several environment-specific details in this path vpc_subnet : /subscriptions/<MY_SUBSCRIPTION_ID>/resourceGroups/<MY_RESOURCE_GROUP>/providers/Microsoft.Network/virtualNetworks/<MY_VNET>/subnets/<MY_SUBNET> storage_account : opsman # account name of container # Optional # only needed if your client doesn't have the needed storage permissions storage_key : pEuXDaDK/WWo... ssh_public_key : ssh-rsa AAAAB3NzaC1yc2EAZ... # ssh key to access VM vm_name : ops-manager-vm # default : Ops Manager-vm boot_disk_size : 100 # default : 200 cloud_name : AzureCloud # default : AzureCloud # This flag is only respected by the create-vm & upgrade-opsman commands # set to true if you want to create the new opsman vm with unmanaged disk # delete-vm discovers the disk type from the VM use_unmanaged_disk : false # At least one IP address (public or private) # needs to be assigned to the VM. # It is also permissable to assign both. public_ip : 1.2.3.4 private_ip : 10.0.0.3 # code_snippet azure-configuration end GCP 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # code_snippet gcp-configuration start yaml --- opsman-configuration : gcp : gcp_service_account : | { \"type\": \"service_account\", \"project_id\": \"project-id\", \"private_key_id\": \"af719b1ca48f7b6ac67ca9c5319cb175\", \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n...\\n-----END PRIVATE KEY-----\\n\", \"client_email\": \"user@project-id.iam.gserviceaccount.com\", \"client_id\": \"1234567890\", \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\", \"token_uri\": \"https://accounts.google.com/o/oauth2/token\", \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\", \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/user%40project-id.iam.gserviceaccount.com\" } project : project-id region : us-central1 zone : us-central1-b vm_name : ops-manager-vm # default : Ops Manager-vm # For SharedVPC: projects/[HOST_PROJECT_ID]/regions/[REGION]/subnetworks/[SUBNET] vpc_subnet : infrastructure-subnet tags : ops-manager # This CPU, Memory and disk size demonstrated here # match the defaults, and needn't be included if these are the desired values custom_cpu : 2 custom_memory : 8 boot_disk_size : 100 # At least one IP address (public or private) needs to be assigned to the VM. public_ip : 1.2.3.4. private_ip : 10.0.0.2 # code_snippet gcp-configuration end OpenStack 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # code_snippet openstack-configuration start yaml --- opsman-configuration : openstack : auth_url : http://os.example.com:5000/v2.0 project_name : project net_id : 26a13112-b6c2-11e8-96f8-529269fb1459 username : admin password : password key_pair_name : opsman-keypair security_group_name : opsman-sec-group vm_name : ops-manager-vm # default : Ops Manager-vm # At least one IP address (public or private) needs to be assigned to the VM. public_ip : 1.2.3.4 private_ip : 10.0.0.3 flavor : m1.xlarge # default : m1.xlarge project_domain_name : default user_domain_name : default identity_api_version : 3 # default : 2 insecure : true # default : false availability_zone : zone-01 # code_snippet openstack-configuration end vSphere 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # code_snippet vsphere-configuration start yaml --- opsman-configuration : vsphere : vcenter : url : vcenter.example.com username : admin password : password datastore : exmple-ds-1 ca_cert : certificate host : example-host # vCenter host to deploy Ops Manager in datacenter : example-dc resource_pool : /example-dc/host/example-host/Resources/ResPool # or /<Data Center Name>/host/<Cluster Name> folder : /example-dc/vm/Folder insecure : 1 # default : 0 (secure); 1 (insecure) disk_type : thin # example : thin|thick private_ip : 10.0.0.2 dns : 8.8.8.8 ntp : ntp.example.com # example : ntp.ubuntu.com ssh_password : password ssh_public_key : ssh-rsa ...... # for Ops Manager >= 2.3 hostname : pcf.example.com network : virtual-network # vcenter network to deploy to netmask : 255.255.255.192 gateway : 192.168.10.1 vm_name : Ops_Manager # default : Ops_Manager memory : 8 # default : 8 GB cpu : 1 # default : 1 # code_snippet vsphere-configuration end","title":"Ops Manager VM Configuration"},{"location":"how-to-guides/brainDump.html#ops-manager-environment-file","text":"env.yml holds authentication and target information for a particular Ops Manager. This file is required by upgrade-opsman because after the VM is recreated, the task will import the provided installation.zip to Ops Manager to finish the process. If your foundation uses authentication other than basic auth, please reference Inputs and Outputs for more detail on UAA-based authentication. An example env.yml is shown below. As mentioned in the comment, decryption-passphrase is required for import-installation , and therefore required for upgrade-opsman . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- target : https://pcf.example.com connect-timeout : 30 # default 5 request-timeout : 1800 # default 1800 skip-ssl-validation : false # default false username : username password : password # decryption-passphrase is optional, # except for use with `import-installation`. # OpsMan depends on the passphrase # to decrypt the imported installation. # For other commands, providing this key allows # decryption of the OpsMan VM after reboot, # which would otherwise need to be done manually. decryption-passphrase : passphrase","title":"Ops Manager Environment File"},{"location":"how-to-guides/brainDump.html#vars-files","text":"If you are using files to store vars be rendered into your configuration templates, these files should be in your git repo under the vars directory. For more information on vars files, see Secrets Handling .","title":"Vars files"},{"location":"how-to-guides/brainDump.html#valid-exported-ops-manager-installation","text":"upgrade-opsman will not allow you to execute the task unless the installation provided to the task is a installation provided by Ops Manager itself. In the UI, this is located on the Settings Page of Ops Manager. Platform Automation strongly recommends automatically exporting and persisting the Ops Manager installation on a regular basis. In order to do so, you can set your pipeline to run the export-installation task on a daily trigger. This should be persisted into S3 or a blobstore of your choice. You can start your pipeline by first creating this export-installation task and persisting it in an S3 bucket. Warning It is recommended to persist the zip file exported from export-installation to an external file store (eg S3) on a regular basis. The exported installation can restore the Ops Manager to a working state if it is non-functional. Requirements for this task include: the Platform Automation image the Platform Automation tasks a configuration path for your env file interpolation of the env file with credhub a resource to store the exported installation into Starting our concourse pipeline, we need the following resources: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 - name : platform-automation-tasks type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : .*tasks-(.*).zip - name : platform-automation-image type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : .*image-(.*).tgz - name : configuration type : git source : private_key : ((configuration.private_key)) uri : ((configuration.uri)) branch : master - name : installation type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.installation)) regexp : installation-(.*).zip In our jobs section, we need a job that will trigger daily to pull down the Ops Manager installation and store it in S3. This looks like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 - name : export-installation serial : true plan : - aggregate : - get : daily-trigger trigger : true - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : interpolate-env-creds image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : '/pipeline/vsphere' INTERPOLATION_PATHS : ((foundation))/config SKIP_MISSING : true input_mapping : files : configuration output_mapping : interpolated-files : interpolated-configs - task : export-installation image : platform-automation-image file : platform-automation-tasks/tasks/export-installation.yml input_mapping : env : interpolated-env params : ENV_FILE : ((foundation))/env/env.yml INSTALLATION_FILE : installation-$timestamp.zip - put : installation params : file : installation/installation*.zip Once this resource is persisted, we can safely run upgrade-opsman , knowing that we can never truly lose our foundation. This is also important in case something happens to the VM externally (whether accidentally deleted, or a similar disaster occurs). If something does happen to the original Ops Manager VM, this installation can be imported by any newly created Ops Manager VM. Warning It is recommended to persist the zip file exported from export-installation to an external file store (eg S3) on a regular basis. The exported installation can restore the Ops Manager to a working state if it is non-functional.","title":"Valid exported Ops Manager installation"},{"location":"how-to-guides/brainDump.html#retrieving-existing-ops-manager-director-configuration","text":"If you would like to automate the configuration of your Ops Manager, you first need to externalize the director configuration. Using Platform Automation, this is done using Docker or by adding a job to the pipeline. Docker To get the currently configured Ops Manager configuration, we have to: Import the image 1 docker import ${ PLATFORM_AUTOMATION_IMAGE_TGZ } platform-automation-image Where ${PLATFORM_AUTOMATION_IMAGE_TGZ} is the image file downloaded from Pivnet. Then, you can use docker run to pass it arbitrary commands. Here, we're running the om CLI to see what commands are available: 1 2 docker run -it --rm -v $PWD :/workspace -w /workspace platform-automation-image \\ om -h Note: that this will have access read and write files in your current working directory. If you need to mount other directories as well, you can add additional -v arguments. The command we will use to extract the current director configuration is called staged-director-config . This is an om command that calls the Ops Manager API to pull down the currently configured director configuration. To run this using Docker, you will need the env file created above as ${ENV_FILE} : 1 2 docker run -it --rm -v $PWD :/workspace -w /workspace platform-automation-image \\ om --env ${ ENV_FILE } staged-director-config --include-placeholders --include-placeholders is an optional flag, but highly recommended if you want a full configuration for your Ops Manager. This flag will replace any fields marked as \"secret\" in your Ops Manager config with ((parametrized)) variables. If you would prefer to not work with ((parametrized)) variables, you can substitute --include-placeholders with --include-credentials . Warning --include-credentials WILL expose passwords and secrets in plain text . Therefore, --include-placeholders is recommended, but not required. Pipeline To add [ staged-director-config ] to your pipeline, you will need the following resources: the Platform Automation image the Platform Automation tasks a configuration path for your env file a resource to store the exported configuration into Starting our Concourse pipeline, we need the following resources : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 - name : platform-automation-tasks type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : .*tasks-(.*).zip - name : platform-automation-image type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : .*image-(.*).tgz - name : configuration type : git source : private_key : ((configuration.private_key)) uri : ((configuration.uri)) branch : master In our jobs section, we need a job that will interpolate the env file, pull down the Ops Manager director config, and store the director config in the configuration directory (this can be the same resource as where the env is located, but will be stored in the config instead of the env directory). In order to persist the director config in your git repo, we first need to make a commit, detailing the change we made, and where in your git repo the change happened. A way to do this is shown below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 - name : staged-director-config plan : - aggregate : - get : platform-automation-tasks params : { unpack : true } - get : platform-automation-image params : { unpack : true } - get : configuration - task : interpolate-env-creds image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : '/pipeline/vsphere' INTERPOLATION_PATHS : ((foundation))/config SKIP_MISSING : true input_mapping : files : configuration output_mapping : interpolated-files : interpolated-configs - task : staged-director-config image : platform-automation-image file : platform-automation-tasks/tasks/staged-director-config.yml input_mapping : env : interpolated-env output_mapping : generated-config : configuration/((foundation))/config params : ENV_FILE : ((foundation))/env/env.yml - task : make-commit image : platform-automation-image file : platform-automation-tasks/tasks/make-git-commit.yml input_mapping : repository : configuration file-source : configuration/((foundation))/config output_mapping : repository-commit : configuration-commit params : FILE_SOURCE_PATH : director.yml FILE_DESTINATION_PATH : config/((foundation))/director.yml GIT_AUTHOR_EMAIL : \"git-author-email@example.com\" GIT_AUTHOR_NAME : \"Git Author\" COMMIT_MESSAGE : \"Update director.yml file\" - put : configuration params : repository : configuration-commit merge : true","title":"Retrieving Existing Ops Manager Director Configuration"},{"location":"how-to-guides/brainDump.html#retrieving-existing-product-configurations","text":"If you would like to automate the configuration of your products, you first need to externalize each product's configuration. Using Platform Automation, this is done using Docker or by adding a job to the pipeline. Docker As an example, we are going to start with the PAS tile. To get the currently configured PAS configuration, we have to: Import the image 1 docker import ${ PLATFORM_AUTOMATION_IMAGE_TGZ } platform-automation-image Where ${PLATFORM_AUTOMATION_IMAGE_TGZ} is the image file downloaded from Pivnet. Then, you can use docker run to pass it arbitrary commands. Here, we're running the om CLI to see what commands are available: 1 2 docker run -it --rm -v $PWD :/workspace -w /workspace platform-automation-image \\ om -h Note: that this will have access read and write files in your current working directory. If you need to mount other directories as well, you can add additional -v arguments. The command we will use to extract the current director configuration is called staged-config . This is an om command that calls the Ops Manager API to pull down the currently configured product configuration given a product slug. To run this using Docker, you will need the env file created above as ${ENV_FILE} . The product slug for the PAS tile, within Ops Manager, is cf . To find the slug of your product, you can run the following docker command: 1 2 docker run -it --rm -v $PWD :/workspace -w /workspace platform-automation-image \\ om --env ${ ENV_FILE } staged-products This will give you a table like the following: 1 2 3 4 5 6 7 + ---------------+-----------------+ | NAME | VERSION | + ---------------+-----------------+ | cf | 2 . x . x | | p - healthwatch | 1 . x . x - build . x | | p - bosh | 2 . x . x - build . x | + ---------------+-----------------+ The values in the NAME column are the slugs of each product you have deployed. For this How to Guide , we only have PAS and Healthwatch. Info p-bosh is the product slug of Ops Manager. However, staged-config cannot be used to extract the director config. To do so, you must use staged-director-config With the appropriate product ${SLUG} , we can run the following docker command to pull down the configuration of the chosen tile: 1 2 docker run -it --rm -v $PWD :/workspace -w /workspace platform-automation-image \\ om --env ${ ENV_FILE } staged-config --product-name ${ SLUG } --include-placeholders --include-placeholders is an optional flag, but highly recommended if you want a full configuration for your tile. This flag will replace any fields marked as \"secret\" by the product in the config with ((parametrized)) variables. If you would prefer to not work with ((parametrized)) variables, you can substitute --include-placeholders with --include-credentials . Warning --include-credentials WILL expose passwords and secrets in plain text . Therefore, --include-placeholders is recommended, but not required. Pipeline To add [ staged-config ] to your pipeline, you will need the following resources: the Platform Automation image the Platform Automation tasks a configuration path for your env file a resource to store the exported configuration into Starting our Concourse pipeline, we need the following resources: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 - name : platform-automation-tasks type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : .*tasks-(.*).zip - name : platform-automation-image type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : .*image-(.*).tgz - name : configuration type : git source : private_key : ((configuration.private_key)) uri : ((configuration.uri)) branch : master In our jobs section, we need a job that will interpolate the env file, pull down the product config, and store the director config in the configuration directory (this can be the same resource as where the env is located, but will be stored in the config instead of the env directory). In order to persist the product config in your git repo, we first need to make a commit, detailing the change we made, and where in your git repo the change happened. A way to do this is shown below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 - name : staged-director-config plan : - aggregate : - get : platform-automation-tasks params : { unpack : true } - get : platform-automation-image params : { unpack : true } - get : configuration - task : interpolate-env-creds image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : '/pipeline/vsphere' INTERPOLATION_PATHS : ((foundation))/config SKIP_MISSING : true input_mapping : files : configuration output_mapping : interpolated-files : interpolated-configs - task : staged-director-config image : platform-automation-image file : platform-automation-tasks/tasks/staged-director-config.yml input_mapping : env : interpolated-env output_mapping : generated-config : configuration/((foundation))/config params : ENV_FILE : ((foundation))/env/env.yml - task : make-commit image : platform-automation-image file : platform-automation-tasks/tasks/make-git-commit.yml input_mapping : repository : configuration file-source : configuration/((foundation))/config output_mapping : repository-commit : configuration-commit params : FILE_SOURCE_PATH : director.yml FILE_DESTINATION_PATH : config/((foundation))/director.yml GIT_AUTHOR_EMAIL : \"git-author-email@example.com\" GIT_AUTHOR_NAME : \"Git Author\" COMMIT_MESSAGE : \"Update director.yml file\" - put : configuration params : repository : configuration-commit merge : true To retrieve the configuration for Healthwatch, we can simply duplicate the steps used for PAS. The ${SLUG} for Healthwatch, as we retrieved from staged-products , is p-healthwatch","title":"Retrieving Existing Product Configurations"},{"location":"how-to-guides/brainDump.html#creating-a-pipeline-to-upgrade-ops-manager","text":"With the director configuration, product configurations, resources gathered, and config files created, we can finally begin to create a pipeline that will automatically update your Ops Manager. At this point, your file tree should now look something like what is shown below. The following tree structure assumes that you are using a mix of vars files and credhub for each of the products used, for the Ops Manager director, and for the upgrade-opsman config file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u251c\u2500\u2500 foundation \u2502 \u251c\u2500\u2500 config \u2502 \u2502 \u251c\u2500\u2500 cf . yml \u2502 \u2502 \u251c\u2500\u2500 director . yml \u2502 \u2502 \u251c\u2500\u2500 healthwatch . yml \u2502 \u2502 \u2514\u2500\u2500 opsman . yml \u2502 \u251c\u2500\u2500 env \u2502 \u2502 \u2514\u2500\u2500 env . yml \u2502 \u251c\u2500\u2500 state \u2502 \u2502 \u2514\u2500\u2500 state . yml \u2502 \u2514\u2500\u2500 vars \u2502 \u251c\u2500\u2500 cf - vars . yml \u2502 \u251c\u2500\u2500 director - vars . yml \u2502 \u251c\u2500\u2500 healthwatch - vars . yml \u2502 \u2514\u2500\u2500 opsman - vars . yml Let's review the resources that are required by Concourse for upgrading Ops Manager: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 resources : - name : platform-automation-tasks type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : .*tasks-(.*).zip - name : platform-automation-image type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : .*image-(.*).tgz - name : configuration type : git source : private_key : ((configuration.private_key)) uri : ((configuration.uri)) branch : master - name : installation type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.installation)) regexp : installation-(.*).zip - name : opsman-image type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : \\[ops-manager,(.*)\\].*.ova # vsphere ex: ops-manager-(.*).ova # for exporting installation daily - name : daily-trigger type : time source : interval : 24h Before we write the jobs portion of the pipeline, let's take a look at the tasks we should run in order to function smoothly: a passing export installation In order for this job to run, a passing export-installation job must have been completed. Again, by exporting the Ops Manager installation, we can ensure that we have a backup in-case of some error. With a backed up installation, upgrade-opsman can be run over-and-over regardless of which stage the task failed during. This is also important in case something happens to the VM externally (whether accidentally deleted, or a similar disaster occurs). If something does happen to the original Ops Manager VM, this installation can be imported by any newly created Ops Manager VM. Please refer to the Valid exported Ops Manager installation section for details on this job. Warning It is recommended to persist the zip file exported from export-installation to an external file store (eg S3) on a regular basis. The exported installation can restore the Ops Manager to a working state if it is non-functional. interpolated env and config This job uses a mix of secret and non-secret variables that are interpolated from both Credhub and vars files. This can be seen in both the interpolate-config-creds and interpolate-env-creds tasks. These tasks return the interpolated-config and interpolated-env files that are further used by the upgrade-opsman task. For further information on how this works and how you can set it up, see the Parameterizing Secrets section of this guide and the Secrets Handling page. ensure commit The ensure portion of this pipeline is used to ensure that the state file is committed to the repository, whether upgrading succeeded or failed. This way, the repository always has the most up to date state.yml file that reflects the current condition of Ops Manager. This is important so that subsequent runs of this task or other tasks don't attempt to target a Ops Manager VM that is deleted or in a bad state. Info When attempting to trouble-shoot the upgrade-opsman task, it may be necessary to manually remove the vm_id from your state.yml file. If the Ops Manager VM is in an unresponsive state or the state.yml file does not reflect the most up to date VM information, (for example, if the ensure fails for some reason) manually removing the vm_id will allow the upgrade task to start in a fresh state and create the VM during the next run. upgrade ops man task The upgrade-opsman task uses all the inputs provided, including the interpolated files, the Ops Manager image, the state.yml file, the env.yml file, and the opsman.yml file, to run. Upon completion, Ops Manager will be upgraded. The following flowchart gives a high level overview of how the task makes decisions for an upgrade: graph TD; versionChk[Is the existing Ops Manager version less than the Ops Manager image?]; versionError[Version Check Error]; delete[Delete the existing Ops Manager VM]; create[Create a new Ops Manager VM]; iaasErr[IAAS CLI error]; import[Import the provided Installation]; iaasErr2[IAAS CLI error]; versionChk -- No --> versionError; versionChk -- Yes --> delete; delete -- Success --> create; delete -- Failure --> iaasErr; create -- Success --> import ; create -- Failure --> iaasErr2; On successive invocations of the task, it may offer different behaviour than the previous run. This aids in recovering from failures (ie: from an IAAS) that occur. For examples on common errors and troubleshooting, see the Troubleshooting section. apply director changes task Finally, using the interpolated environment file, the apply-director-changes task will apply any remaining upgrade changes to Ops Manager. Upon completion of this task, upgrading Ops Manager is now complete. By placing all of these tasks into a pipeline, you can get something like the following: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 jobs : - name : export-installation serial : true plan : - aggregate : - get : daily-trigger trigger : true - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : interpolate-env-creds image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : '/pipeline/vsphere' INTERPOLATION_PATHS : ((foundation))/config SKIP_MISSING : true input_mapping : files : configuration output_mapping : interpolated-files : interpolated-configs - task : export-installation image : platform-automation-image file : platform-automation-tasks/tasks/export-installation.yml input_mapping : env : interpolated-env params : ENV_FILE : ((foundation))/env/env.yml INSTALLATION_FILE : installation-$timestamp.zip - put : installation params : file : installation/installation*.zip - name : upgrade-opsman plan : - aggregate : - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : opsman-image - get : installation passed : [ export-installation ] - get : configuration - task : interpolate-env-creds image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : '/pipeline/vsphere' INTERPOLATION_PATHS : ((foundation))/env SKIP_MISSING : true input_mapping : files : configuration output_mapping : interpolated-files : interpolated-env - task : interpolate-config-creds image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : '/pipeline/vsphere' INTERPOLATION_PATHS : ((foundation))/config SKIP_MISSING : true input_mapping : files : configuration output_mapping : interpolated-files : interpolated-configs - task : upgrade-opsman image : platform-automation-image file : platform-automation-tasks/tasks/upgrade-opsman.yml input_mapping : image : opsman-image state : configuration config : interpolated-configs env : interpolated-env params : VARS_FILES : vars/((foundation))/vars/opsman-vars.yml ENV_FILE : ((foundation))/env/env.yml OPSMAN_CONFIG_FILE : ((foundation))/config/opsman.yml STATE_FILE : ((foundation))/state/state.yml ensure : do : - task : make-commit image : platform-automation-image file : platform-automation-tasks/tasks/make-git-commit.yml input_mapping : repository : configuration file-source : configuration/((foundation))/config output_mapping : repository-commit : configuration-commit params : FILE_SOURCE_PATH : cf.yml # the filename will be called ${SLUG}.yml FILE_DESTINATION_PATH : config/((foundation))/director.yml GIT_AUTHOR_EMAIL : \"git-author-email@example.com\" GIT_AUTHOR_NAME : \"Git Author\" COMMIT_MESSAGE : \"Update director.yml file\" - put : configuration params : repository : configuration-commit merge : true - put : configuration params : repository : configuration-commit merge : true - task : apply-director-changes image : platform-automation-image file : platform-automation-tasks/tasks/apply-director-changes.yml input_mapping : env : interpolated-env params : ENV_FILE : ((foundation))/env/env.yml With your pipeline completed, you are now ready to trigger export-installation , and get started!","title":"Creating a Pipeline to Upgrade Ops Manager"},{"location":"how-to-guides/brainDump.html#troubleshooting","text":"When you are upgrading your Ops Manager you may get version check or IaaS CLI errors. For information about troubleshooting these errors, see Version Check Errors and IaaS CLI Errors below.","title":"Troubleshooting"},{"location":"how-to-guides/brainDump.html#version-check-errors","text":"1) Downgrading is not supported by Ops Manager (Manual Intervention Required) Ops Manager does not support downgrading to a lower version. SOLUTION: Try the upgrade again with a newer version of Ops Manager. 2) Could not authenticate with Ops Manager (Manual Intervention Required) Credentials provided in the auth file do not match the credentials of an already deployed Ops Manager. SOLUTION: To change the credentials when upgrading an Ops Manager, you must update the password in your Account Settings. Then, you will need to update the following two files with the changes: auth.yml env.yml 3) The Ops Manager API is inaccessible (Recoverable) The task could not communicate with Ops Manager. SOLUTION: Rerun the upgrade-opsman task. The task will assume that the Ops Manager VM is not created, and will run the create-vm and import-installation tasks.","title":"Version Check Errors"},{"location":"how-to-guides/brainDump.html#iaas-cli-errors","text":"1) When the CLI for a supported IAAS fails for any reason (i.e., bad network, outage, etc) we treat this as an IAAS CLI error. The following tasks can return an error from the IAAS's CLI: delete-vm , create-vm SOLUTION: The specific error will be returned as output, but most errors can simply be fixed by re-running the upgrade-opsman task.","title":"IAAS CLI Errors"},{"location":"how-to-guides/brainDump.html#todo","text":"Defining yaml anchors, triggers, time, etc.","title":"TODO"},{"location":"how-to-guides/install-opsman.html","text":"Writing a Pipeline to Install Ops Manager Prerequisites Over the course of this guide, we're going to use Platform Automation for PCF to create a pipeline using Concourse . Before we get started, you'll need a few things ready to go: Credentials for an IaaS that Ops Manager is compatible with A Concourse instance with access to a Credhub instance and to the Internet Github.com account Read/write credentials and bucket name for an S3 bucket An account on https://network.pivotal.io (Pivnet) A MacOS workstation with Docker installed and a text editor you like a terminal emulator you like a browser that works with Concourse, like Firefox or Chrome and git IaaS It doesn't actually matter what IaaS you use for Ops Manager, as long as your Concourse can connect to it. Pipelines built with Platform Automation can be platform-agnostic. It will be very helpful to have a basic familiarity with the following. If you don't have basic familiarity with all these things, that's okay. We'll explain some basics, and link to resources to learn more: the bash terminal git YAML Concourse A note on the prerequisites While this guide uses Github to provide a git remote, and an S3 bucket as a blobstore, Platform Automation supports arbitrary git providers and S3-compatible blobstores. If you need to use an alternate one, that's okay. We picked specific examples so we could describe some steps in detail. Some details may be different if you follow along with different providers. If you're comfortable navigating those differences on your own, go for it! Similarly, in this guide, we assume the MacOS operating system. This should all work fine on Linux, too, but there might be differences in the paths you'll need to figure out. Creating a Concourse Pipeline Platform Automation's tasks and image are meant to be used in a Concourse pipeline. So, let's make one. Using your bash command-line client, create a directory to keep your pipeline files in, and cd into it. 1 2 mkdir platform-automation-pipelines cd !$ \" !$ \" !$ is a bash shortcut. Pronounced \"bang, dollar-sign,\" it means \"use the last argument from the most recent command.\" In this case, that's the directory we just created! This is not a Platform Automation thing, this is just a bash tip dearly beloved of at least one Platform Automator. Before we get started with the pipeline itself, we'll gather some variables in a file we can use throughout our pipeline. Open your text editor and create vars.yml . Here's what it should look like to start, we can add things to this as we go: 1 2 3 platform-automation-bucket : your-bucket-name credhub-server : https://your-credhub.example.com opsman-url : https://pcf.foundation.example.com Using a DNS This example assumes that you're using DNS and hostnames. You can use IP addresses for all these resources instead, but you still need to provide the information as a URL, for example: https://120.121.123.124 Now, create a file called pipeline.yml . Naming We'll use pipeline.yml in our examples throughout this guide. However, you may create multiple pipelines over time. If there's a more sensible name for the pipeline you're working on, feel free to use that instead. Write this at the top, and save the file. This is YAML for \"the start of the document. It's optional, but traditional: 1 --- Now you have a pipeline file! Nominally! Well, look. It's valid YAML, at least. Getting fly Let's try to set it as a pipeline with fly , the Concourse command-line Interface (CLI). First, check if we've got fly installed at all: 1 fly -v If it gives you back a version number, great! Skip ahead to Setting The Pipeline If it says something like -bash: fly: command not found , we have a little work to do; we've got to get fly . Navigate to the address for your Concourse instance in a web browser. At this point, you don't even need to be signed in! If there are no public pipelines, you should see something like this: If there are public pipelines, or if you're signed in and there are pipelines you can see, you'll see something similar in the lower-right hand corner. Click the icon for your OS and save the file, mv the resulting file to somewhere in your $PATH , and use chmod to make it executable: A note on command-line examples Some of these, you can copy-paste directly into your terminal. Some of them won't work that way, or even if they did, would require you to edit them to replace our example values with your actual values. We recommend you type all of the bash examples in by hand, substituting values, if necessary, as you go. Don't forget that you can often hit the tab key to auto-complete the name of files that already exist; it makes all that typing just a little easier, and serves as a sort of command-line autocorrect. 1 2 mv ~/Downloads/fly /usr/local/bin/fly chmod +x !$ Congrats! You got fly . Okay but what did I just do? FAIR QUESTION. You downloaded the fly binary, moved it into bash's PATH, which is where bash looks for things to execute when you type a command, and then added permissions that allow it to be e x ecuted. Now, the CLI is installed - and we won't have to do all that again, because fly has the ability to update itself, which we'll get into later. Setting The Pipeline Okay now let's try to set our pipeline with fly , the Concourse CLI. fly keeps a list of Concourses it knows how to talk to. Let's see if the Concourse we want is already on the list: 1 fly targets If you see the address of the Concourse you want to use in the list, note down its name, and use it in the login command: 1 fly -t control-plane login Control-plane? We're going to use the name control-plane for our Concourse in this guide. It's not a special name, it just happens to be the name of the Concourse we want to use in our target list. If you don't see the Concourse you need, you can add it with the -c ( --concourse-url )flag: 1 fly -t control-plane login -c https://your-concourse.example.com You should see a login link you can click on to complete login from your browser. Stay on target The -t flag sets the name when used with login and -c . In the future, you can leave out the -c argument. If you ever want to know what a short flag stands for, you can run the command with -h ( --help ) at the end. Pipeline-setting time! We'll use the name \"foundation\" for this pipeline, but if your foundation has an actual name, use that instead. 1 fly -t control-plane set-pipeline -p foundation -c pipeline.yml It should say no changes to apply , which is fair, since we gave it an empty YAML doc. Version discrepancy If fly says something about a \"version discrepancy,\" \"significant\" or otherwise, just do as it says: run fly sync and try again. fly sync automatically updates the CLI with the version that matches the Concourse you're targeting. Useful! Your First Job Let's see Concourse actually do something, yeah? Add this to your pipeline.yml , starting on the line after the --- : 1 wait : no nevermind let's get version control first Good point. Don't actually add that to your pipeline config yet. Or if you have, delete it, so your whole pipeline looks like this again: 1 --- Reverting edits to our pipeline is something we'll probably want to do again. This is one of many reasons we want to keep our pipeline under version control. So let's make this directory a git repo! But First, git init git should come back with information about the commit you just created: 1 2 git init git commit --allow-empty -m \"Empty initial commit\" If it gives you a config error instead, you might need to configure git a bit. Here's a good guide to initial setup. Get that done, and try again. Now we can add our pipeline.yml , so in the future it's easy to get back to that soothing --- state. 1 2 git add pipeline.yml vars.yml git commit -m \"Add pipeline and starter vars\" Let's just make sure we're all tidy: 1 git status git should come back with nothing to commit, working tree clean . Great. Now we can safely make changes. Git commits git commits are the basic unit of code history. Making frequent, small, commits with good commit messages makes it much easier to figure out why things are the way they are, and to return to the way things were in simpler, better times. Writing short commit messages that capture the intent of the change (in an imperative style) can be tough, but it really does make the pipeline's history much more legible, both to future-you, and to current-and-future teammates and collaborators. The Test Task Platform Automation comes with a test task meant to validate that it's been installed correctly. Let's use it to get setup. Add this to your pipeline.yml , starting on the line after the --- : 1 2 3 4 5 6 jobs : - name : test plan : - task : test image : platform-automation-image file : platform-automation-tasks/tasks/test.yml If we try to set this now, Concourse will take it: 1 fly -t control-plane set-pipeline -p foundation -c pipeline.yml Now we should be able to see our pipeline in the Concourse UI. It'll be paused, so click the \"play\" button to unpause it. Then, click in to the gray box for our test job, and hit the \"plus\" button to schedule a build. It should error immediately, with unknown artifact source: platform-automation-tasks . We didn't give it a source for our task file. We've got a bit of pipeline code that Concourse accepts. Before we start doing the next part, this would be a good moment to make a commit: 1 2 git add pipeline.yml git commit -m \"Add (nonfunctional) test task\" With that done, we can try to get the inputs we need by adding get steps to the plan before the task, like so: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 jobs : - name : test plan : - get : platform-automation-image resource : platform-automation params : globs : [ \"*image*.tgz\" ] unpack : true - get : platform-automation-tasks resource : platform-automation params : globs : [ \"*tasks*.zip\" ] unpack : true - task : test image : platform-automation-image file : platform-automation-tasks/tasks/test.yml If we try to fly set this, fly will complain about invalid resources. To actually make the image and file we want to use available, we'll need some Resources. Adding Resources Resources are Concourse's main approach to managing artifacts. We need an image, and the tasks directory - so we'll tell Concourse how to get these things by declaring Resources for them. In this case, we'll be downloading the image and the tasks directory from Pivnet. Before we can declare the resources themselves, we have to teach Concourse to talk to Pivnet. (Many resource types are built in, but this one isn't.) Add the following to your pipeline file. We'll put it above the jobs entry. 1 2 3 4 5 6 7 8 9 10 11 12 resource_types : - name : pivnet type : docker-image source : repository : pivotalcf/pivnet-resource tag : latest-final resources : - name : platform-automation type : pivnet source : product_slug : platform-automation api_token : ((pivnet-refresh-token)) The API token is a credential, which we'll pass via the command-line when setting the pipeline, so we don't accidentally check it in. Grab a refresh token from your Pivnet profile and clicking \"Request New Refresh Token.\" Then use that token in the following command: Keep it secret, keep it safe Bash commands that start with a space character are not saved in your history. This can be very useful for cases like this, where you want to pass a secret, but don't want it saved. Commands in this guide that contain a secret start with a space, which can be easy to miss. 1 2 3 4 5 # note the space before the command fly -t control-plane set-pipeline \\ -p foundation \\ -c pipeline.yml \\ -v pivnet-refresh-token = your-api-token Warning When you get your Pivnet token as described above, any previous Pivnet tokens you may have gotten will stop working. If you're using your Pivnet refresh token anywhere, retrieve it from your existing secret storage rather than getting a new one, or you'll end up needing to update it everywhere it's used. Go back to the Concourse UI and trigger another build. This time, it should pass. Commit time! 1 2 git add pipeline.yml git commit -m \"Add resources needed for test task\" We'd rather not pass our Pivnet token every time we need to set the pipeline. Fortunately, Concourse can integrate with secret storage services. Let's put our API token in Credhub so Concourse can get it. First we'll need to login: Backslashes in bash examples The following example has been broken across multiple lines by using backslash characters ( \\ ) to escape the newlines. We'll be doing this a lot to keep the examples readable. When you're typing these out, you can skip that and just put it all on one line. 1 2 3 4 # again, note the space at the start credhub login --server example.com \\ --client-id your-client-id \\ --client-secret your-client-secret Logging in to credhub Depending on your credential type, you may need to pass client-id and client-secret , as we do above, or username and password . We use the client approach because that's the credential type that automation should usually be working with. Nominally, a username represents a person, and a client represents a system; this isn't always exactly how things are in practice. Use whichever type of credential you have in your case. Note that if you exclude either set of flags, Credhub will interactively prompt for username and password , and hide the characters of your password when you type them. This method of entry can be better in some situations. Then, we can set the credential name to the path where Concourse will look for it : 1 2 3 4 5 # note the starting space credhub set \\ --name /concourse/your-team-name/pivnet-refresh-token \\ --type value \\ --value your-credhub-refresh-token Now, let's set that pipeline again, without passing a secret this time. 1 2 3 fly -t control-plane set-pipeline \\ -p foundation \\ -c pipeline.yml This should succeed, and the diff Concourse shows you should replace the literal credential with ((pivnet-refresh-token)) . Visit the UI again and re-run the test job; this should also succeed. Download Ops Manager product We're finally in a position to do work! Let's switch out the test job for one that downloads and installs Ops Manager. We can do this by changing: the name of the job the name of the task the file of the task Our first task within the job should be download-product . It has an additional required input; we need the config file download-product uses to talk to Pivnet. We'll write that file and make it available as a resource in a moment, for now, we'll just get it (and reference it in our params) as if it's there. It also has an additional output (the downloaded image). We're just going to use it in a subsequent step, so we don't have to put it anywhere. Finally, while it's fine for test to run in parallel, the install process shouldn't. So, we'll add serial: true to the job, too. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 jobs : - name : install-ops-manager serial : true plan : - get : platform-automation-image resource : platform-automation params : globs : [ \"*image*.tgz\" ] unpack : true - get : platform-automation-tasks resource : platform-automation params : globs : [ \"*tasks*.zip\" ] unpack : true - get : config - task : download-product image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : foundation/download-ops-manager.yml If we try to fly this up to Concourse, it will again complain about resources that don't exist. So, let's make them. The first new resource we need is the config file. We'll push our git repo to a remote on Github to make this (and later, other) configuration available to the pipelines. Github has good instructions you can follow to create a new repository on Github. You can skip over the part about using git init to setup your repo, since we already did that . Once you've setup your remote and used git push to send what you've got so far, we can add a new directory to hold foundation-specific configuration. (We'll use the name \"foundation\" for this directory, but if your foundation has an actual name, use that instead.) You will also need to add the repository URL to vars.yml so we can reference it later, when we declare the corresponding resource. 1 pipeline-repo : git@github.com:username/platform-automation-pipelines 1 2 mkdir -p foundation cd !$ download-ops-man.yml holds creds for communicating with Pivnet, and uniquely identifies an Ops Manager image to download. An example download-ops-man.yml is shown below. If your foundation uses authentication other than basic auth, please reference Inputs and Outputs for more detail on UAA-based authentication. Write an download-ops-man.yml for your Ops Manager. AWS 1 2 3 4 5 --- pivnet-api-token : ((pivnet-token)) pivnet-file-glob : \"ops-manager-aws*.yml\" pivnet-product-slug : ops-manager product-version-regex : ^2\\.5\\.\\d+$ Azure 1 2 3 4 5 --- pivnet-api-token : ((pivnet-token)) pivnet-file-glob : \"ops-manager-azure*.yml\" pivnet-product-slug : ops-manager product-version-regex : ^2\\.5\\.\\d+$ GCP 1 2 3 4 5 --- pivnet-api-token : ((pivnet-token)) pivnet-file-glob : \"ops-manager-gcp*.yml\" pivnet-product-slug : ops-manager product-version-regex : ^2\\.5\\.\\d+$ OpenStack 1 2 3 4 5 --- pivnet-api-token : ((pivnet-token)) pivnet-file-glob : \"ops-manager-openstack*.raw\" pivnet-product-slug : ops-manager product-version-regex : ^2\\.5\\.\\d+$ vSphere 1 2 3 4 5 --- pivnet-api-token : ((pivnet-token)) pivnet-file-glob : \"ops-manager-vsphere*.ova\" pivnet-product-slug : ops-manager product-version-regex : ^2\\.5\\.\\d+$ Add and commit the new file: 1 2 3 git add foundation/env.yml git commit -m \"Add environment file for foundation\" git push Now that the env file we need is in our git remote, we need to add a resource to tell Concourse how to get it as env . Since this is (probably) a private repo, we'll need to create a deploy key Concourse can use to access it. Follow Github's instructions for creating a read-only deploy key. Then, put the private key in Credhub so we can use it in our pipeline: 1 2 3 4 5 6 # note the starting space credhub set \\ --name /concourse/your-team-name/plat-auto-pipes-deploy-key \\ --type ssh \\ --private the/filepath/of/the/key-id_rsa \\ --public the/filepath/of/the/key-id_rsa.pub Then, add this to the resources section of your pipeline file: 1 2 3 4 5 6 - name : config type : git source : uri : ((pipeline-repo)) private_key : ((plat-auto-pipes-deploy-key)) branch : master We'll need to put the pivnet token in Crehub: 1 2 3 4 # note the starting space throughout credhub set \\ -n /concourse/your-team-name/foundation/pivnet-token \\ -t value -v your-pivnet-token Credhub paths and pipeline names Notice that we've added an element to the cred paths; now we're using the foundation name. If you look at Concourse's lookup rules, you'll see that it searches the pipeline-specific path before the team path. Since our pipeline is named for the foundation it's used to manage, we can use this to scope access to our foundation-specific information to just this pipeline. By contrast, the Pivnet token may be valuable across several pipelines (and associated foundations), so we scoped that to our team. In order to perform interpolation in one of our input files, we'll need the credhub-interpolate task Earlier, we relied on Concourse's native integration with Credhub for interpolation. That worked because we needed to use the variable in the pipeline itself, not in one of our inputs. We can add it to our job after we've retrieved our download-ops-man.yml input, but before the download-product task: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 jobs : - name : install-ops-manager serial : true plan : - get : platform-automation-image resource : platform-automation params : globs : [ \"*image*.tgz\" ] unpack : true - get : platform-automation-tasks resource : platform-automation params : globs : [ \"*tasks*.zip\" ] unpack : true - get : config - task : credhub-interpolate image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : https://your-credhub.example.com PREFIX : /concourse/your-team-name/foundation INTERPOLATION_PATHS : foundation # contains download-ops-manager.yml input_mapping : files : env output_mapping : interpolated-files : interpolated-config - task : download-product image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : foundation/download-ops-manager.yml input_mapping : config : interpolated-config output_mapping The credhub-interpolate task for this job maps the output from the task ( interpolated-files ) to interpolated-config . This can be used by the next task in the job to more explicitly define the inputs/outputs of each task. It is also okay to leave the output as interpolated-files if it is appropriately referenced in the next task Notice the input mappings of the credhub-interpolate and download-product tasks. This allows us to use the output of one task as in input of another. We now need to put our credhub_client and credhub_secret into Credhub, so Concourse's native integration can retrieve them and pass them as configuration to the credhub-interpolate task. 1 2 3 4 5 6 7 # note the starting space throughout credhub set \\ -n /concourse/your-team-name/credhub-client \\ -t value -v your-credhub-client credhub set \\ -n /concourse/your-team-name/credhub-secret \\ -t value -v your-credhub-secret Now, the credhub-interpolate task will interpolate our config input, and pass it to download-product as config . The job will download the product now. This is a good commit point. 1 2 3 git add pipeline.yml git commit -m 'download the Ops Manager image' git push Creating Resources for Your Ops Manager Before Platform Automation can create a VM for your Ops Manager installation, there are a certain number of resources required by the VM creation and the Ops Manager director installation processes. These resources are created directly on the IaaS of your choice, and read in as configuration for your Ops Manager. There are two main ways of creating these resources, and you should use whichever method is right for you and your setup. Terraform : There are open source terraforming scripts we recommend for use, as they are maintained by the Pivotal organization. These scripts are found in open source repos under the pivotal-cf org in GitHub. terraforming-aws terraforming-azure terraforming-gcp terraforming-openstack terraforming-vsphere Each of these repos contain instructions in their respective README s designed to get you started. Most of the manual keys that you need to fill out will be in a terraform.tfvars file (for more specific instruction, please consult the README ). If there are specific aspects of the terraforming repos that do not work for you, you can overwrite some properties using an override.tf file. Manual Installation : Pivotal has extensive documentation to manually create the resources needed if you are unable or do not wish to use Terraform. As with the Terraform solution, however, there are different docs depending on the IaaS you are installing Ops Manager onto. When going through the documentation required for your IaaS, be sure to stop before deploying the Ops Manager image. Platform Automation will do this for you. aws azure gcp openstack vsphere NOTE : if you need to install an earlier version of Ops Manager, select your desired version from the dropdown at the top of the page. Create the Ops Manager VM Now that we have an Ops Manager image and the resources required to deploy a VM, let's add the new task to the install-opsman job. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 jobs : - name : install-ops-manager serial : true plan : - get : platform-automation-image resource : platform-automation params : globs : [ \"*image*.tgz\" ] unpack : true - get : platform-automation-tasks resource : platform-automation params : globs : [ \"*tasks*.zip\" ] unpack : true - get : config - task : credhub-interpolate image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : https://your-credhub.example.com PREFIX : /concourse/your-team-name/foundation INTERPOLATION_PATHS : foundation # contains download-ops-manager.yml input_mapping : files : env output_mapping : interpolated-files : interpolated-config - task : download-product image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : foundation/download-ops-manager.yml input_mapping : config : interpolated-config - task : create-vm image : platform-automation-image file : platform-automation-tasks/tasks/create-vm.yml If we try to fly this up to Concourse, it will again complain about resources that don't exist. So, let's make them. Looking over the list of inputs for create-vm we still need two required inputs: config state The optional inputs are vars used with the config, so we'll get to those when we do config . Let's start with the config file. We'll write an Ops Manager VM Configuration file to foundation/opsman.yml . The properties available vary by IaaS, for example: IaaS credentials networking setup (IP address, subnet, security group, etc) ssh key datacenter/availability zone/region AWS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- opsman-configuration : aws : region : us-west-2 vm_name : ops-manager-foundation vpc_subnet_id : subnet-0292bc845215c2cbf security_group_id : sg-0354f804ba7c4bc41 key_pair_name : ops-manager-key private_ip : 10.0.0.2 use_instance_profile : true iam_instance_profile_name : ops-manager-iam # Note that because this config contains no secrets # and is already written to a foundation-specific filepath, # it need not be interpolated. Azure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 --- opsman-configuration : azure : subscription_id : 90f35f10-ea9e-4e80-aac4-d6778b995532 resource_group : res-group tenant_id : 3e52862f-a01e-4b97-98d5-f31a409df682 client_id : 5782deb6-9195-4827-83ae-a13fda90aa0d client_secret : ((opsman-client-secret)) location : westus container : opsmanagerimage network_security_group : ops-manager-security-group vpc_subnet : /subscriptions/<MY_SUBSCRIPTION_ID>/resourceGroups/<MY_RESOURCE_GROUP>/providers/Microsoft.Network/virtualNetworks/<MY_VNET>/subnets/<MY_SUBNET> storage_account : opsman # used for the Ops Manager image ssh_public_key : ssh-rsa AAAAB3NzaC1yc2EAZ... # public key for ssh to VM vm_name : ops-manager-foundation private_ip : 10.0.0.3 # Note that as this contains a secret, it will need to be interpolated. # The opsman-client-secret will need to be set in credhub. GCP 1 2 3 4 5 6 7 8 9 10 11 12 13 14 --- opsman-configuration : gcp : gcp_service_account : ((gcp_service_account_json)) project : project-id region : us-central1 zone : us-central1-b vm_name : ops-manager-foundation vpc_subnet : infrastructure-subnet tags : ops-manager private_ip : 10.0.0.2 # Note that as this contains a secret, it will need to be interpolated. # The gcp_service_account_json will need to be set in credhub. OpenStack 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 --- opsman-configuration : openstack : auth_url : http://os.example.com:5000/v2.0 project_name : project net_id : 26a13112-b6c2-11e8-96f8-529269fb1459 username : admin password : ((opsman-openstack-password)) key_pair_name : opsman-keypair security_group_name : opsman-sec-group vm_name : ops-manager-foundation private_ip : 10.0.0.3 project_domain_name : default user_domain_name : default availability_zone : zone-01 # Note that as this contains a secret, it will need to be interpolated. # The opsman-openstack-password will need to be set in credhub. vSphere 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 --- opsman-configuration : vsphere : vcenter : url : vcenter.example.com username : admin password : ((vcenter-password)) datastore : exmple-ds-1 ca_cert : certificate host : example-host # vCenter host to deploy Ops Manager on datacenter : example-dc resource_pool : /example-dc/host/example-host/Resources/ResPool folder : /example-dc/vm/Folder disk_type : thin private_ip : 10.0.0.2 dns : 8.8.8.8 ntp : ntp.example.com ssh_public_key : ssh-rsa ...... hostname : pcf.example.com network : virtual-network # vcenter network to deploy to netmask : 255.255.255.192 gateway : 192.168.10.1 vm_name : ops-manager-foundation # Note that as this contains a secret, it will need to be interpolated. # The vcenter-password will need to be set in credhub. These examples all make assumptions about the details of your soon-to-be Ops Manager's configuration. See the reference docs for this file for more details about your options and per-IaaS caveats. Once you have your config file, commit and push it: 1 2 3 git add foundation/opsman.yml git commit -m \"Add opsman config\" git push The state input is a placeholder which will be filled in by the create-vm task output. This will be used later to keep track of the vm so it can be upgraded, which you can learn about in the upgrade-how-to . The create-vm task in the install-opsman will need to be updated to use the download-product image, Ops Manager configuration file, and the placeholder state file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 jobs : - name : install-ops-manager serial : true plan : - get : platform-automation-image resource : platform-automation params : globs : [ \"*image*.tgz\" ] unpack : true - get : platform-automation-tasks resource : platform-automation params : globs : [ \"*tasks*.zip\" ] unpack : true - get : config - task : credhub-interpolate image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : https://your-credhub.example.com PREFIX : /concourse/your-team-name/foundation INTERPOLATION_PATHS : foundation # contains download-ops-manager.yml input_mapping : files : env output_mapping : interpolated-files : interpolated-config - task : download-product image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : foundation/download-ops-manager.yml input_mapping : config : interpolated-config - task : create-vm image : platform-automation-image file : platform-automation-tasks/tasks/create-vm.yml params : OPSMAN_CONFIG_FILE : foundation/opsman.yml STATE_FILE : foundation/state.yml input_mapping : config : interpolated-config state : config image : downloaded-product Set the pipeline. Before we run the job, we should ensure that state.yml is always persisted regardless of whether the install-opsman job failed or passed. To do this, we can add the following section to the job: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 jobs : - name : install-ops-manager serial : true plan : - get : platform-automation-image resource : platform-automation params : globs : [ \"*image*.tgz\" ] unpack : true - get : platform-automation-tasks resource : platform-automation params : globs : [ \"*tasks*.zip\" ] unpack : true - get : config - task : credhub-interpolate image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : https://your-credhub.example.com PREFIX : /concourse/your-team-name/foundation INTERPOLATION_PATHS : foundation # contains download-ops-manager.yml input_mapping : files : env output_mapping : interpolated-files : interpolated-config - task : download-product image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : foundation/download-ops-manager.yml input_mapping : config : interpolated-config - task : create-vm image : platform-automation-image file : platform-automation-tasks/tasks/create-vm.yml params : OPSMAN_CONFIG_FILE : foundation/opsman.yml STATE_FILE : foundation/state.yml input_mapping : config : interpolated-config state : config image : downloaded-product ensure : do : - task : make-commit image : platform-automation-image file : platform-automation-tasks/tasks/make-git-commit.yml input_mapping : repository : config file-source : generated-state output_mapping : repository-commit : config-commit params : FILE_SOURCE_PATH : state.yml FILE_DESTINATION_PATH : ((foundation))/state/state.yml GIT_AUTHOR_EMAIL : \"pcf-pipeline-bot@example.com\" GIT_AUTHOR_NAME : \"Platform Automation Bot\" COMMIT_MESSAGE : 'Update state file' - put : config params : repository : config-commit merge : true Set the pipeline one final time, run the job, and see it pass. 1 2 3 git add pipeline.yml git commit -m \"Install Ops Manager in CI\" git push Your install pipeline is now complete. You are now free to move on to the next steps of your automation journey.","title":"Installing Ops Manager"},{"location":"how-to-guides/install-opsman.html#writing-a-pipeline-to-install-ops-manager","text":"","title":"Writing a Pipeline to Install Ops Manager"},{"location":"how-to-guides/install-opsman.html#prerequisites","text":"Over the course of this guide, we're going to use Platform Automation for PCF to create a pipeline using Concourse . Before we get started, you'll need a few things ready to go: Credentials for an IaaS that Ops Manager is compatible with A Concourse instance with access to a Credhub instance and to the Internet Github.com account Read/write credentials and bucket name for an S3 bucket An account on https://network.pivotal.io (Pivnet) A MacOS workstation with Docker installed and a text editor you like a terminal emulator you like a browser that works with Concourse, like Firefox or Chrome and git IaaS It doesn't actually matter what IaaS you use for Ops Manager, as long as your Concourse can connect to it. Pipelines built with Platform Automation can be platform-agnostic. It will be very helpful to have a basic familiarity with the following. If you don't have basic familiarity with all these things, that's okay. We'll explain some basics, and link to resources to learn more: the bash terminal git YAML Concourse A note on the prerequisites While this guide uses Github to provide a git remote, and an S3 bucket as a blobstore, Platform Automation supports arbitrary git providers and S3-compatible blobstores. If you need to use an alternate one, that's okay. We picked specific examples so we could describe some steps in detail. Some details may be different if you follow along with different providers. If you're comfortable navigating those differences on your own, go for it! Similarly, in this guide, we assume the MacOS operating system. This should all work fine on Linux, too, but there might be differences in the paths you'll need to figure out.","title":"Prerequisites"},{"location":"how-to-guides/install-opsman.html#creating-a-concourse-pipeline","text":"Platform Automation's tasks and image are meant to be used in a Concourse pipeline. So, let's make one. Using your bash command-line client, create a directory to keep your pipeline files in, and cd into it. 1 2 mkdir platform-automation-pipelines cd !$ \" !$ \" !$ is a bash shortcut. Pronounced \"bang, dollar-sign,\" it means \"use the last argument from the most recent command.\" In this case, that's the directory we just created! This is not a Platform Automation thing, this is just a bash tip dearly beloved of at least one Platform Automator. Before we get started with the pipeline itself, we'll gather some variables in a file we can use throughout our pipeline. Open your text editor and create vars.yml . Here's what it should look like to start, we can add things to this as we go: 1 2 3 platform-automation-bucket : your-bucket-name credhub-server : https://your-credhub.example.com opsman-url : https://pcf.foundation.example.com Using a DNS This example assumes that you're using DNS and hostnames. You can use IP addresses for all these resources instead, but you still need to provide the information as a URL, for example: https://120.121.123.124 Now, create a file called pipeline.yml . Naming We'll use pipeline.yml in our examples throughout this guide. However, you may create multiple pipelines over time. If there's a more sensible name for the pipeline you're working on, feel free to use that instead. Write this at the top, and save the file. This is YAML for \"the start of the document. It's optional, but traditional: 1 --- Now you have a pipeline file! Nominally! Well, look. It's valid YAML, at least.","title":"Creating a Concourse Pipeline"},{"location":"how-to-guides/install-opsman.html#getting-fly","text":"Let's try to set it as a pipeline with fly , the Concourse command-line Interface (CLI). First, check if we've got fly installed at all: 1 fly -v If it gives you back a version number, great! Skip ahead to Setting The Pipeline If it says something like -bash: fly: command not found , we have a little work to do; we've got to get fly . Navigate to the address for your Concourse instance in a web browser. At this point, you don't even need to be signed in! If there are no public pipelines, you should see something like this: If there are public pipelines, or if you're signed in and there are pipelines you can see, you'll see something similar in the lower-right hand corner. Click the icon for your OS and save the file, mv the resulting file to somewhere in your $PATH , and use chmod to make it executable: A note on command-line examples Some of these, you can copy-paste directly into your terminal. Some of them won't work that way, or even if they did, would require you to edit them to replace our example values with your actual values. We recommend you type all of the bash examples in by hand, substituting values, if necessary, as you go. Don't forget that you can often hit the tab key to auto-complete the name of files that already exist; it makes all that typing just a little easier, and serves as a sort of command-line autocorrect. 1 2 mv ~/Downloads/fly /usr/local/bin/fly chmod +x !$ Congrats! You got fly . Okay but what did I just do? FAIR QUESTION. You downloaded the fly binary, moved it into bash's PATH, which is where bash looks for things to execute when you type a command, and then added permissions that allow it to be e x ecuted. Now, the CLI is installed - and we won't have to do all that again, because fly has the ability to update itself, which we'll get into later.","title":"Getting fly"},{"location":"how-to-guides/install-opsman.html#setting-the-pipeline","text":"Okay now let's try to set our pipeline with fly , the Concourse CLI. fly keeps a list of Concourses it knows how to talk to. Let's see if the Concourse we want is already on the list: 1 fly targets If you see the address of the Concourse you want to use in the list, note down its name, and use it in the login command: 1 fly -t control-plane login Control-plane? We're going to use the name control-plane for our Concourse in this guide. It's not a special name, it just happens to be the name of the Concourse we want to use in our target list. If you don't see the Concourse you need, you can add it with the -c ( --concourse-url )flag: 1 fly -t control-plane login -c https://your-concourse.example.com You should see a login link you can click on to complete login from your browser. Stay on target The -t flag sets the name when used with login and -c . In the future, you can leave out the -c argument. If you ever want to know what a short flag stands for, you can run the command with -h ( --help ) at the end. Pipeline-setting time! We'll use the name \"foundation\" for this pipeline, but if your foundation has an actual name, use that instead. 1 fly -t control-plane set-pipeline -p foundation -c pipeline.yml It should say no changes to apply , which is fair, since we gave it an empty YAML doc. Version discrepancy If fly says something about a \"version discrepancy,\" \"significant\" or otherwise, just do as it says: run fly sync and try again. fly sync automatically updates the CLI with the version that matches the Concourse you're targeting. Useful!","title":"Setting The Pipeline"},{"location":"how-to-guides/install-opsman.html#your-first-job","text":"Let's see Concourse actually do something, yeah? Add this to your pipeline.yml , starting on the line after the --- : 1 wait : no nevermind let's get version control first Good point. Don't actually add that to your pipeline config yet. Or if you have, delete it, so your whole pipeline looks like this again: 1 --- Reverting edits to our pipeline is something we'll probably want to do again. This is one of many reasons we want to keep our pipeline under version control. So let's make this directory a git repo!","title":"Your First Job"},{"location":"how-to-guides/install-opsman.html#but-first-git-init","text":"git should come back with information about the commit you just created: 1 2 git init git commit --allow-empty -m \"Empty initial commit\" If it gives you a config error instead, you might need to configure git a bit. Here's a good guide to initial setup. Get that done, and try again. Now we can add our pipeline.yml , so in the future it's easy to get back to that soothing --- state. 1 2 git add pipeline.yml vars.yml git commit -m \"Add pipeline and starter vars\" Let's just make sure we're all tidy: 1 git status git should come back with nothing to commit, working tree clean . Great. Now we can safely make changes. Git commits git commits are the basic unit of code history. Making frequent, small, commits with good commit messages makes it much easier to figure out why things are the way they are, and to return to the way things were in simpler, better times. Writing short commit messages that capture the intent of the change (in an imperative style) can be tough, but it really does make the pipeline's history much more legible, both to future-you, and to current-and-future teammates and collaborators.","title":"But First, git init"},{"location":"how-to-guides/install-opsman.html#the-test-task","text":"Platform Automation comes with a test task meant to validate that it's been installed correctly. Let's use it to get setup. Add this to your pipeline.yml , starting on the line after the --- : 1 2 3 4 5 6 jobs : - name : test plan : - task : test image : platform-automation-image file : platform-automation-tasks/tasks/test.yml If we try to set this now, Concourse will take it: 1 fly -t control-plane set-pipeline -p foundation -c pipeline.yml Now we should be able to see our pipeline in the Concourse UI. It'll be paused, so click the \"play\" button to unpause it. Then, click in to the gray box for our test job, and hit the \"plus\" button to schedule a build. It should error immediately, with unknown artifact source: platform-automation-tasks . We didn't give it a source for our task file. We've got a bit of pipeline code that Concourse accepts. Before we start doing the next part, this would be a good moment to make a commit: 1 2 git add pipeline.yml git commit -m \"Add (nonfunctional) test task\" With that done, we can try to get the inputs we need by adding get steps to the plan before the task, like so: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 jobs : - name : test plan : - get : platform-automation-image resource : platform-automation params : globs : [ \"*image*.tgz\" ] unpack : true - get : platform-automation-tasks resource : platform-automation params : globs : [ \"*tasks*.zip\" ] unpack : true - task : test image : platform-automation-image file : platform-automation-tasks/tasks/test.yml If we try to fly set this, fly will complain about invalid resources. To actually make the image and file we want to use available, we'll need some Resources.","title":"The Test Task"},{"location":"how-to-guides/install-opsman.html#adding-resources","text":"Resources are Concourse's main approach to managing artifacts. We need an image, and the tasks directory - so we'll tell Concourse how to get these things by declaring Resources for them. In this case, we'll be downloading the image and the tasks directory from Pivnet. Before we can declare the resources themselves, we have to teach Concourse to talk to Pivnet. (Many resource types are built in, but this one isn't.) Add the following to your pipeline file. We'll put it above the jobs entry. 1 2 3 4 5 6 7 8 9 10 11 12 resource_types : - name : pivnet type : docker-image source : repository : pivotalcf/pivnet-resource tag : latest-final resources : - name : platform-automation type : pivnet source : product_slug : platform-automation api_token : ((pivnet-refresh-token)) The API token is a credential, which we'll pass via the command-line when setting the pipeline, so we don't accidentally check it in. Grab a refresh token from your Pivnet profile and clicking \"Request New Refresh Token.\" Then use that token in the following command: Keep it secret, keep it safe Bash commands that start with a space character are not saved in your history. This can be very useful for cases like this, where you want to pass a secret, but don't want it saved. Commands in this guide that contain a secret start with a space, which can be easy to miss. 1 2 3 4 5 # note the space before the command fly -t control-plane set-pipeline \\ -p foundation \\ -c pipeline.yml \\ -v pivnet-refresh-token = your-api-token Warning When you get your Pivnet token as described above, any previous Pivnet tokens you may have gotten will stop working. If you're using your Pivnet refresh token anywhere, retrieve it from your existing secret storage rather than getting a new one, or you'll end up needing to update it everywhere it's used. Go back to the Concourse UI and trigger another build. This time, it should pass. Commit time! 1 2 git add pipeline.yml git commit -m \"Add resources needed for test task\" We'd rather not pass our Pivnet token every time we need to set the pipeline. Fortunately, Concourse can integrate with secret storage services. Let's put our API token in Credhub so Concourse can get it. First we'll need to login: Backslashes in bash examples The following example has been broken across multiple lines by using backslash characters ( \\ ) to escape the newlines. We'll be doing this a lot to keep the examples readable. When you're typing these out, you can skip that and just put it all on one line. 1 2 3 4 # again, note the space at the start credhub login --server example.com \\ --client-id your-client-id \\ --client-secret your-client-secret Logging in to credhub Depending on your credential type, you may need to pass client-id and client-secret , as we do above, or username and password . We use the client approach because that's the credential type that automation should usually be working with. Nominally, a username represents a person, and a client represents a system; this isn't always exactly how things are in practice. Use whichever type of credential you have in your case. Note that if you exclude either set of flags, Credhub will interactively prompt for username and password , and hide the characters of your password when you type them. This method of entry can be better in some situations. Then, we can set the credential name to the path where Concourse will look for it : 1 2 3 4 5 # note the starting space credhub set \\ --name /concourse/your-team-name/pivnet-refresh-token \\ --type value \\ --value your-credhub-refresh-token Now, let's set that pipeline again, without passing a secret this time. 1 2 3 fly -t control-plane set-pipeline \\ -p foundation \\ -c pipeline.yml This should succeed, and the diff Concourse shows you should replace the literal credential with ((pivnet-refresh-token)) . Visit the UI again and re-run the test job; this should also succeed.","title":"Adding Resources"},{"location":"how-to-guides/install-opsman.html#download-ops-manager-product","text":"We're finally in a position to do work! Let's switch out the test job for one that downloads and installs Ops Manager. We can do this by changing: the name of the job the name of the task the file of the task Our first task within the job should be download-product . It has an additional required input; we need the config file download-product uses to talk to Pivnet. We'll write that file and make it available as a resource in a moment, for now, we'll just get it (and reference it in our params) as if it's there. It also has an additional output (the downloaded image). We're just going to use it in a subsequent step, so we don't have to put it anywhere. Finally, while it's fine for test to run in parallel, the install process shouldn't. So, we'll add serial: true to the job, too. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 jobs : - name : install-ops-manager serial : true plan : - get : platform-automation-image resource : platform-automation params : globs : [ \"*image*.tgz\" ] unpack : true - get : platform-automation-tasks resource : platform-automation params : globs : [ \"*tasks*.zip\" ] unpack : true - get : config - task : download-product image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : foundation/download-ops-manager.yml If we try to fly this up to Concourse, it will again complain about resources that don't exist. So, let's make them. The first new resource we need is the config file. We'll push our git repo to a remote on Github to make this (and later, other) configuration available to the pipelines. Github has good instructions you can follow to create a new repository on Github. You can skip over the part about using git init to setup your repo, since we already did that . Once you've setup your remote and used git push to send what you've got so far, we can add a new directory to hold foundation-specific configuration. (We'll use the name \"foundation\" for this directory, but if your foundation has an actual name, use that instead.) You will also need to add the repository URL to vars.yml so we can reference it later, when we declare the corresponding resource. 1 pipeline-repo : git@github.com:username/platform-automation-pipelines 1 2 mkdir -p foundation cd !$ download-ops-man.yml holds creds for communicating with Pivnet, and uniquely identifies an Ops Manager image to download. An example download-ops-man.yml is shown below. If your foundation uses authentication other than basic auth, please reference Inputs and Outputs for more detail on UAA-based authentication. Write an download-ops-man.yml for your Ops Manager. AWS 1 2 3 4 5 --- pivnet-api-token : ((pivnet-token)) pivnet-file-glob : \"ops-manager-aws*.yml\" pivnet-product-slug : ops-manager product-version-regex : ^2\\.5\\.\\d+$ Azure 1 2 3 4 5 --- pivnet-api-token : ((pivnet-token)) pivnet-file-glob : \"ops-manager-azure*.yml\" pivnet-product-slug : ops-manager product-version-regex : ^2\\.5\\.\\d+$ GCP 1 2 3 4 5 --- pivnet-api-token : ((pivnet-token)) pivnet-file-glob : \"ops-manager-gcp*.yml\" pivnet-product-slug : ops-manager product-version-regex : ^2\\.5\\.\\d+$ OpenStack 1 2 3 4 5 --- pivnet-api-token : ((pivnet-token)) pivnet-file-glob : \"ops-manager-openstack*.raw\" pivnet-product-slug : ops-manager product-version-regex : ^2\\.5\\.\\d+$ vSphere 1 2 3 4 5 --- pivnet-api-token : ((pivnet-token)) pivnet-file-glob : \"ops-manager-vsphere*.ova\" pivnet-product-slug : ops-manager product-version-regex : ^2\\.5\\.\\d+$ Add and commit the new file: 1 2 3 git add foundation/env.yml git commit -m \"Add environment file for foundation\" git push Now that the env file we need is in our git remote, we need to add a resource to tell Concourse how to get it as env . Since this is (probably) a private repo, we'll need to create a deploy key Concourse can use to access it. Follow Github's instructions for creating a read-only deploy key. Then, put the private key in Credhub so we can use it in our pipeline: 1 2 3 4 5 6 # note the starting space credhub set \\ --name /concourse/your-team-name/plat-auto-pipes-deploy-key \\ --type ssh \\ --private the/filepath/of/the/key-id_rsa \\ --public the/filepath/of/the/key-id_rsa.pub Then, add this to the resources section of your pipeline file: 1 2 3 4 5 6 - name : config type : git source : uri : ((pipeline-repo)) private_key : ((plat-auto-pipes-deploy-key)) branch : master We'll need to put the pivnet token in Crehub: 1 2 3 4 # note the starting space throughout credhub set \\ -n /concourse/your-team-name/foundation/pivnet-token \\ -t value -v your-pivnet-token Credhub paths and pipeline names Notice that we've added an element to the cred paths; now we're using the foundation name. If you look at Concourse's lookup rules, you'll see that it searches the pipeline-specific path before the team path. Since our pipeline is named for the foundation it's used to manage, we can use this to scope access to our foundation-specific information to just this pipeline. By contrast, the Pivnet token may be valuable across several pipelines (and associated foundations), so we scoped that to our team. In order to perform interpolation in one of our input files, we'll need the credhub-interpolate task Earlier, we relied on Concourse's native integration with Credhub for interpolation. That worked because we needed to use the variable in the pipeline itself, not in one of our inputs. We can add it to our job after we've retrieved our download-ops-man.yml input, but before the download-product task: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 jobs : - name : install-ops-manager serial : true plan : - get : platform-automation-image resource : platform-automation params : globs : [ \"*image*.tgz\" ] unpack : true - get : platform-automation-tasks resource : platform-automation params : globs : [ \"*tasks*.zip\" ] unpack : true - get : config - task : credhub-interpolate image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : https://your-credhub.example.com PREFIX : /concourse/your-team-name/foundation INTERPOLATION_PATHS : foundation # contains download-ops-manager.yml input_mapping : files : env output_mapping : interpolated-files : interpolated-config - task : download-product image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : foundation/download-ops-manager.yml input_mapping : config : interpolated-config output_mapping The credhub-interpolate task for this job maps the output from the task ( interpolated-files ) to interpolated-config . This can be used by the next task in the job to more explicitly define the inputs/outputs of each task. It is also okay to leave the output as interpolated-files if it is appropriately referenced in the next task Notice the input mappings of the credhub-interpolate and download-product tasks. This allows us to use the output of one task as in input of another. We now need to put our credhub_client and credhub_secret into Credhub, so Concourse's native integration can retrieve them and pass them as configuration to the credhub-interpolate task. 1 2 3 4 5 6 7 # note the starting space throughout credhub set \\ -n /concourse/your-team-name/credhub-client \\ -t value -v your-credhub-client credhub set \\ -n /concourse/your-team-name/credhub-secret \\ -t value -v your-credhub-secret Now, the credhub-interpolate task will interpolate our config input, and pass it to download-product as config . The job will download the product now. This is a good commit point. 1 2 3 git add pipeline.yml git commit -m 'download the Ops Manager image' git push","title":"Download Ops Manager product"},{"location":"how-to-guides/install-opsman.html#creating-resources-for-your-ops-manager","text":"Before Platform Automation can create a VM for your Ops Manager installation, there are a certain number of resources required by the VM creation and the Ops Manager director installation processes. These resources are created directly on the IaaS of your choice, and read in as configuration for your Ops Manager. There are two main ways of creating these resources, and you should use whichever method is right for you and your setup. Terraform : There are open source terraforming scripts we recommend for use, as they are maintained by the Pivotal organization. These scripts are found in open source repos under the pivotal-cf org in GitHub. terraforming-aws terraforming-azure terraforming-gcp terraforming-openstack terraforming-vsphere Each of these repos contain instructions in their respective README s designed to get you started. Most of the manual keys that you need to fill out will be in a terraform.tfvars file (for more specific instruction, please consult the README ). If there are specific aspects of the terraforming repos that do not work for you, you can overwrite some properties using an override.tf file. Manual Installation : Pivotal has extensive documentation to manually create the resources needed if you are unable or do not wish to use Terraform. As with the Terraform solution, however, there are different docs depending on the IaaS you are installing Ops Manager onto. When going through the documentation required for your IaaS, be sure to stop before deploying the Ops Manager image. Platform Automation will do this for you. aws azure gcp openstack vsphere NOTE : if you need to install an earlier version of Ops Manager, select your desired version from the dropdown at the top of the page.","title":"Creating Resources for Your Ops Manager"},{"location":"how-to-guides/install-opsman.html#create-the-ops-manager-vm","text":"Now that we have an Ops Manager image and the resources required to deploy a VM, let's add the new task to the install-opsman job. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 jobs : - name : install-ops-manager serial : true plan : - get : platform-automation-image resource : platform-automation params : globs : [ \"*image*.tgz\" ] unpack : true - get : platform-automation-tasks resource : platform-automation params : globs : [ \"*tasks*.zip\" ] unpack : true - get : config - task : credhub-interpolate image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : https://your-credhub.example.com PREFIX : /concourse/your-team-name/foundation INTERPOLATION_PATHS : foundation # contains download-ops-manager.yml input_mapping : files : env output_mapping : interpolated-files : interpolated-config - task : download-product image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : foundation/download-ops-manager.yml input_mapping : config : interpolated-config - task : create-vm image : platform-automation-image file : platform-automation-tasks/tasks/create-vm.yml If we try to fly this up to Concourse, it will again complain about resources that don't exist. So, let's make them. Looking over the list of inputs for create-vm we still need two required inputs: config state The optional inputs are vars used with the config, so we'll get to those when we do config . Let's start with the config file. We'll write an Ops Manager VM Configuration file to foundation/opsman.yml . The properties available vary by IaaS, for example: IaaS credentials networking setup (IP address, subnet, security group, etc) ssh key datacenter/availability zone/region AWS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- opsman-configuration : aws : region : us-west-2 vm_name : ops-manager-foundation vpc_subnet_id : subnet-0292bc845215c2cbf security_group_id : sg-0354f804ba7c4bc41 key_pair_name : ops-manager-key private_ip : 10.0.0.2 use_instance_profile : true iam_instance_profile_name : ops-manager-iam # Note that because this config contains no secrets # and is already written to a foundation-specific filepath, # it need not be interpolated. Azure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 --- opsman-configuration : azure : subscription_id : 90f35f10-ea9e-4e80-aac4-d6778b995532 resource_group : res-group tenant_id : 3e52862f-a01e-4b97-98d5-f31a409df682 client_id : 5782deb6-9195-4827-83ae-a13fda90aa0d client_secret : ((opsman-client-secret)) location : westus container : opsmanagerimage network_security_group : ops-manager-security-group vpc_subnet : /subscriptions/<MY_SUBSCRIPTION_ID>/resourceGroups/<MY_RESOURCE_GROUP>/providers/Microsoft.Network/virtualNetworks/<MY_VNET>/subnets/<MY_SUBNET> storage_account : opsman # used for the Ops Manager image ssh_public_key : ssh-rsa AAAAB3NzaC1yc2EAZ... # public key for ssh to VM vm_name : ops-manager-foundation private_ip : 10.0.0.3 # Note that as this contains a secret, it will need to be interpolated. # The opsman-client-secret will need to be set in credhub. GCP 1 2 3 4 5 6 7 8 9 10 11 12 13 14 --- opsman-configuration : gcp : gcp_service_account : ((gcp_service_account_json)) project : project-id region : us-central1 zone : us-central1-b vm_name : ops-manager-foundation vpc_subnet : infrastructure-subnet tags : ops-manager private_ip : 10.0.0.2 # Note that as this contains a secret, it will need to be interpolated. # The gcp_service_account_json will need to be set in credhub. OpenStack 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 --- opsman-configuration : openstack : auth_url : http://os.example.com:5000/v2.0 project_name : project net_id : 26a13112-b6c2-11e8-96f8-529269fb1459 username : admin password : ((opsman-openstack-password)) key_pair_name : opsman-keypair security_group_name : opsman-sec-group vm_name : ops-manager-foundation private_ip : 10.0.0.3 project_domain_name : default user_domain_name : default availability_zone : zone-01 # Note that as this contains a secret, it will need to be interpolated. # The opsman-openstack-password will need to be set in credhub. vSphere 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 --- opsman-configuration : vsphere : vcenter : url : vcenter.example.com username : admin password : ((vcenter-password)) datastore : exmple-ds-1 ca_cert : certificate host : example-host # vCenter host to deploy Ops Manager on datacenter : example-dc resource_pool : /example-dc/host/example-host/Resources/ResPool folder : /example-dc/vm/Folder disk_type : thin private_ip : 10.0.0.2 dns : 8.8.8.8 ntp : ntp.example.com ssh_public_key : ssh-rsa ...... hostname : pcf.example.com network : virtual-network # vcenter network to deploy to netmask : 255.255.255.192 gateway : 192.168.10.1 vm_name : ops-manager-foundation # Note that as this contains a secret, it will need to be interpolated. # The vcenter-password will need to be set in credhub. These examples all make assumptions about the details of your soon-to-be Ops Manager's configuration. See the reference docs for this file for more details about your options and per-IaaS caveats. Once you have your config file, commit and push it: 1 2 3 git add foundation/opsman.yml git commit -m \"Add opsman config\" git push The state input is a placeholder which will be filled in by the create-vm task output. This will be used later to keep track of the vm so it can be upgraded, which you can learn about in the upgrade-how-to . The create-vm task in the install-opsman will need to be updated to use the download-product image, Ops Manager configuration file, and the placeholder state file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 jobs : - name : install-ops-manager serial : true plan : - get : platform-automation-image resource : platform-automation params : globs : [ \"*image*.tgz\" ] unpack : true - get : platform-automation-tasks resource : platform-automation params : globs : [ \"*tasks*.zip\" ] unpack : true - get : config - task : credhub-interpolate image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : https://your-credhub.example.com PREFIX : /concourse/your-team-name/foundation INTERPOLATION_PATHS : foundation # contains download-ops-manager.yml input_mapping : files : env output_mapping : interpolated-files : interpolated-config - task : download-product image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : foundation/download-ops-manager.yml input_mapping : config : interpolated-config - task : create-vm image : platform-automation-image file : platform-automation-tasks/tasks/create-vm.yml params : OPSMAN_CONFIG_FILE : foundation/opsman.yml STATE_FILE : foundation/state.yml input_mapping : config : interpolated-config state : config image : downloaded-product Set the pipeline. Before we run the job, we should ensure that state.yml is always persisted regardless of whether the install-opsman job failed or passed. To do this, we can add the following section to the job: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 jobs : - name : install-ops-manager serial : true plan : - get : platform-automation-image resource : platform-automation params : globs : [ \"*image*.tgz\" ] unpack : true - get : platform-automation-tasks resource : platform-automation params : globs : [ \"*tasks*.zip\" ] unpack : true - get : config - task : credhub-interpolate image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : https://your-credhub.example.com PREFIX : /concourse/your-team-name/foundation INTERPOLATION_PATHS : foundation # contains download-ops-manager.yml input_mapping : files : env output_mapping : interpolated-files : interpolated-config - task : download-product image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : foundation/download-ops-manager.yml input_mapping : config : interpolated-config - task : create-vm image : platform-automation-image file : platform-automation-tasks/tasks/create-vm.yml params : OPSMAN_CONFIG_FILE : foundation/opsman.yml STATE_FILE : foundation/state.yml input_mapping : config : interpolated-config state : config image : downloaded-product ensure : do : - task : make-commit image : platform-automation-image file : platform-automation-tasks/tasks/make-git-commit.yml input_mapping : repository : config file-source : generated-state output_mapping : repository-commit : config-commit params : FILE_SOURCE_PATH : state.yml FILE_DESTINATION_PATH : ((foundation))/state/state.yml GIT_AUTHOR_EMAIL : \"pcf-pipeline-bot@example.com\" GIT_AUTHOR_NAME : \"Platform Automation Bot\" COMMIT_MESSAGE : 'Update state file' - put : config params : repository : config-commit merge : true Set the pipeline one final time, run the job, and see it pass. 1 2 3 git add pipeline.yml git commit -m \"Install Ops Manager in CI\" git push Your install pipeline is now complete. You are now free to move on to the next steps of your automation journey.","title":"Create the Ops Manager VM"},{"location":"how-to-guides/upgrade-existing-opsman.html","text":"Writing a Pipeline to Upgrade an Existing Ops Manager Prerequisites Over the course of this guide, we're going to use Platform Automation for PCF to create a pipeline using Concourse . Before we get started, you'll need a few things ready to go: A running Ops Manager VM that you would like to upgrade Credentials for an IaaS that Ops Manager is compatible with A Concourse instance with access to a Credhub instance and to the Internet Github.com account Read/write credentials and bucket name for an S3 bucket An account on https://network.pivotal.io (Pivnet) A MacOS workstation with Docker installed and a text editor you like a terminal emulator you like a browser that works with Concourse, like Firefox or Chrome and git IaaS It doesn't actually matter what IaaS you use for Ops Manager, as long as your Concourse can connect to it. Pipelines built with Platform Automation can be platform-agnostic. It will be very helpful to have a basic familiarity with the following. If you don't have basic familiarity with all these things, that's okay. We'll explain some basics, and link to resources to learn more: the bash terminal git YAML Concourse A note on the prerequisites While this guide uses Github to provide a git remote, and an S3 bucket as a blobstore, Platform Automation supports arbitrary git providers and S3-compatible blobstores. If you need to use an alternate one, that's okay. We picked specific examples so we could describe some steps in detail. Some details may be different if you follow along with different providers. If you're comfortable navigating those differences on your own, go for it! Similarly, in this guide, we assume the MacOS operating system. This should all work fine on Linux, too, but there might be differences in the paths you'll need to figure out. Creating a Concourse Pipeline Platform Automation's tasks and image are meant to be used in a Concourse pipeline. So, let's make one. Using your bash command-line client, create a directory to keep your pipeline files in, and cd into it. 1 2 mkdir platform-automation-pipelines cd !$ \" !$ \" !$ is a bash shortcut. Pronounced \"bang, dollar-sign,\" it means \"use the last argument from the most recent command.\" In this case, that's the directory we just created! This is not a Platform Automation thing, this is just a bash tip dearly beloved of at least one Platform Automator. Before we get started with the pipeline itself, we'll gather some variables in a file we can use throughout our pipeline. Open your text editor and create vars.yml . Here's what it should look like to start, we can add things to this as we go: 1 2 3 platform-automation-bucket : your-bucket-name credhub-server : https://your-credhub.example.com opsman-url : https://pcf.foundation.example.com Using a DNS This example assumes that you're using DNS and hostnames. You can use IP addresses for all these resources instead, but you still need to provide the information as a URL, for example: https://120.121.123.124 Now, create a file called pipeline.yml . Naming We'll use pipeline.yml in our examples throughout this guide. However, you may create multiple pipelines over time. If there's a more sensible name for the pipeline you're working on, feel free to use that instead. Write this at the top, and save the file. This is YAML for \"the start of the document. It's optional, but traditional: 1 --- Now you have a pipeline file! Nominally! Well, look. It's valid YAML, at least. Getting fly Let's try to set it as a pipeline with fly , the Concourse command-line Interface (CLI). First, check if we've got fly installed at all: 1 fly -v If it gives you back a version number, great! Skip ahead to Setting The Pipeline If it says something like -bash: fly: command not found , we have a little work to do; we've got to get fly . Navigate to the address for your Concourse instance in a web browser. At this point, you don't even need to be signed in! If there are no public pipelines, you should see something like this: If there are public pipelines, or if you're signed in and there are pipelines you can see, you'll see something similar in the lower-right hand corner. Click the icon for your OS and save the file, mv the resulting file to somewhere in your $PATH , and use chmod to make it executable: A note on command-line examples Some of these, you can copy-paste directly into your terminal. Some of them won't work that way, or even if they did, would require you to edit them to replace our example values with your actual values. We recommend you type all of the bash examples in by hand, substituting values, if necessary, as you go. Don't forget that you can often hit the tab key to auto-complete the name of files that already exist; it makes all that typing just a little easier, and serves as a sort of command-line autocorrect. 1 2 mv ~/Downloads/fly /usr/local/bin/fly chmod +x !$ Congrats! You got fly . Okay but what did I just do? FAIR QUESTION. You downloaded the fly binary, moved it into bash's PATH, which is where bash looks for things to execute when you type a command, and then added permissions that allow it to be e x ecuted. Now, the CLI is installed - and we won't have to do all that again, because fly has the ability to update itself, which we'll get into later. Setting The Pipeline Okay now let's try to set our pipeline with fly , the Concourse CLI. fly keeps a list of Concourses it knows how to talk to. Let's see if the Concourse we want is already on the list: 1 fly targets If you see the address of the Concourse you want to use in the list, note down its name, and use it in the login command: 1 fly -t control-plane login Control-plane? We're going to use the name control-plane for our Concourse in this guide. It's not a special name, it just happens to be the name of the Concourse we want to use in our target list. If you don't see the Concourse you need, you can add it with the -c ( --concourse-url )flag: 1 fly -t control-plane login -c https://your-concourse.example.com You should see a login link you can click on to complete login from your browser. Stay on target The -t flag sets the name when used with login and -c . In the future, you can leave out the -c argument. If you ever want to know what a short flag stands for, you can run the command with -h ( --help ) at the end. Pipeline-setting time! We'll use the name \"foundation\" for this pipeline, but if your foundation has an actual name, use that instead. 1 fly -t control-plane set-pipeline -p foundation -c pipeline.yml It should say no changes to apply , which is fair, since we gave it an empty YAML doc. Version discrepancy If fly says something about a \"version discrepancy,\" \"significant\" or otherwise, just do as it says: run fly sync and try again. fly sync automatically updates the CLI with the version that matches the Concourse you're targeting. Useful! Your First Job Let's see Concourse actually do something, yeah? Add this to your pipeline.yml , starting on the line after the --- : 1 wait : no nevermind let's get version control first Good point. Don't actually add that to your pipeline config yet. Or if you have, delete it, so your whole pipeline looks like this again: 1 --- Reverting edits to our pipeline is something we'll probably want to do again. This is one of many reasons we want to keep our pipeline under version control. So let's make this directory a git repo! But First, git init git should come back with information about the commit you just created: 1 2 git init git commit --allow-empty -m \"Empty initial commit\" If it gives you a config error instead, you might need to configure git a bit. Here's a good guide to initial setup. Get that done, and try again. Now we can add our pipeline.yml , so in the future it's easy to get back to that soothing --- state. 1 2 git add pipeline.yml vars.yml git commit -m \"Add pipeline and starter vars\" Let's just make sure we're all tidy: 1 git status git should come back with nothing to commit, working tree clean . Great. Now we can safely make changes. Git commits git commits are the basic unit of code history. Making frequent, small, commits with good commit messages makes it much easier to figure out why things are the way they are, and to return to the way things were in simpler, better times. Writing short commit messages that capture the intent of the change (in an imperative style) can be tough, but it really does make the pipeline's history much more legible, both to future-you, and to current-and-future teammates and collaborators. The Test Task Platform Automation comes with a test task meant to validate that it's been installed correctly. Let's use it to get setup. Add this to your pipeline.yml , starting on the line after the --- : 1 2 3 4 5 6 jobs : - name : test plan : - task : test image : platform-automation-image file : platform-automation-tasks/tasks/test.yml If we try to set this now, Concourse will take it: 1 fly -t control-plane set-pipeline -p foundation -c pipeline.yml Now we should be able to see our pipeline in the Concourse UI. It'll be paused, so click the \"play\" button to unpause it. Then, click in to the gray box for our test job, and hit the \"plus\" button to schedule a build. It should error immediately, with unknown artifact source: platform-automation-tasks . We didn't give it a source for our task file. We've got a bit of pipeline code that Concourse accepts. Before we start doing the next part, this would be a good moment to make a commit: 1 2 git add pipeline.yml git commit -m \"Add (nonfunctional) test task\" With that done, we can try to get the inputs we need by adding get steps to the plan before the task, like so: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 jobs : - name : test plan : - get : platform-automation-image resource : platform-automation params : globs : [ \"*image*.tgz\" ] unpack : true - get : platform-automation-tasks resource : platform-automation params : globs : [ \"*tasks*.zip\" ] unpack : true - task : test image : platform-automation-image file : platform-automation-tasks/tasks/test.yml If we try to fly set this, fly will complain about invalid resources. To actually make the image and file we want to use available, we'll need some Resources. Adding Resources Resources are Concourse's main approach to managing artifacts. We need an image, and the tasks directory - so we'll tell Concourse how to get these things by declaring Resources for them. In this case, we'll be downloading the image and the tasks directory from Pivnet. Before we can declare the resources themselves, we have to teach Concourse to talk to Pivnet. (Many resource types are built in, but this one isn't.) Add the following to your pipeline file. We'll put it above the jobs entry. 1 2 3 4 5 6 7 8 9 10 11 12 resource_types : - name : pivnet type : docker-image source : repository : pivotalcf/pivnet-resource tag : latest-final resources : - name : platform-automation type : pivnet source : product_slug : platform-automation api_token : ((pivnet-refresh-token)) The API token is a credential, which we'll pass via the command-line when setting the pipeline, so we don't accidentally check it in. Grab a refresh token from your Pivnet profile and clicking \"Request New Refresh Token.\" Then use that token in the following command: Keep it secret, keep it safe Bash commands that start with a space character are not saved in your history. This can be very useful for cases like this, where you want to pass a secret, but don't want it saved. Commands in this guide that contain a secret start with a space, which can be easy to miss. 1 2 3 4 5 # note the space before the command fly -t control-plane set-pipeline \\ -p foundation \\ -c pipeline.yml \\ -v pivnet-refresh-token = your-api-token Warning When you get your Pivnet token as described above, any previous Pivnet tokens you may have gotten will stop working. If you're using your Pivnet refresh token anywhere, retrieve it from your existing secret storage rather than getting a new one, or you'll end up needing to update it everywhere it's used. Go back to the Concourse UI and trigger another build. This time, it should pass. Commit time! 1 2 git add pipeline.yml git commit -m \"Add resources needed for test task\" We'd rather not pass our Pivnet token every time we need to set the pipeline. Fortunately, Concourse can integrate with secret storage services. Let's put our API token in Credhub so Concourse can get it. First we'll need to login: Backslashes in bash examples The following example has been broken across multiple lines by using backslash characters ( \\ ) to escape the newlines. We'll be doing this a lot to keep the examples readable. When you're typing these out, you can skip that and just put it all on one line. 1 2 3 4 # again, note the space at the start credhub login --server example.com \\ --client-id your-client-id \\ --client-secret your-client-secret Logging in to credhub Depending on your credential type, you may need to pass client-id and client-secret , as we do above, or username and password . We use the client approach because that's the credential type that automation should usually be working with. Nominally, a username represents a person, and a client represents a system; this isn't always exactly how things are in practice. Use whichever type of credential you have in your case. Note that if you exclude either set of flags, Credhub will interactively prompt for username and password , and hide the characters of your password when you type them. This method of entry can be better in some situations. Then, we can set the credential name to the path where Concourse will look for it : 1 2 3 4 5 # note the starting space credhub set \\ --name /concourse/your-team-name/pivnet-refresh-token \\ --type value \\ --value your-credhub-refresh-token Now, let's set that pipeline again, without passing a secret this time. 1 2 3 fly -t control-plane set-pipeline \\ -p foundation \\ -c pipeline.yml This should succeed, and the diff Concourse shows you should replace the literal credential with ((pivnet-refresh-token)) . Visit the UI again and re-run the test job; this should also succeed. Exporting The Installation We're finally in a position to do work! While ultimately we want to upgrade Ops Manager, to do that safely we first need to download and persist an export of the current installation. Export your installation routinely We strongly recommend automatically exporting the Ops Manager installation and persisting it to your blobstore on a regular basis. This ensures that if you need to upgrade (or restore!) your Ops Manager for any reason, you'll have the latest installation info available. Later in this tutorial, we'll be adding a time trigger for exactly this reason. Let's switch out the test job for one that exports our existing Ops Manager's installation state. We can switch the task out by changing: the name of the job the name of the task the file of the task export-installation has an additional required input. We need the env file used to talk to Ops Manager. We'll write that file and make it available as a resource in a moment, for now, we'll just get it as if it's there. It also has an additional output (the exported installation). Again, for now, we'll just write that like we have somewhere to put it. Finally, while it's fine for test to run in parallel, export-installation shouldn't. So, we'll add serial: true to the job, too. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 jobs : - name : export-installation serial : true plan : - get : platform-automation-image resource : platform-automation params : globs : [ \"*image*.tgz\" ] unpack : true - get : platform-automation-tasks resource : platform-automation params : globs : [ \"*tasks*.zip\" ] unpack : true - get : env - task : export-installation image : platform-automation-image file : platform-automation-tasks/tasks/export-installation.yml - put : installation params : file : installation/installation-*.zip If we try to fly this up to Concourse, it will again complain about resources that don't exist. So, let's make them. The first new resource we need is the env file. We'll push our git repo to a remote on Github to make this (and later, other) configuration available to the pipelines. Github has good instructions you can follow to create a new repository on Github. You can skip over the part about using git init to setup your repo, since we already did that . Once you've setup your remote and used git push to send what you've got so far, we can add a new directory to hold foundation-specific configuration. (We'll use the name \"foundation\" for this directory, but if your foundation has an actual name, use that instead.) You will also need to add the repository URL to vars.yml so we can reference it later, when we declare the corresponding resource. 1 pipeline-repo : git@github.com:username/platform-automation-pipelines 1 2 mkdir -p foundation cd !$ Now lets write an env.yml for your Ops Manager. env.yml holds authentication and target information for a particular Ops Manager. An example env.yml is shown below. As mentioned in the comment, decryption-passphrase is required for import-installation , and therefore required for upgrade-opsman . If your foundation uses authentication other than basic auth, please reference Inputs and Outputs for more detail on UAA-based authentication. 1 2 3 4 target : ((opsman-url)) username : ((opsman-username)) password : ((opsman-password)) decryption-passphrase : ((opsman-decryption-passphrase)) Add and commit the new env.yml file: 1 2 3 git add foundation/env.yml git commit -m \"Add environment file for foundation\" git push Now that the env file we need is in our git remote, we need to add a resource to tell Concourse how to get it as env . Since this is (probably) a private repo, we'll need to create a deploy key Concourse can use to access it. Follow Github's instructions for creating a read-only deploy key. Then, put the private key in Credhub so we can use it in our pipeline: 1 2 3 4 5 6 # note the starting space credhub set \\ --name /concourse/your-team-name/plat-auto-pipes-deploy-key \\ --type ssh \\ --private the/filepath/of/the/key-id_rsa \\ --public the/filepath/of/the/key-id_rsa.pub Then, add this to the resources section of your pipeline file: 1 2 3 4 5 6 - name : env type : git source : uri : ((pipeline-repo)) private_key : ((plat-auto-pipes-deploy-key)) branch : master We'll put the credentials we need in Credhub: 1 2 3 4 5 6 7 8 9 10 # note the starting space throughout credhub set \\ -n /concourse/your-team-name/foundation/opsman-username \\ -t value -v your-opsman-username credhub set \\ -n /concourse/your-team-name/foundation/opsman-password \\ -t value -v your-opsman-password credhub set \\ -n /concourse/your-team-name/foundation/opsman-decryption-passphrase \\ -t value -v your-opsman-decryption-passphrase Credhub paths and pipeline names Notice that we've added an element to the cred paths; now we're using the foundation name. If you look at Concourse's lookup rules, you'll see that it searches the pipeline-specific path before the team path. Since our pipeline is named for the foundation it's used to manage, we can use this to scope access to our foundation-specific information to just this pipeline. By contrast, the Pivnet token may be valuable across several pipelines (and associated foundations), so we scoped that to our team. In order to perform interpolation in one of our input files, we'll need the credhub-interpolate task Earlier, we relied on Concourse's native integration with Credhub for interpolation. That worked because we needed to use the variable in the pipeline itself, not in one of our inputs. We can add it to our job after we've retrieved our env input, but before the export-installation task: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 jobs : - name : export-installation serial : true plan : - get : platform-automation-image resource : platform-automation params : globs : [ \"*image*.tgz\" ] unpack : true - get : platform-automation-tasks resource : platform-automation params : globs : [ \"*tasks*.zip\" ] unpack : true - get : env - task : credhub-interpolate image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : https://your-credhub.example.com PREFIX : /concourse/your-team-name/foundation INTERPOLATION_PATHS : foundation # contains env.yml input_mapping : files : env output_mapping : interpolated-files : interpolated-env - task : export-installation image : platform-automation-image file : platform-automation-tasks/tasks/export-installation.yml params : ENV_FILE : your/env/path/env.yml input_mapping : env : interpolated-env - put : installation params : file : installation/installation-*.zip output_mapping The credhub-interpolate task for this job maps the output from the task ( interpolated-files ) to interpolated-env . This can be used by the next task in the job to more explicitly define the inputs/outputs of each task. It is also okay to leave the output as interpolated-files if it is appropriately referenced in the next task Notice the input mappings of the credhub-interpolate and export-installation tasks. This allows us to use the output of one task as in input of another. We now need to put our credhub_client and credhub_secret into Credhub, so Concourse's native integration can retrieve them and pass them as configuration to the credhub-interpolate task. 1 2 3 4 5 6 7 # note the starting space throughout credhub set \\ -n /concourse/your-team-name/credhub-client \\ -t value -v your-credhub-client credhub set \\ -n /concourse/your-team-name/credhub-secret \\ -t value -v your-credhub-secret Now, the credhub-interpolate task will interpolate our config input, and pass it to export-installation as config . The other new resource we need is a blobstore, so we can persist the exported installation. We'll add an S3 resource to the resources section: 1 2 3 4 5 6 7 - name : installation type : s3 source : access_key_id : ((s3-access-key-id)) secret_access_key : ((s3-secret-key)) bucket : ((platform-automation-bucket)) regexp : foundation/installation-(.*).zip Again, we'll need to save the credentials in Credhub: 1 2 3 4 5 6 7 # note the starting space throughout credhub set \\ -n /concourse/your-team-name/s3-access-key-id \\ -t value -v your-bucket-s3-access-key-id credhub set \\ -n /concourse/your-team-name/s3-secret-key \\ -t value -v your-s3-secret-key This time (and in the future), when we set the pipeline with fly , we'll need to load vars from vars.yml . 1 2 3 4 5 # note the space before the command fly -t control-plane set-pipeline \\ -p foundation \\ -c pipeline.yml \\ -l vars.yml Now you can manually trigger a build, and see it pass. Bash command history You'll be using this, the ultimate form of the fly command to set your pipeline, for the rest of the tutorial. You can save yourself some typing by using your bash history. You can cycle through previous commands with the up and down arrows. Alternatively, Ctrl-r will search your bash history. Just hit Ctrl-r, type fly , and it'll show you the last fly command you ran. Run it with enter. Instead of running it, you can hit Ctrl-r again to see the matching command before that. This is also a good commit point: 1 2 3 git add pipeline.yml vars.yml git commit -m \"Export foundation installation in CI\" git push Performing The Upgrade Now that we have an exported installation, we'll create another Concourse job to do the upgrade itself. We want the export and the upgrade in separate jobs so they can be triggered (and re-run) independently. We know this new job is going to center on the upgrade-opsman task. Click through to the task description, and write a new job that has get steps for our platform-automation resources and all the inputs we already know how to get: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 - name : upgrade-opsman serial : true plan : - get : platform-automation-image resource : platform-automation params : globs : [ \"*image*.tgz\" ] unpack : true - get : platform-automation-tasks resource : platform-automation params : globs : [ \"*tasks*.zip\" ] unpack : true - get : env - get : installation We should be able to set this with fly and see it pass, but it doesn't do anything other than download the resources. Still, we can make a commit here: 1 2 3 git add pipeline.yml git commit -m \"Setup initial gets for upgrade job\" git push Is this really a commit point though? We like frequent, small commits that can be fly set and, ideally, go green. This one doesn't actually do anything though, right? Fair, but: setting and running the job gives you feedback on your syntax and variable usage. It can catch typos, resources you forgot to add or misnamed, etc. Committing when you get to a working point helps keeps the diffs small, and the history tractable. Also, down the line, if you've got more than one pair working on a foundation, the small commits help you keep off one another's toes. We don't demonstrate this workflow here, but it can even be useful to make a commit, use fly to see if it works, and then push it if and only if it works. If it doesn't, you can use git commit --amend once you've figured out why and fixed it. This workflow makes it easy to keep what is set on Concourse and what is pushed to your source control remote in sync. Looking over the list of inputs for upgrade-opsman we still need three required inputs: state config image The optional inputs are vars used with the config, so we'll get to those when we do config . Let's start with the state file . We need to record the iaas we're on and the ID of the currently deployed Ops Manager VM. Different IaaS uniquely identify VMs differently; here are examples for what this file should look like, depending on your IaaS: AWS 1 2 3 iaas : aws # Instance ID of the AWS VM vm_id : i-12345678987654321 Azure 1 2 3 iaas : azure # Computer Name of the Azure VM vm_id : vm_name GCP 1 2 3 iaas : gcp # Name of the VM in GCP vm_id : vm_name OpenStack 1 2 3 iaas : openstack # Instance ID from the OpenStack Overview vm_id : 12345678-9876-5432-1abc-defghijklmno vSphere 1 2 3 iaas : vsphere # Path to the VM in vCenter vm_id : /datacenter/vm/folder/vm_name Find what you need for your IaaS, write it in your repo as foundation/state.yml , commit it, and push it: 1 2 3 git add foundation/state.yml git commit -m \"Add state file for foundation Ops Manager\" git push We can map the env resource to upgrade-opsman 's state input once we add the task. But first, we've got two more inputs to arrange for. Let's do config next. We'll write an Ops Manager VM Configuration file to foundation/opsman.yml . The properties available vary by IaaS; regardless, you can often inspect your existing Ops Manager in your IaaS's console (or, if your Ops Manager was created with Terraform, look at your terraform outputs) to find the necessary values. AWS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- opsman-configuration : aws : region : us-west-2 vm_name : ops-manager-foundation vpc_subnet_id : subnet-0292bc845215c2cbf security_group_id : sg-0354f804ba7c4bc41 key_pair_name : ops-manager-key private_ip : 10.0.0.2 use_instance_profile : true iam_instance_profile_name : ops-manager-iam # Note that because this config contains no secrets # and is already written to a foundation-specific filepath, # it need not be interpolated. Azure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 --- opsman-configuration : azure : subscription_id : 90f35f10-ea9e-4e80-aac4-d6778b995532 resource_group : res-group tenant_id : 3e52862f-a01e-4b97-98d5-f31a409df682 client_id : 5782deb6-9195-4827-83ae-a13fda90aa0d client_secret : ((opsman-client-secret)) location : westus container : opsmanagerimage network_security_group : ops-manager-security-group vpc_subnet : /subscriptions/<MY_SUBSCRIPTION_ID>/resourceGroups/<MY_RESOURCE_GROUP>/providers/Microsoft.Network/virtualNetworks/<MY_VNET>/subnets/<MY_SUBNET> storage_account : opsman # used for the Ops Manager image ssh_public_key : ssh-rsa AAAAB3NzaC1yc2EAZ... # public key for ssh to VM vm_name : ops-manager-foundation private_ip : 10.0.0.3 # Note that as this contains a secret, it will need to be interpolated. # The opsman-client-secret will need to be set in credhub. GCP 1 2 3 4 5 6 7 8 9 10 11 12 13 14 --- opsman-configuration : gcp : gcp_service_account : ((gcp_service_account_json)) project : project-id region : us-central1 zone : us-central1-b vm_name : ops-manager-foundation vpc_subnet : infrastructure-subnet tags : ops-manager private_ip : 10.0.0.2 # Note that as this contains a secret, it will need to be interpolated. # The gcp_service_account_json will need to be set in credhub. OpenStack 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 --- opsman-configuration : openstack : auth_url : http://os.example.com:5000/v2.0 project_name : project net_id : 26a13112-b6c2-11e8-96f8-529269fb1459 username : admin password : ((opsman-openstack-password)) key_pair_name : opsman-keypair security_group_name : opsman-sec-group vm_name : ops-manager-foundation private_ip : 10.0.0.3 project_domain_name : default user_domain_name : default availability_zone : zone-01 # Note that as this contains a secret, it will need to be interpolated. # The opsman-openstack-password will need to be set in credhub. vSphere 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 --- opsman-configuration : vsphere : vcenter : url : vcenter.example.com username : admin password : ((vcenter-password)) datastore : exmple-ds-1 ca_cert : certificate host : example-host # vCenter host to deploy Ops Manager on datacenter : example-dc resource_pool : /example-dc/host/example-host/Resources/ResPool folder : /example-dc/vm/Folder disk_type : thin private_ip : 10.0.0.2 dns : 8.8.8.8 ntp : ntp.example.com ssh_public_key : ssh-rsa ...... hostname : pcf.example.com network : virtual-network # vcenter network to deploy to netmask : 255.255.255.192 gateway : 192.168.10.1 vm_name : ops-manager-foundation # Note that as this contains a secret, it will need to be interpolated. # The vcenter-password will need to be set in credhub. These examples all make assumptions about the details of your existing Ops Manager's configuration. See the reference docs for this file for more details about your options and per-IaaS caveats. Once you have your config file, commit and push it: 1 2 3 git add foundation/opsman.yml git commit -m \"Add opsman config\" git push Finally, we need the image for the new Ops Manager version. We'll use the download-product task. It requires a config file to specify which Ops Manager to get, and to provide Pivotal Network credentials. Name this file foundation/download-opsman.yml : 1 2 3 4 5 --- pivnet-api-token : ((pivnet-refresh-token)) # interpolated from Credhub pivnet-file-glob : \"ops-manager*.ova\" pivnet-product-slug : ops-manager product-version-regex : ^2\\.5\\.0.*$ You know the drill. 1 2 3 git add foundation/download-opsman.yml git commit -m \"Add download opsman config\" git push Now, we can put it all together: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 - name : upgrade-opsman serial : true plan : - get : platform-automation-image resource : platform-automation params : globs : [ \"*image*.tgz\" ] unpack : true - get : platform-automation-tasks resource : platform-automation params : globs : [ \"*tasks*.zip\" ] unpack : true - get : env - get : installation - task : credhub-interpolate image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : /concourse/your-team-name/foundation # A file path that includes env.yml, opsman.yml, download-opsman.yml INTERPOLATION_PATHS : foundation input_mapping : files : env output_mapping : interpolated-files : interpolated-configs - task : download-opsman-image image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : download-opsman.yml input_mapping : config : interpolated-configs - task : upgrade-opsman image : platform-automation-image file : platform-automation-tasks/tasks/upgrade-opsman.yml input_mapping : config : interpolated-configs image : downloaded-product secrets : interpolated-configs state : env params : ENV_FILE : foundation/env.yml OPSMAN_CONFIG_FILE : foundation/opsman.yml STATE_FILE : foundation/state.yml Set the pipeline. Before we run the job, we should ensure that state.yml is always persisted regardless of whether the upgrade-opsman job failed or passed. To do this, we can add the following section to the job: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 - name : upgrade-opsman serial : true plan : - get : platform-automation-image resource : platform-automation params : globs : [ \"*image*.tgz\" ] unpack : true - get : platform-automation-tasks resource : platform-automation params : globs : [ \"*tasks*.zip\" ] unpack : true - get : env - get : installation - task : credhub-interpolate image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : /concourse/your-team-name/foundation # A file path that includes env.yml, opsman.yml, download-opsman.yml INTERPOLATION_PATHS : foundation input_mapping : files : env output_mapping : interpolated-files : interpolated-configs - task : download-opsman-image image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : download-opsman.yml input_mapping : config : interpolated-configs - task : upgrade-opsman image : platform-automation-image file : platform-automation-tasks/tasks/upgrade-opsman.yml input_mapping : config : interpolated-configs image : downloaded-product secrets : interpolated-configs state : env params : ENV_FILE : foundation/env.yml OPSMAN_CONFIG_FILE : foundation/opsman.yml STATE_FILE : foundation/state.yml ensure : do : - task : make-commit image : platform-automation-image file : platform-automation-tasks/tasks/make-git-commit.yml input_mapping : repository : env file-source : generated-state output_mapping : repository-commit : env-commit params : FILE_SOURCE_PATH : foundation/state.yml FILE_DESTINATION_PATH : foundation/state.yml GIT_AUTHOR_EMAIL : \"ci-user@example.com\" GIT_AUTHOR_NAME : \"CI User\" COMMIT_MESSAGE : 'Update state file' - put : env params : repository : env-commit merge : true Set the pipeline one final time, run the job, and see it pass. 1 2 3 git add pipeline.yml git commit -m \"Upgrade Ops Manager in CI\" git push Your upgrade pipeline is now complete. You are now free to move on to the next steps of your automation journey.","title":"Upgrading an Existing Ops Manager"},{"location":"how-to-guides/upgrade-existing-opsman.html#writing-a-pipeline-to-upgrade-an-existing-ops-manager","text":"","title":"Writing a Pipeline to Upgrade an Existing Ops Manager"},{"location":"how-to-guides/upgrade-existing-opsman.html#prerequisites","text":"Over the course of this guide, we're going to use Platform Automation for PCF to create a pipeline using Concourse . Before we get started, you'll need a few things ready to go: A running Ops Manager VM that you would like to upgrade Credentials for an IaaS that Ops Manager is compatible with A Concourse instance with access to a Credhub instance and to the Internet Github.com account Read/write credentials and bucket name for an S3 bucket An account on https://network.pivotal.io (Pivnet) A MacOS workstation with Docker installed and a text editor you like a terminal emulator you like a browser that works with Concourse, like Firefox or Chrome and git IaaS It doesn't actually matter what IaaS you use for Ops Manager, as long as your Concourse can connect to it. Pipelines built with Platform Automation can be platform-agnostic. It will be very helpful to have a basic familiarity with the following. If you don't have basic familiarity with all these things, that's okay. We'll explain some basics, and link to resources to learn more: the bash terminal git YAML Concourse A note on the prerequisites While this guide uses Github to provide a git remote, and an S3 bucket as a blobstore, Platform Automation supports arbitrary git providers and S3-compatible blobstores. If you need to use an alternate one, that's okay. We picked specific examples so we could describe some steps in detail. Some details may be different if you follow along with different providers. If you're comfortable navigating those differences on your own, go for it! Similarly, in this guide, we assume the MacOS operating system. This should all work fine on Linux, too, but there might be differences in the paths you'll need to figure out.","title":"Prerequisites"},{"location":"how-to-guides/upgrade-existing-opsman.html#creating-a-concourse-pipeline","text":"Platform Automation's tasks and image are meant to be used in a Concourse pipeline. So, let's make one. Using your bash command-line client, create a directory to keep your pipeline files in, and cd into it. 1 2 mkdir platform-automation-pipelines cd !$ \" !$ \" !$ is a bash shortcut. Pronounced \"bang, dollar-sign,\" it means \"use the last argument from the most recent command.\" In this case, that's the directory we just created! This is not a Platform Automation thing, this is just a bash tip dearly beloved of at least one Platform Automator. Before we get started with the pipeline itself, we'll gather some variables in a file we can use throughout our pipeline. Open your text editor and create vars.yml . Here's what it should look like to start, we can add things to this as we go: 1 2 3 platform-automation-bucket : your-bucket-name credhub-server : https://your-credhub.example.com opsman-url : https://pcf.foundation.example.com Using a DNS This example assumes that you're using DNS and hostnames. You can use IP addresses for all these resources instead, but you still need to provide the information as a URL, for example: https://120.121.123.124 Now, create a file called pipeline.yml . Naming We'll use pipeline.yml in our examples throughout this guide. However, you may create multiple pipelines over time. If there's a more sensible name for the pipeline you're working on, feel free to use that instead. Write this at the top, and save the file. This is YAML for \"the start of the document. It's optional, but traditional: 1 --- Now you have a pipeline file! Nominally! Well, look. It's valid YAML, at least.","title":"Creating a Concourse Pipeline"},{"location":"how-to-guides/upgrade-existing-opsman.html#getting-fly","text":"Let's try to set it as a pipeline with fly , the Concourse command-line Interface (CLI). First, check if we've got fly installed at all: 1 fly -v If it gives you back a version number, great! Skip ahead to Setting The Pipeline If it says something like -bash: fly: command not found , we have a little work to do; we've got to get fly . Navigate to the address for your Concourse instance in a web browser. At this point, you don't even need to be signed in! If there are no public pipelines, you should see something like this: If there are public pipelines, or if you're signed in and there are pipelines you can see, you'll see something similar in the lower-right hand corner. Click the icon for your OS and save the file, mv the resulting file to somewhere in your $PATH , and use chmod to make it executable: A note on command-line examples Some of these, you can copy-paste directly into your terminal. Some of them won't work that way, or even if they did, would require you to edit them to replace our example values with your actual values. We recommend you type all of the bash examples in by hand, substituting values, if necessary, as you go. Don't forget that you can often hit the tab key to auto-complete the name of files that already exist; it makes all that typing just a little easier, and serves as a sort of command-line autocorrect. 1 2 mv ~/Downloads/fly /usr/local/bin/fly chmod +x !$ Congrats! You got fly . Okay but what did I just do? FAIR QUESTION. You downloaded the fly binary, moved it into bash's PATH, which is where bash looks for things to execute when you type a command, and then added permissions that allow it to be e x ecuted. Now, the CLI is installed - and we won't have to do all that again, because fly has the ability to update itself, which we'll get into later.","title":"Getting fly"},{"location":"how-to-guides/upgrade-existing-opsman.html#setting-the-pipeline","text":"Okay now let's try to set our pipeline with fly , the Concourse CLI. fly keeps a list of Concourses it knows how to talk to. Let's see if the Concourse we want is already on the list: 1 fly targets If you see the address of the Concourse you want to use in the list, note down its name, and use it in the login command: 1 fly -t control-plane login Control-plane? We're going to use the name control-plane for our Concourse in this guide. It's not a special name, it just happens to be the name of the Concourse we want to use in our target list. If you don't see the Concourse you need, you can add it with the -c ( --concourse-url )flag: 1 fly -t control-plane login -c https://your-concourse.example.com You should see a login link you can click on to complete login from your browser. Stay on target The -t flag sets the name when used with login and -c . In the future, you can leave out the -c argument. If you ever want to know what a short flag stands for, you can run the command with -h ( --help ) at the end. Pipeline-setting time! We'll use the name \"foundation\" for this pipeline, but if your foundation has an actual name, use that instead. 1 fly -t control-plane set-pipeline -p foundation -c pipeline.yml It should say no changes to apply , which is fair, since we gave it an empty YAML doc. Version discrepancy If fly says something about a \"version discrepancy,\" \"significant\" or otherwise, just do as it says: run fly sync and try again. fly sync automatically updates the CLI with the version that matches the Concourse you're targeting. Useful!","title":"Setting The Pipeline"},{"location":"how-to-guides/upgrade-existing-opsman.html#your-first-job","text":"Let's see Concourse actually do something, yeah? Add this to your pipeline.yml , starting on the line after the --- : 1 wait : no nevermind let's get version control first Good point. Don't actually add that to your pipeline config yet. Or if you have, delete it, so your whole pipeline looks like this again: 1 --- Reverting edits to our pipeline is something we'll probably want to do again. This is one of many reasons we want to keep our pipeline under version control. So let's make this directory a git repo!","title":"Your First Job"},{"location":"how-to-guides/upgrade-existing-opsman.html#but-first-git-init","text":"git should come back with information about the commit you just created: 1 2 git init git commit --allow-empty -m \"Empty initial commit\" If it gives you a config error instead, you might need to configure git a bit. Here's a good guide to initial setup. Get that done, and try again. Now we can add our pipeline.yml , so in the future it's easy to get back to that soothing --- state. 1 2 git add pipeline.yml vars.yml git commit -m \"Add pipeline and starter vars\" Let's just make sure we're all tidy: 1 git status git should come back with nothing to commit, working tree clean . Great. Now we can safely make changes. Git commits git commits are the basic unit of code history. Making frequent, small, commits with good commit messages makes it much easier to figure out why things are the way they are, and to return to the way things were in simpler, better times. Writing short commit messages that capture the intent of the change (in an imperative style) can be tough, but it really does make the pipeline's history much more legible, both to future-you, and to current-and-future teammates and collaborators.","title":"But First, git init"},{"location":"how-to-guides/upgrade-existing-opsman.html#the-test-task","text":"Platform Automation comes with a test task meant to validate that it's been installed correctly. Let's use it to get setup. Add this to your pipeline.yml , starting on the line after the --- : 1 2 3 4 5 6 jobs : - name : test plan : - task : test image : platform-automation-image file : platform-automation-tasks/tasks/test.yml If we try to set this now, Concourse will take it: 1 fly -t control-plane set-pipeline -p foundation -c pipeline.yml Now we should be able to see our pipeline in the Concourse UI. It'll be paused, so click the \"play\" button to unpause it. Then, click in to the gray box for our test job, and hit the \"plus\" button to schedule a build. It should error immediately, with unknown artifact source: platform-automation-tasks . We didn't give it a source for our task file. We've got a bit of pipeline code that Concourse accepts. Before we start doing the next part, this would be a good moment to make a commit: 1 2 git add pipeline.yml git commit -m \"Add (nonfunctional) test task\" With that done, we can try to get the inputs we need by adding get steps to the plan before the task, like so: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 jobs : - name : test plan : - get : platform-automation-image resource : platform-automation params : globs : [ \"*image*.tgz\" ] unpack : true - get : platform-automation-tasks resource : platform-automation params : globs : [ \"*tasks*.zip\" ] unpack : true - task : test image : platform-automation-image file : platform-automation-tasks/tasks/test.yml If we try to fly set this, fly will complain about invalid resources. To actually make the image and file we want to use available, we'll need some Resources.","title":"The Test Task"},{"location":"how-to-guides/upgrade-existing-opsman.html#adding-resources","text":"Resources are Concourse's main approach to managing artifacts. We need an image, and the tasks directory - so we'll tell Concourse how to get these things by declaring Resources for them. In this case, we'll be downloading the image and the tasks directory from Pivnet. Before we can declare the resources themselves, we have to teach Concourse to talk to Pivnet. (Many resource types are built in, but this one isn't.) Add the following to your pipeline file. We'll put it above the jobs entry. 1 2 3 4 5 6 7 8 9 10 11 12 resource_types : - name : pivnet type : docker-image source : repository : pivotalcf/pivnet-resource tag : latest-final resources : - name : platform-automation type : pivnet source : product_slug : platform-automation api_token : ((pivnet-refresh-token)) The API token is a credential, which we'll pass via the command-line when setting the pipeline, so we don't accidentally check it in. Grab a refresh token from your Pivnet profile and clicking \"Request New Refresh Token.\" Then use that token in the following command: Keep it secret, keep it safe Bash commands that start with a space character are not saved in your history. This can be very useful for cases like this, where you want to pass a secret, but don't want it saved. Commands in this guide that contain a secret start with a space, which can be easy to miss. 1 2 3 4 5 # note the space before the command fly -t control-plane set-pipeline \\ -p foundation \\ -c pipeline.yml \\ -v pivnet-refresh-token = your-api-token Warning When you get your Pivnet token as described above, any previous Pivnet tokens you may have gotten will stop working. If you're using your Pivnet refresh token anywhere, retrieve it from your existing secret storage rather than getting a new one, or you'll end up needing to update it everywhere it's used. Go back to the Concourse UI and trigger another build. This time, it should pass. Commit time! 1 2 git add pipeline.yml git commit -m \"Add resources needed for test task\" We'd rather not pass our Pivnet token every time we need to set the pipeline. Fortunately, Concourse can integrate with secret storage services. Let's put our API token in Credhub so Concourse can get it. First we'll need to login: Backslashes in bash examples The following example has been broken across multiple lines by using backslash characters ( \\ ) to escape the newlines. We'll be doing this a lot to keep the examples readable. When you're typing these out, you can skip that and just put it all on one line. 1 2 3 4 # again, note the space at the start credhub login --server example.com \\ --client-id your-client-id \\ --client-secret your-client-secret Logging in to credhub Depending on your credential type, you may need to pass client-id and client-secret , as we do above, or username and password . We use the client approach because that's the credential type that automation should usually be working with. Nominally, a username represents a person, and a client represents a system; this isn't always exactly how things are in practice. Use whichever type of credential you have in your case. Note that if you exclude either set of flags, Credhub will interactively prompt for username and password , and hide the characters of your password when you type them. This method of entry can be better in some situations. Then, we can set the credential name to the path where Concourse will look for it : 1 2 3 4 5 # note the starting space credhub set \\ --name /concourse/your-team-name/pivnet-refresh-token \\ --type value \\ --value your-credhub-refresh-token Now, let's set that pipeline again, without passing a secret this time. 1 2 3 fly -t control-plane set-pipeline \\ -p foundation \\ -c pipeline.yml This should succeed, and the diff Concourse shows you should replace the literal credential with ((pivnet-refresh-token)) . Visit the UI again and re-run the test job; this should also succeed.","title":"Adding Resources"},{"location":"how-to-guides/upgrade-existing-opsman.html#exporting-the-installation","text":"We're finally in a position to do work! While ultimately we want to upgrade Ops Manager, to do that safely we first need to download and persist an export of the current installation. Export your installation routinely We strongly recommend automatically exporting the Ops Manager installation and persisting it to your blobstore on a regular basis. This ensures that if you need to upgrade (or restore!) your Ops Manager for any reason, you'll have the latest installation info available. Later in this tutorial, we'll be adding a time trigger for exactly this reason. Let's switch out the test job for one that exports our existing Ops Manager's installation state. We can switch the task out by changing: the name of the job the name of the task the file of the task export-installation has an additional required input. We need the env file used to talk to Ops Manager. We'll write that file and make it available as a resource in a moment, for now, we'll just get it as if it's there. It also has an additional output (the exported installation). Again, for now, we'll just write that like we have somewhere to put it. Finally, while it's fine for test to run in parallel, export-installation shouldn't. So, we'll add serial: true to the job, too. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 jobs : - name : export-installation serial : true plan : - get : platform-automation-image resource : platform-automation params : globs : [ \"*image*.tgz\" ] unpack : true - get : platform-automation-tasks resource : platform-automation params : globs : [ \"*tasks*.zip\" ] unpack : true - get : env - task : export-installation image : platform-automation-image file : platform-automation-tasks/tasks/export-installation.yml - put : installation params : file : installation/installation-*.zip If we try to fly this up to Concourse, it will again complain about resources that don't exist. So, let's make them. The first new resource we need is the env file. We'll push our git repo to a remote on Github to make this (and later, other) configuration available to the pipelines. Github has good instructions you can follow to create a new repository on Github. You can skip over the part about using git init to setup your repo, since we already did that . Once you've setup your remote and used git push to send what you've got so far, we can add a new directory to hold foundation-specific configuration. (We'll use the name \"foundation\" for this directory, but if your foundation has an actual name, use that instead.) You will also need to add the repository URL to vars.yml so we can reference it later, when we declare the corresponding resource. 1 pipeline-repo : git@github.com:username/platform-automation-pipelines 1 2 mkdir -p foundation cd !$ Now lets write an env.yml for your Ops Manager. env.yml holds authentication and target information for a particular Ops Manager. An example env.yml is shown below. As mentioned in the comment, decryption-passphrase is required for import-installation , and therefore required for upgrade-opsman . If your foundation uses authentication other than basic auth, please reference Inputs and Outputs for more detail on UAA-based authentication. 1 2 3 4 target : ((opsman-url)) username : ((opsman-username)) password : ((opsman-password)) decryption-passphrase : ((opsman-decryption-passphrase)) Add and commit the new env.yml file: 1 2 3 git add foundation/env.yml git commit -m \"Add environment file for foundation\" git push Now that the env file we need is in our git remote, we need to add a resource to tell Concourse how to get it as env . Since this is (probably) a private repo, we'll need to create a deploy key Concourse can use to access it. Follow Github's instructions for creating a read-only deploy key. Then, put the private key in Credhub so we can use it in our pipeline: 1 2 3 4 5 6 # note the starting space credhub set \\ --name /concourse/your-team-name/plat-auto-pipes-deploy-key \\ --type ssh \\ --private the/filepath/of/the/key-id_rsa \\ --public the/filepath/of/the/key-id_rsa.pub Then, add this to the resources section of your pipeline file: 1 2 3 4 5 6 - name : env type : git source : uri : ((pipeline-repo)) private_key : ((plat-auto-pipes-deploy-key)) branch : master We'll put the credentials we need in Credhub: 1 2 3 4 5 6 7 8 9 10 # note the starting space throughout credhub set \\ -n /concourse/your-team-name/foundation/opsman-username \\ -t value -v your-opsman-username credhub set \\ -n /concourse/your-team-name/foundation/opsman-password \\ -t value -v your-opsman-password credhub set \\ -n /concourse/your-team-name/foundation/opsman-decryption-passphrase \\ -t value -v your-opsman-decryption-passphrase Credhub paths and pipeline names Notice that we've added an element to the cred paths; now we're using the foundation name. If you look at Concourse's lookup rules, you'll see that it searches the pipeline-specific path before the team path. Since our pipeline is named for the foundation it's used to manage, we can use this to scope access to our foundation-specific information to just this pipeline. By contrast, the Pivnet token may be valuable across several pipelines (and associated foundations), so we scoped that to our team. In order to perform interpolation in one of our input files, we'll need the credhub-interpolate task Earlier, we relied on Concourse's native integration with Credhub for interpolation. That worked because we needed to use the variable in the pipeline itself, not in one of our inputs. We can add it to our job after we've retrieved our env input, but before the export-installation task: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 jobs : - name : export-installation serial : true plan : - get : platform-automation-image resource : platform-automation params : globs : [ \"*image*.tgz\" ] unpack : true - get : platform-automation-tasks resource : platform-automation params : globs : [ \"*tasks*.zip\" ] unpack : true - get : env - task : credhub-interpolate image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : https://your-credhub.example.com PREFIX : /concourse/your-team-name/foundation INTERPOLATION_PATHS : foundation # contains env.yml input_mapping : files : env output_mapping : interpolated-files : interpolated-env - task : export-installation image : platform-automation-image file : platform-automation-tasks/tasks/export-installation.yml params : ENV_FILE : your/env/path/env.yml input_mapping : env : interpolated-env - put : installation params : file : installation/installation-*.zip output_mapping The credhub-interpolate task for this job maps the output from the task ( interpolated-files ) to interpolated-env . This can be used by the next task in the job to more explicitly define the inputs/outputs of each task. It is also okay to leave the output as interpolated-files if it is appropriately referenced in the next task Notice the input mappings of the credhub-interpolate and export-installation tasks. This allows us to use the output of one task as in input of another. We now need to put our credhub_client and credhub_secret into Credhub, so Concourse's native integration can retrieve them and pass them as configuration to the credhub-interpolate task. 1 2 3 4 5 6 7 # note the starting space throughout credhub set \\ -n /concourse/your-team-name/credhub-client \\ -t value -v your-credhub-client credhub set \\ -n /concourse/your-team-name/credhub-secret \\ -t value -v your-credhub-secret Now, the credhub-interpolate task will interpolate our config input, and pass it to export-installation as config . The other new resource we need is a blobstore, so we can persist the exported installation. We'll add an S3 resource to the resources section: 1 2 3 4 5 6 7 - name : installation type : s3 source : access_key_id : ((s3-access-key-id)) secret_access_key : ((s3-secret-key)) bucket : ((platform-automation-bucket)) regexp : foundation/installation-(.*).zip Again, we'll need to save the credentials in Credhub: 1 2 3 4 5 6 7 # note the starting space throughout credhub set \\ -n /concourse/your-team-name/s3-access-key-id \\ -t value -v your-bucket-s3-access-key-id credhub set \\ -n /concourse/your-team-name/s3-secret-key \\ -t value -v your-s3-secret-key This time (and in the future), when we set the pipeline with fly , we'll need to load vars from vars.yml . 1 2 3 4 5 # note the space before the command fly -t control-plane set-pipeline \\ -p foundation \\ -c pipeline.yml \\ -l vars.yml Now you can manually trigger a build, and see it pass. Bash command history You'll be using this, the ultimate form of the fly command to set your pipeline, for the rest of the tutorial. You can save yourself some typing by using your bash history. You can cycle through previous commands with the up and down arrows. Alternatively, Ctrl-r will search your bash history. Just hit Ctrl-r, type fly , and it'll show you the last fly command you ran. Run it with enter. Instead of running it, you can hit Ctrl-r again to see the matching command before that. This is also a good commit point: 1 2 3 git add pipeline.yml vars.yml git commit -m \"Export foundation installation in CI\" git push","title":"Exporting The Installation"},{"location":"how-to-guides/upgrade-existing-opsman.html#performing-the-upgrade","text":"Now that we have an exported installation, we'll create another Concourse job to do the upgrade itself. We want the export and the upgrade in separate jobs so they can be triggered (and re-run) independently. We know this new job is going to center on the upgrade-opsman task. Click through to the task description, and write a new job that has get steps for our platform-automation resources and all the inputs we already know how to get: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 - name : upgrade-opsman serial : true plan : - get : platform-automation-image resource : platform-automation params : globs : [ \"*image*.tgz\" ] unpack : true - get : platform-automation-tasks resource : platform-automation params : globs : [ \"*tasks*.zip\" ] unpack : true - get : env - get : installation We should be able to set this with fly and see it pass, but it doesn't do anything other than download the resources. Still, we can make a commit here: 1 2 3 git add pipeline.yml git commit -m \"Setup initial gets for upgrade job\" git push Is this really a commit point though? We like frequent, small commits that can be fly set and, ideally, go green. This one doesn't actually do anything though, right? Fair, but: setting and running the job gives you feedback on your syntax and variable usage. It can catch typos, resources you forgot to add or misnamed, etc. Committing when you get to a working point helps keeps the diffs small, and the history tractable. Also, down the line, if you've got more than one pair working on a foundation, the small commits help you keep off one another's toes. We don't demonstrate this workflow here, but it can even be useful to make a commit, use fly to see if it works, and then push it if and only if it works. If it doesn't, you can use git commit --amend once you've figured out why and fixed it. This workflow makes it easy to keep what is set on Concourse and what is pushed to your source control remote in sync. Looking over the list of inputs for upgrade-opsman we still need three required inputs: state config image The optional inputs are vars used with the config, so we'll get to those when we do config . Let's start with the state file . We need to record the iaas we're on and the ID of the currently deployed Ops Manager VM. Different IaaS uniquely identify VMs differently; here are examples for what this file should look like, depending on your IaaS: AWS 1 2 3 iaas : aws # Instance ID of the AWS VM vm_id : i-12345678987654321 Azure 1 2 3 iaas : azure # Computer Name of the Azure VM vm_id : vm_name GCP 1 2 3 iaas : gcp # Name of the VM in GCP vm_id : vm_name OpenStack 1 2 3 iaas : openstack # Instance ID from the OpenStack Overview vm_id : 12345678-9876-5432-1abc-defghijklmno vSphere 1 2 3 iaas : vsphere # Path to the VM in vCenter vm_id : /datacenter/vm/folder/vm_name Find what you need for your IaaS, write it in your repo as foundation/state.yml , commit it, and push it: 1 2 3 git add foundation/state.yml git commit -m \"Add state file for foundation Ops Manager\" git push We can map the env resource to upgrade-opsman 's state input once we add the task. But first, we've got two more inputs to arrange for. Let's do config next. We'll write an Ops Manager VM Configuration file to foundation/opsman.yml . The properties available vary by IaaS; regardless, you can often inspect your existing Ops Manager in your IaaS's console (or, if your Ops Manager was created with Terraform, look at your terraform outputs) to find the necessary values. AWS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- opsman-configuration : aws : region : us-west-2 vm_name : ops-manager-foundation vpc_subnet_id : subnet-0292bc845215c2cbf security_group_id : sg-0354f804ba7c4bc41 key_pair_name : ops-manager-key private_ip : 10.0.0.2 use_instance_profile : true iam_instance_profile_name : ops-manager-iam # Note that because this config contains no secrets # and is already written to a foundation-specific filepath, # it need not be interpolated. Azure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 --- opsman-configuration : azure : subscription_id : 90f35f10-ea9e-4e80-aac4-d6778b995532 resource_group : res-group tenant_id : 3e52862f-a01e-4b97-98d5-f31a409df682 client_id : 5782deb6-9195-4827-83ae-a13fda90aa0d client_secret : ((opsman-client-secret)) location : westus container : opsmanagerimage network_security_group : ops-manager-security-group vpc_subnet : /subscriptions/<MY_SUBSCRIPTION_ID>/resourceGroups/<MY_RESOURCE_GROUP>/providers/Microsoft.Network/virtualNetworks/<MY_VNET>/subnets/<MY_SUBNET> storage_account : opsman # used for the Ops Manager image ssh_public_key : ssh-rsa AAAAB3NzaC1yc2EAZ... # public key for ssh to VM vm_name : ops-manager-foundation private_ip : 10.0.0.3 # Note that as this contains a secret, it will need to be interpolated. # The opsman-client-secret will need to be set in credhub. GCP 1 2 3 4 5 6 7 8 9 10 11 12 13 14 --- opsman-configuration : gcp : gcp_service_account : ((gcp_service_account_json)) project : project-id region : us-central1 zone : us-central1-b vm_name : ops-manager-foundation vpc_subnet : infrastructure-subnet tags : ops-manager private_ip : 10.0.0.2 # Note that as this contains a secret, it will need to be interpolated. # The gcp_service_account_json will need to be set in credhub. OpenStack 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 --- opsman-configuration : openstack : auth_url : http://os.example.com:5000/v2.0 project_name : project net_id : 26a13112-b6c2-11e8-96f8-529269fb1459 username : admin password : ((opsman-openstack-password)) key_pair_name : opsman-keypair security_group_name : opsman-sec-group vm_name : ops-manager-foundation private_ip : 10.0.0.3 project_domain_name : default user_domain_name : default availability_zone : zone-01 # Note that as this contains a secret, it will need to be interpolated. # The opsman-openstack-password will need to be set in credhub. vSphere 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 --- opsman-configuration : vsphere : vcenter : url : vcenter.example.com username : admin password : ((vcenter-password)) datastore : exmple-ds-1 ca_cert : certificate host : example-host # vCenter host to deploy Ops Manager on datacenter : example-dc resource_pool : /example-dc/host/example-host/Resources/ResPool folder : /example-dc/vm/Folder disk_type : thin private_ip : 10.0.0.2 dns : 8.8.8.8 ntp : ntp.example.com ssh_public_key : ssh-rsa ...... hostname : pcf.example.com network : virtual-network # vcenter network to deploy to netmask : 255.255.255.192 gateway : 192.168.10.1 vm_name : ops-manager-foundation # Note that as this contains a secret, it will need to be interpolated. # The vcenter-password will need to be set in credhub. These examples all make assumptions about the details of your existing Ops Manager's configuration. See the reference docs for this file for more details about your options and per-IaaS caveats. Once you have your config file, commit and push it: 1 2 3 git add foundation/opsman.yml git commit -m \"Add opsman config\" git push Finally, we need the image for the new Ops Manager version. We'll use the download-product task. It requires a config file to specify which Ops Manager to get, and to provide Pivotal Network credentials. Name this file foundation/download-opsman.yml : 1 2 3 4 5 --- pivnet-api-token : ((pivnet-refresh-token)) # interpolated from Credhub pivnet-file-glob : \"ops-manager*.ova\" pivnet-product-slug : ops-manager product-version-regex : ^2\\.5\\.0.*$ You know the drill. 1 2 3 git add foundation/download-opsman.yml git commit -m \"Add download opsman config\" git push Now, we can put it all together: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 - name : upgrade-opsman serial : true plan : - get : platform-automation-image resource : platform-automation params : globs : [ \"*image*.tgz\" ] unpack : true - get : platform-automation-tasks resource : platform-automation params : globs : [ \"*tasks*.zip\" ] unpack : true - get : env - get : installation - task : credhub-interpolate image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : /concourse/your-team-name/foundation # A file path that includes env.yml, opsman.yml, download-opsman.yml INTERPOLATION_PATHS : foundation input_mapping : files : env output_mapping : interpolated-files : interpolated-configs - task : download-opsman-image image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : download-opsman.yml input_mapping : config : interpolated-configs - task : upgrade-opsman image : platform-automation-image file : platform-automation-tasks/tasks/upgrade-opsman.yml input_mapping : config : interpolated-configs image : downloaded-product secrets : interpolated-configs state : env params : ENV_FILE : foundation/env.yml OPSMAN_CONFIG_FILE : foundation/opsman.yml STATE_FILE : foundation/state.yml Set the pipeline. Before we run the job, we should ensure that state.yml is always persisted regardless of whether the upgrade-opsman job failed or passed. To do this, we can add the following section to the job: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 - name : upgrade-opsman serial : true plan : - get : platform-automation-image resource : platform-automation params : globs : [ \"*image*.tgz\" ] unpack : true - get : platform-automation-tasks resource : platform-automation params : globs : [ \"*tasks*.zip\" ] unpack : true - get : env - get : installation - task : credhub-interpolate image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : /concourse/your-team-name/foundation # A file path that includes env.yml, opsman.yml, download-opsman.yml INTERPOLATION_PATHS : foundation input_mapping : files : env output_mapping : interpolated-files : interpolated-configs - task : download-opsman-image image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : download-opsman.yml input_mapping : config : interpolated-configs - task : upgrade-opsman image : platform-automation-image file : platform-automation-tasks/tasks/upgrade-opsman.yml input_mapping : config : interpolated-configs image : downloaded-product secrets : interpolated-configs state : env params : ENV_FILE : foundation/env.yml OPSMAN_CONFIG_FILE : foundation/opsman.yml STATE_FILE : foundation/state.yml ensure : do : - task : make-commit image : platform-automation-image file : platform-automation-tasks/tasks/make-git-commit.yml input_mapping : repository : env file-source : generated-state output_mapping : repository-commit : env-commit params : FILE_SOURCE_PATH : foundation/state.yml FILE_DESTINATION_PATH : foundation/state.yml GIT_AUTHOR_EMAIL : \"ci-user@example.com\" GIT_AUTHOR_NAME : \"CI User\" COMMIT_MESSAGE : 'Update state file' - put : env params : repository : env-commit merge : true Set the pipeline one final time, run the job, and see it pass. 1 2 3 git add pipeline.yml git commit -m \"Upgrade Ops Manager in CI\" git push Your upgrade pipeline is now complete. You are now free to move on to the next steps of your automation journey.","title":"Performing The Upgrade"},{"location":"pipeline/multiple-products.html","text":"Below you will find a reference pipeline that illustrates the tasks and provides an example of a basic pipeline design. You know your environment and constraints and we don't - we recommend you look at the tasks that make up the pipeline, and see how they can be arranged for your specific automation needs. For a deeper dive into each task see the Task Reference. These Concourse pipelines are examples on how to use the tasks . If you use a different CI/CD platform, you can use these Concourse files as examples of the inputs, outputs, and arguments used in each step in the workflow. Prerequisites Deployed Concourse Info Platform Automation for PCF is based on Concourse CI. We recommend that you have some familiarity with Concourse before getting started. If you are new to Concourse, Concourse CI Tutorials would be a good place to start. Persisted datastore that can be accessed by Concourse resource (e.g. s3, gcs, minio) A valid generating-env-file : this file will contain credentials necessary to login to Ops Manager using the om CLI. It is used by every task within Platform Automation for PCF A valid auth-file : this file will contain the credentials necessary to create the Ops Manager login the first time the VM is created. The choices for this file are simple or saml authentication. Info There will be some crossover between the auth file and the env file due to how om is setup and how the system works. It is highly recommended to parameterize these values, and let a credential management system (such as Credhub) fill in these values for you in order to maintain consistency across files. An [opsman-configuration] file: This file is required to connect to an IAAS, and control the lifecycle management of the Ops Manager VM A director-configuration file: Each Ops Manager needs its own configuration, but it is retrieved differently from a product configuration. This config is used to deploy a new Ops Manager director, or update an existing one. A set of valid product-configuration files: Each product configuration is a yaml file that contains the properties necessary to configure an Ops Manager product using the om tool. This can be used during install or update. (Optional) A working credhub setup with its own UAA client and secret. Retrieving products from Pivnet Please ensure products have been procured from Pivotal Network using the reference-resources . Installing Ops Manager and multiple products The pipeline shows how to compose the tasks to install Ops Manager and the PCF and Healthwatch products. Its dependencies are coming from a trusted git repository, which can be retrieved using this pipeline . Pipeline Components S3 Resources These can either be uploaded manually or from the reference resources pipeline . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 resources : - name : platform-automation-tasks type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : .*tasks-(.*).zip - name : platform-automation-image type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : .*image-(.*).tgz - name : opsman-image type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : \\[ops-manager,(.*)\\].*.ova # vsphere ex: ops-manager-(.*).ova - name : pas-stemcell type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : pas-stemcell/\\[stemcells-ubuntu-xenial,(.*)\\]bosh-stemcell-.*-vsphere.*\\.tgz - name : pas-product type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : \\[elastic-runtime,(.*)\\]cf-.*.pivotal - name : pas-windows-product type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : \\[pas-windows,(.*)\\]pas-windows-.*.pivotal - name : pas-windows-stemcell type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : pas-windows-stemcell/\\[stemcells-windows-server,(.*)\\]bosh-stemcell-.*-vsphere.*\\.tgz - name : healthwatch-stemcell type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : healthwatch-stemcell/\\[stemcells-ubuntu-xenial,(.*)\\]bosh-stemcell-.*-vsphere.*\\.tgz - name : healthwatch-product type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : \\[p-healthwatch,(.*)\\]p-healthwatch-.*.pivotal Exported Installation Resource Warning It is recommended to persist the zip file exported from export-installation to an external file store (eg S3) on a regular basis. The exported installation can restore the Ops Manager to a working state if it is non-functional. 1 2 3 4 5 6 7 8 - name : installation type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.installation)) regexp : installation-(.*).zip Configured Resources These contain values for opsman vm creation, director, product, foundation-specific vars, auth, and env files. For more details, see the Inputs and Outputs section. Platform Automation will not create these resources for you. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # configurations - name : configuration type : git source : private_key : ((configuration.private_key)) uri : ((configuration.uri)) branch : master - name : variable type : git source : private_key : ((variable.private_key)) uri : ((variable.uri)) branch : master Trigger Resources 1 2 3 4 5 6 7 8 9 10 # triggers used to have jobs do something in a timely manner - name : one-time-trigger type : time source : interval : 999999h - name : daily-trigger type : time source : interval : 24h Credhub Interpolate Job ((foundation)) is a value intended to be replaced by the filepath of your foundation directory structure in github (if you are not using multi-foundation, this value can be removed). ((credhub-*)) are values for accessing your Concourse Credhub. These are set when fly -ing your pipeline. For more information on how to fly your pipeline and use ((foundation)) , please reference our How To Guides for your specific workflow. Platform Automation will not create your Credhub or store values into your Credhub for you. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # This task is used in multiple jobs # The yaml anchor \"*interpolate-creds\" is used in its place interpolate-creds : &interpolate-creds image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : '/pipeline/vsphere' INTERPOLATION_PATHS : ((foundation))/config ((foundation))/env SKIP_MISSING : true input_mapping : files : configuration output_mapping : interpolated-files : interpolated-creds Jobs Each job corresponds to a \"box\" on the visual representation of your Concourse pipeline. These jobs consume resources defined above. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 jobs : - name : install-opsman serial : true serial_groups : [ install ] plan : - aggregate : - get : platform-automation-image params : unpack : true - get : one-time-trigger trigger : true - get : platform-automation-tasks params : unpack : true - get : configuration - task : interpolate-download-opsman-image-creds image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : '/pipeline/vsphere' INTERPOLATION_PATHS : \"download-product-configs\" SKIP_MISSING : true input_mapping : files : configuration output_mapping : interpolated-files : product-configs - task : download-opsman-image image : platform-automation-image file : platform-automation-tasks/tasks/download-product-s3.yml params : CONFIG_FILE : download-product-configs/opsman.yml input_mapping : config : product-configs output_mapping : downloaded-product : opsman-image - get : variable - task : interpolate-creds << : *interpolate-creds - task : create-vm image : platform-automation-image file : platform-automation-tasks/tasks/create-vm.yml input_mapping : image : opsman-image state : configuration config : interpolated-creds vars : variable params : VARS_FILES : vars/((foundation))/vars/opsman-vars.yml STATE_FILE : ((foundation))/state/state.yml OPSMAN_CONFIG_FILE : ((foundation))/config/opsman.yml ensure : &make-state-commit do : - task : make-commit image : platform-automation-image file : platform-automation-tasks/tasks/make-git-commit.yml input_mapping : repository : configuration file-source : generated-state output_mapping : repository-commit : configuration-commit params : FILE_SOURCE_PATH : state.yml FILE_DESTINATION_PATH : ((foundation))/state/state.yml GIT_AUTHOR_EMAIL : \"pcf-pipeline-bot@example.com\" GIT_AUTHOR_NAME : \"Platform Automation Bot\" COMMIT_MESSAGE : 'Update state file' - put : configuration params : repository : configuration-commit merge : true - task : configure-authentication image : platform-automation-image file : platform-automation-tasks/tasks/configure-authentication.yml attempts : 10 input_mapping : env : configuration config : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml AUTH_CONFIG_FILE : ((foundation))/config/auth.yml - task : configure-director image : platform-automation-image file : platform-automation-tasks/tasks/configure-director.yml input_mapping : config : interpolated-creds env : interpolated-creds vars : variable params : VARS_FILES : vars/((foundation))/vars/director-vars.yml ENV_FILE : ((foundation))/env/env.yml DIRECTOR_CONFIG_FILE : ((foundation))/config/director.yml - task : apply-director-changes image : platform-automation-image file : platform-automation-tasks/tasks/apply-director-changes.yml input_mapping : env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml - name : staged-director-config serial : true plan : - aggregate : - get : platform-automation-tasks params : { unpack : true } - get : platform-automation-image params : { unpack : true } - get : configuration - task : interpolate-creds << : *interpolate-creds - task : staged-director-config image : platform-automation-image file : platform-automation-tasks/tasks/staged-director-config.yml input_mapping : env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml ensure : *make-state-commit - name : export-installation serial : true plan : - aggregate : - get : daily-trigger trigger : true - get : platform-automation-image params : unpack : true passed : [ install-opsman ] - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - get : one-time-trigger passed : [ install-opsman ] - task : interpolate-creds << : *interpolate-creds - task : export-installation image : platform-automation-image file : platform-automation-tasks/tasks/export-installation.yml input_mapping : env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml INSTALLATION_FILE : installation-$timestamp.zip - put : installation params : file : installation/installation*.zip - name : upgrade-opsman serial : true serial_groups : [ install ] plan : - aggregate : - get : one-time-trigger passed : [ export-installation ] - get : platform-automation-image params : unpack : true trigger : true - get : platform-automation-tasks params : unpack : true - get : opsman-image - get : installation passed : [ export-installation ] - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : upgrade-opsman image : platform-automation-image file : platform-automation-tasks/tasks/upgrade-opsman.yml input_mapping : image : opsman-image state : configuration config : interpolated-creds env : interpolated-creds vars : variable params : VARS_FILES : vars/((foundation))/vars/opsman-vars.yml ENV_FILE : ((foundation))/env/env.yml OPSMAN_CONFIG_FILE : ((foundation))/config/opsman.yml STATE_FILE : ((foundation))/state/state.yml ensure : *make-state-commit - task : configure-director image : platform-automation-image file : platform-automation-tasks/tasks/configure-director.yml input_mapping : config : interpolated-creds env : interpolated-creds vars : variable params : VARS_FILES : vars/((foundation))/vars/director-vars.yml ENV_FILE : ((foundation))/env/env.yml DIRECTOR_CONFIG_FILE : ((foundation))/config/director.yml - task : apply-director-changes image : platform-automation-image file : platform-automation-tasks/tasks/apply-director-changes.yml input_mapping : env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml - name : upload-and-stage-pas serial : true plan : - aggregate : - get : platform-automation-image params : unpack : true passed : - upgrade-opsman trigger : true - get : platform-automation-tasks params : unpack : true - get : pas-product - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : upload-product image : platform-automation-image file : platform-automation-tasks/tasks/upload-product.yml input_mapping : product : pas-product env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml - task : stage-product image : platform-automation-image file : platform-automation-tasks/tasks/stage-product.yml input_mapping : product : pas-product env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml - name : configure-pas serial : true plan : - aggregate : - get : platform-automation-image params : unpack : true passed : - upload-and-stage-pas trigger : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : configure-pas image : platform-automation-image file : platform-automation-tasks/tasks/configure-product.yml input_mapping : config : interpolated-creds env : interpolated-creds vars : variable params : CONFIG_FILE : ((foundation))/config/cf.yml VARS_FILES : vars/((foundation))/vars/cf-vars.yml ENV_FILE : ((foundation))/env/env.yml - name : upload-and-stage-pas-windows serial : true plan : - aggregate : - get : platform-automation-image params : unpack : true passed : - upgrade-opsman trigger : true - get : platform-automation-tasks params : unpack : true - get : pas-windows-product - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : upload-product image : platform-automation-image file : platform-automation-tasks/tasks/upload-product.yml input_mapping : product : pas-windows-product env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml - task : stage-product image : platform-automation-image file : platform-automation-tasks/tasks/stage-product.yml input_mapping : product : pas-windows-product env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml - name : configure-pas-windows serial : true plan : - aggregate : - get : platform-automation-image params : unpack : true passed : - upload-and-stage-pas-windows trigger : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : configure-pas image : platform-automation-image file : platform-automation-tasks/tasks/configure-product.yml input_mapping : config : interpolated-creds env : interpolated-creds vars : variable params : CONFIG_FILE : ((foundation))/config/pas-windows.yml VARS_FILES : vars/((foundation))/vars/cf-vars.yml ENV_FILE : ((foundation))/env/env.yml - name : upload-stemcells serial : true plan : - aggregate : - get : platform-automation-image params : unpack : true passed : - upgrade-opsman trigger : true - get : platform-automation-tasks params : unpack : true - get : pas-stemcell trigger : true - get : pas-windows-stemcell trigger : true - get : healthwatch-stemcell trigger : true - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : upload-pas-stemcell image : platform-automation-image file : platform-automation-tasks/tasks/upload-stemcell.yml input_mapping : env : interpolated-creds stemcell : pas-stemcell params : ENV_FILE : ((foundation))/env/env.yml - task : upload-pas-windows-stemcell image : platform-automation-image file : platform-automation-tasks/tasks/upload-stemcell.yml input_mapping : env : interpolated-creds stemcell : pas-windows-stemcell params : ENV_FILE : ((foundation))/env/env.yml - task : upload-healthwatch-stemcell image : platform-automation-image file : platform-automation-tasks/tasks/upload-stemcell.yml input_mapping : env : interpolated-creds stemcell : healthwatch-stemcell params : ENV_FILE : ((foundation))/env/env.yml - name : upload-and-stage-healthwatch serial : true plan : - aggregate : - get : platform-automation-image params : unpack : true trigger : true passed : [ \"upgrade-opsman\" ] - get : platform-automation-tasks params : unpack : true - get : healthwatch-product - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : upload-and-stage-product image : platform-automation-image file : platform-automation-tasks/tasks/upload-and-stage-product.yml input_mapping : product : healthwatch-product env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml - name : configure-healthwatch serial : true plan : - aggregate : - get : platform-automation-image params : unpack : true passed : - upload-and-stage-healthwatch trigger : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : configure-healthwatch image : platform-automation-image file : platform-automation-tasks/tasks/configure-product.yml input_mapping : config : interpolated-creds env : interpolated-creds vars : variable params : CONFIG_FILE : ((foundation))/config/healthwatch.yml ENV_FILE : ((foundation))/env/env.yml - name : apply-product-changes serial : true plan : - aggregate : - get : platform-automation-image params : unpack : true passed : - configure-healthwatch - configure-pas - configure-pas-windows trigger : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : apply-product-changes image : platform-automation-image file : platform-automation-tasks/tasks/apply-changes.yml input_mapping : env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml - name : staged-pas-config plan : - aggregate : - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : staged-config image : platform-automation-image file : platform-automation-tasks/tasks/staged-config.yml input_mapping : env : interpolated-creds params : PRODUCT_NAME : cf ENV_FILE : ((foundation))/env/env.yml ensure : *make-state-commit - name : staged-healthwatch-config plan : - aggregate : - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : staged-config image : platform-automation-image file : platform-automation-tasks/tasks/staged-config.yml input_mapping : env : interpolated-creds params : PRODUCT_NAME : p-healthwatch ENV_FILE : ((foundation))/env/env.yml ensure : *make-state-commit","title":"Ops Manager + multiple products"},{"location":"pipeline/multiple-products.html#prerequisites","text":"Deployed Concourse Info Platform Automation for PCF is based on Concourse CI. We recommend that you have some familiarity with Concourse before getting started. If you are new to Concourse, Concourse CI Tutorials would be a good place to start. Persisted datastore that can be accessed by Concourse resource (e.g. s3, gcs, minio) A valid generating-env-file : this file will contain credentials necessary to login to Ops Manager using the om CLI. It is used by every task within Platform Automation for PCF A valid auth-file : this file will contain the credentials necessary to create the Ops Manager login the first time the VM is created. The choices for this file are simple or saml authentication. Info There will be some crossover between the auth file and the env file due to how om is setup and how the system works. It is highly recommended to parameterize these values, and let a credential management system (such as Credhub) fill in these values for you in order to maintain consistency across files. An [opsman-configuration] file: This file is required to connect to an IAAS, and control the lifecycle management of the Ops Manager VM A director-configuration file: Each Ops Manager needs its own configuration, but it is retrieved differently from a product configuration. This config is used to deploy a new Ops Manager director, or update an existing one. A set of valid product-configuration files: Each product configuration is a yaml file that contains the properties necessary to configure an Ops Manager product using the om tool. This can be used during install or update. (Optional) A working credhub setup with its own UAA client and secret. Retrieving products from Pivnet Please ensure products have been procured from Pivotal Network using the reference-resources .","title":"Prerequisites"},{"location":"pipeline/multiple-products.html#installing-ops-manager-and-multiple-products","text":"The pipeline shows how to compose the tasks to install Ops Manager and the PCF and Healthwatch products. Its dependencies are coming from a trusted git repository, which can be retrieved using this pipeline .","title":"Installing Ops Manager and multiple products"},{"location":"pipeline/multiple-products.html#pipeline-components","text":"","title":"Pipeline Components"},{"location":"pipeline/multiple-products.html#s3-resources","text":"These can either be uploaded manually or from the reference resources pipeline . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 resources : - name : platform-automation-tasks type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : .*tasks-(.*).zip - name : platform-automation-image type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : .*image-(.*).tgz - name : opsman-image type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : \\[ops-manager,(.*)\\].*.ova # vsphere ex: ops-manager-(.*).ova - name : pas-stemcell type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : pas-stemcell/\\[stemcells-ubuntu-xenial,(.*)\\]bosh-stemcell-.*-vsphere.*\\.tgz - name : pas-product type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : \\[elastic-runtime,(.*)\\]cf-.*.pivotal - name : pas-windows-product type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : \\[pas-windows,(.*)\\]pas-windows-.*.pivotal - name : pas-windows-stemcell type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : pas-windows-stemcell/\\[stemcells-windows-server,(.*)\\]bosh-stemcell-.*-vsphere.*\\.tgz - name : healthwatch-stemcell type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : healthwatch-stemcell/\\[stemcells-ubuntu-xenial,(.*)\\]bosh-stemcell-.*-vsphere.*\\.tgz - name : healthwatch-product type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : \\[p-healthwatch,(.*)\\]p-healthwatch-.*.pivotal","title":"S3 Resources"},{"location":"pipeline/multiple-products.html#exported-installation-resource","text":"Warning It is recommended to persist the zip file exported from export-installation to an external file store (eg S3) on a regular basis. The exported installation can restore the Ops Manager to a working state if it is non-functional. 1 2 3 4 5 6 7 8 - name : installation type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.installation)) regexp : installation-(.*).zip","title":"Exported Installation Resource"},{"location":"pipeline/multiple-products.html#configured-resources","text":"These contain values for opsman vm creation, director, product, foundation-specific vars, auth, and env files. For more details, see the Inputs and Outputs section. Platform Automation will not create these resources for you. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # configurations - name : configuration type : git source : private_key : ((configuration.private_key)) uri : ((configuration.uri)) branch : master - name : variable type : git source : private_key : ((variable.private_key)) uri : ((variable.uri)) branch : master","title":"Configured Resources"},{"location":"pipeline/multiple-products.html#trigger-resources","text":"1 2 3 4 5 6 7 8 9 10 # triggers used to have jobs do something in a timely manner - name : one-time-trigger type : time source : interval : 999999h - name : daily-trigger type : time source : interval : 24h","title":"Trigger Resources"},{"location":"pipeline/multiple-products.html#credhub-interpolate-job","text":"((foundation)) is a value intended to be replaced by the filepath of your foundation directory structure in github (if you are not using multi-foundation, this value can be removed). ((credhub-*)) are values for accessing your Concourse Credhub. These are set when fly -ing your pipeline. For more information on how to fly your pipeline and use ((foundation)) , please reference our How To Guides for your specific workflow. Platform Automation will not create your Credhub or store values into your Credhub for you. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # This task is used in multiple jobs # The yaml anchor \"*interpolate-creds\" is used in its place interpolate-creds : &interpolate-creds image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : '/pipeline/vsphere' INTERPOLATION_PATHS : ((foundation))/config ((foundation))/env SKIP_MISSING : true input_mapping : files : configuration output_mapping : interpolated-files : interpolated-creds","title":"Credhub Interpolate Job"},{"location":"pipeline/multiple-products.html#jobs","text":"Each job corresponds to a \"box\" on the visual representation of your Concourse pipeline. These jobs consume resources defined above. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 jobs : - name : install-opsman serial : true serial_groups : [ install ] plan : - aggregate : - get : platform-automation-image params : unpack : true - get : one-time-trigger trigger : true - get : platform-automation-tasks params : unpack : true - get : configuration - task : interpolate-download-opsman-image-creds image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : '/pipeline/vsphere' INTERPOLATION_PATHS : \"download-product-configs\" SKIP_MISSING : true input_mapping : files : configuration output_mapping : interpolated-files : product-configs - task : download-opsman-image image : platform-automation-image file : platform-automation-tasks/tasks/download-product-s3.yml params : CONFIG_FILE : download-product-configs/opsman.yml input_mapping : config : product-configs output_mapping : downloaded-product : opsman-image - get : variable - task : interpolate-creds << : *interpolate-creds - task : create-vm image : platform-automation-image file : platform-automation-tasks/tasks/create-vm.yml input_mapping : image : opsman-image state : configuration config : interpolated-creds vars : variable params : VARS_FILES : vars/((foundation))/vars/opsman-vars.yml STATE_FILE : ((foundation))/state/state.yml OPSMAN_CONFIG_FILE : ((foundation))/config/opsman.yml ensure : &make-state-commit do : - task : make-commit image : platform-automation-image file : platform-automation-tasks/tasks/make-git-commit.yml input_mapping : repository : configuration file-source : generated-state output_mapping : repository-commit : configuration-commit params : FILE_SOURCE_PATH : state.yml FILE_DESTINATION_PATH : ((foundation))/state/state.yml GIT_AUTHOR_EMAIL : \"pcf-pipeline-bot@example.com\" GIT_AUTHOR_NAME : \"Platform Automation Bot\" COMMIT_MESSAGE : 'Update state file' - put : configuration params : repository : configuration-commit merge : true - task : configure-authentication image : platform-automation-image file : platform-automation-tasks/tasks/configure-authentication.yml attempts : 10 input_mapping : env : configuration config : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml AUTH_CONFIG_FILE : ((foundation))/config/auth.yml - task : configure-director image : platform-automation-image file : platform-automation-tasks/tasks/configure-director.yml input_mapping : config : interpolated-creds env : interpolated-creds vars : variable params : VARS_FILES : vars/((foundation))/vars/director-vars.yml ENV_FILE : ((foundation))/env/env.yml DIRECTOR_CONFIG_FILE : ((foundation))/config/director.yml - task : apply-director-changes image : platform-automation-image file : platform-automation-tasks/tasks/apply-director-changes.yml input_mapping : env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml - name : staged-director-config serial : true plan : - aggregate : - get : platform-automation-tasks params : { unpack : true } - get : platform-automation-image params : { unpack : true } - get : configuration - task : interpolate-creds << : *interpolate-creds - task : staged-director-config image : platform-automation-image file : platform-automation-tasks/tasks/staged-director-config.yml input_mapping : env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml ensure : *make-state-commit - name : export-installation serial : true plan : - aggregate : - get : daily-trigger trigger : true - get : platform-automation-image params : unpack : true passed : [ install-opsman ] - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - get : one-time-trigger passed : [ install-opsman ] - task : interpolate-creds << : *interpolate-creds - task : export-installation image : platform-automation-image file : platform-automation-tasks/tasks/export-installation.yml input_mapping : env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml INSTALLATION_FILE : installation-$timestamp.zip - put : installation params : file : installation/installation*.zip - name : upgrade-opsman serial : true serial_groups : [ install ] plan : - aggregate : - get : one-time-trigger passed : [ export-installation ] - get : platform-automation-image params : unpack : true trigger : true - get : platform-automation-tasks params : unpack : true - get : opsman-image - get : installation passed : [ export-installation ] - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : upgrade-opsman image : platform-automation-image file : platform-automation-tasks/tasks/upgrade-opsman.yml input_mapping : image : opsman-image state : configuration config : interpolated-creds env : interpolated-creds vars : variable params : VARS_FILES : vars/((foundation))/vars/opsman-vars.yml ENV_FILE : ((foundation))/env/env.yml OPSMAN_CONFIG_FILE : ((foundation))/config/opsman.yml STATE_FILE : ((foundation))/state/state.yml ensure : *make-state-commit - task : configure-director image : platform-automation-image file : platform-automation-tasks/tasks/configure-director.yml input_mapping : config : interpolated-creds env : interpolated-creds vars : variable params : VARS_FILES : vars/((foundation))/vars/director-vars.yml ENV_FILE : ((foundation))/env/env.yml DIRECTOR_CONFIG_FILE : ((foundation))/config/director.yml - task : apply-director-changes image : platform-automation-image file : platform-automation-tasks/tasks/apply-director-changes.yml input_mapping : env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml - name : upload-and-stage-pas serial : true plan : - aggregate : - get : platform-automation-image params : unpack : true passed : - upgrade-opsman trigger : true - get : platform-automation-tasks params : unpack : true - get : pas-product - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : upload-product image : platform-automation-image file : platform-automation-tasks/tasks/upload-product.yml input_mapping : product : pas-product env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml - task : stage-product image : platform-automation-image file : platform-automation-tasks/tasks/stage-product.yml input_mapping : product : pas-product env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml - name : configure-pas serial : true plan : - aggregate : - get : platform-automation-image params : unpack : true passed : - upload-and-stage-pas trigger : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : configure-pas image : platform-automation-image file : platform-automation-tasks/tasks/configure-product.yml input_mapping : config : interpolated-creds env : interpolated-creds vars : variable params : CONFIG_FILE : ((foundation))/config/cf.yml VARS_FILES : vars/((foundation))/vars/cf-vars.yml ENV_FILE : ((foundation))/env/env.yml - name : upload-and-stage-pas-windows serial : true plan : - aggregate : - get : platform-automation-image params : unpack : true passed : - upgrade-opsman trigger : true - get : platform-automation-tasks params : unpack : true - get : pas-windows-product - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : upload-product image : platform-automation-image file : platform-automation-tasks/tasks/upload-product.yml input_mapping : product : pas-windows-product env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml - task : stage-product image : platform-automation-image file : platform-automation-tasks/tasks/stage-product.yml input_mapping : product : pas-windows-product env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml - name : configure-pas-windows serial : true plan : - aggregate : - get : platform-automation-image params : unpack : true passed : - upload-and-stage-pas-windows trigger : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : configure-pas image : platform-automation-image file : platform-automation-tasks/tasks/configure-product.yml input_mapping : config : interpolated-creds env : interpolated-creds vars : variable params : CONFIG_FILE : ((foundation))/config/pas-windows.yml VARS_FILES : vars/((foundation))/vars/cf-vars.yml ENV_FILE : ((foundation))/env/env.yml - name : upload-stemcells serial : true plan : - aggregate : - get : platform-automation-image params : unpack : true passed : - upgrade-opsman trigger : true - get : platform-automation-tasks params : unpack : true - get : pas-stemcell trigger : true - get : pas-windows-stemcell trigger : true - get : healthwatch-stemcell trigger : true - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : upload-pas-stemcell image : platform-automation-image file : platform-automation-tasks/tasks/upload-stemcell.yml input_mapping : env : interpolated-creds stemcell : pas-stemcell params : ENV_FILE : ((foundation))/env/env.yml - task : upload-pas-windows-stemcell image : platform-automation-image file : platform-automation-tasks/tasks/upload-stemcell.yml input_mapping : env : interpolated-creds stemcell : pas-windows-stemcell params : ENV_FILE : ((foundation))/env/env.yml - task : upload-healthwatch-stemcell image : platform-automation-image file : platform-automation-tasks/tasks/upload-stemcell.yml input_mapping : env : interpolated-creds stemcell : healthwatch-stemcell params : ENV_FILE : ((foundation))/env/env.yml - name : upload-and-stage-healthwatch serial : true plan : - aggregate : - get : platform-automation-image params : unpack : true trigger : true passed : [ \"upgrade-opsman\" ] - get : platform-automation-tasks params : unpack : true - get : healthwatch-product - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : upload-and-stage-product image : platform-automation-image file : platform-automation-tasks/tasks/upload-and-stage-product.yml input_mapping : product : healthwatch-product env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml - name : configure-healthwatch serial : true plan : - aggregate : - get : platform-automation-image params : unpack : true passed : - upload-and-stage-healthwatch trigger : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : configure-healthwatch image : platform-automation-image file : platform-automation-tasks/tasks/configure-product.yml input_mapping : config : interpolated-creds env : interpolated-creds vars : variable params : CONFIG_FILE : ((foundation))/config/healthwatch.yml ENV_FILE : ((foundation))/env/env.yml - name : apply-product-changes serial : true plan : - aggregate : - get : platform-automation-image params : unpack : true passed : - configure-healthwatch - configure-pas - configure-pas-windows trigger : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : apply-product-changes image : platform-automation-image file : platform-automation-tasks/tasks/apply-changes.yml input_mapping : env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml - name : staged-pas-config plan : - aggregate : - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : staged-config image : platform-automation-image file : platform-automation-tasks/tasks/staged-config.yml input_mapping : env : interpolated-creds params : PRODUCT_NAME : cf ENV_FILE : ((foundation))/env/env.yml ensure : *make-state-commit - name : staged-healthwatch-config plan : - aggregate : - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : staged-config image : platform-automation-image file : platform-automation-tasks/tasks/staged-config.yml input_mapping : env : interpolated-creds params : PRODUCT_NAME : p-healthwatch ENV_FILE : ((foundation))/env/env.yml ensure : *make-state-commit","title":"Jobs"},{"location":"pipeline/resources.html","text":"Below you will find a reference pipeline that illustrates the tasks and provides an example of a basic pipeline design. You know your environment and constraints and we don't - we recommend you look at the tasks that make up the pipeline, and see how they can be arranged for your specific automation needs. For a deeper dive into each task see the Task Reference. These Concourse pipelines are examples on how to use the tasks . If you use a different CI/CD platform, you can use these Concourse files as examples of the inputs, outputs, and arguments used in each step in the workflow. Prerequisites Deployed Concourse Info Platform Automation for PCF is based on Concourse CI. We recommend that you have some familiarity with Concourse before getting started. If you are new to Concourse, Concourse CI Tutorials would be a good place to start. Persisted datastore that can be accessed by Concourse resource (e.g. s3, gcs, minio) Pivnet access to Platform Automation A set of valid download-product-config files: Each product has a configuration YAML of what version to download from Pivotal Network. Retrieval from Pivotal Network Ops Manager 2.5 The filename for the artifact downloaded from Ops Manager is changed! If your resources or pipelines have a regex for the Ops Manager filename, you may be affected. (Please see Ops Manager's official notice for more information) The pipeline downloads dependencies consumed by the tasks and places them into a trusted s3-like storage provider. This helps other concourse deployments without internet access retrieve task dependencies. S3 filename prefixing Note the unique regex format for blob names, for example: \\[p-healthwatch,(.*)\\]p-healthwatch-.*.pivotal . Pivnet filenames will not always contain the necessary metadata to accurately download files from S3. So, the product slug and version are prepended when using download-product . For more information on how this works, and what to expect when using download-product and download-product-s3 , refer to the download-product task reference. The pipeline requires configuration for the download-product task. Below are examples that can be used. Healthwatch 1 2 3 4 5 6 7 8 9 10 11 --- pivnet-api-token : ((pivnet-token)) pivnet-file-glob : \"*.pivotal\" pivnet-product-slug : p-healthwatch product-version : 1.4.4 stemcell-iaas : vsphere s3-access-key-id : ((s3_access_key_id)) s3-secret-access-key : ((s3_secret_access_key)) s3-bucket : ((s3_pivnet_products_bucket)) s3-region-name : ((s3_region_name)) PAS 1 2 3 4 5 6 7 8 9 10 11 --- pivnet-api-token : ((pivnet-token)) pivnet-file-glob : \"*cf*.pivotal\" pivnet-product-slug : elastic-runtime product-version-regex : ^2\\.[56]\\.0.*$ stemcell-iaas : vsphere s3-access-key-id : ((s3_access_key_id)) s3-secret-access-key : ((s3_secret_access_key)) s3-bucket : ((s3_pivnet_products_bucket)) s3-region-name : ((s3_region_name)) PAS Windows 1 2 3 4 5 6 7 8 9 10 --- pivnet-api-token : ((pivnet-token)) pivnet-file-glob : \"*.pivotal\" pivnet-product-slug : pas-windows product-version-regex : ^2\\.[56]\\.0.*$ s3-access-key-id : ((s3_access_key_id)) s3-secret-access-key : ((s3_secret_access_key)) s3-bucket : ((s3_pivnet_products_bucket)) s3-region-name : ((s3_region_name)) OpsMan 1 2 3 4 5 6 7 8 9 10 --- pivnet-api-token : ((pivnet-token)) pivnet-file-glob : \"ops-manager*.ova\" pivnet-product-slug : ops-manager product-version-regex : ^2\\.[56]\\.\\d+.*$ s3-access-key-id : ((s3_access_key_id)) s3-secret-access-key : ((s3_secret_access_key)) s3-bucket : ((s3_pivnet_products_bucket)) s3-region-name : ((s3_region_name)) PKS 1 2 3 4 5 6 7 8 9 10 11 --- pivnet-api-token : ((pivnet-token)) pivnet-file-glob : \"pivotal-container-service*.pivotal\" pivnet-product-slug : pivotal-container-service product-version-regex : ^1\\.3\\..*$ stemcell-iaas : vsphere s3-access-key-id : ((s3_access_key_id)) s3-secret-access-key : ((s3_secret_access_key)) s3-bucket : ((s3_pivnet_products_bucket)) s3-region-name : ((s3_region_name)) Pipeline Components Resource Types This custom resource type uses the pivnet resource to pull down and separate both pieces of the Platform Automation product (tasks and image) so they can be stored separately in S3. 1 2 3 4 5 6 resource_types : - name : pivnet type : docker-image source : repository : pivotalcf/pivnet-resource tag : latest-final Product Resources S3 resources where Platform Automation download-product outputs will be stored. Each product/stemcell needs a separate resource defined. Platform Automation will not create these resources for you. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 resources : - name : healthwatch-product type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : \\[p-healthwatch,(.*)\\]p-healthwatch-.*.pivotal - name : healthwatch-stemcell type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : healthwatch-stemcell/\\[stemcells-ubuntu-xenial,(.*)\\]bosh-stemcell-.*-vsphere.*\\.tgz - name : opsman-product type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : \\[ops-manager,(.*)\\].*.ova - name : pas-product type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : \\[elastic-runtime,(.*)\\]cf-.*.pivotal - name : pas-stemcell type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : pas-stemcell/\\[stemcells-ubuntu-xenial,(.*)\\]bosh-stemcell-.*-vsphere.*\\.tgz - name : pks-product type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : \\[pivotal-container-service,(.*)\\]pivotal-container-service-.*.pivotal - name : pks-stemcell type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : pks-stemcell/\\[stemcells-ubuntu-xenial,(.*)\\]bosh-stemcell-.*-vsphere.*\\.tgz - name : pas-windows-product type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : \\[pas-windows,(.*)\\]pas-windows-.*.pivotal Platform Automation Resources platform-automation-pivnet is downloaded directly from Pivnet and will be used to download all other products from Pivnet. platform-automation-tasks and platform-automation-image are S3 resources that will be stored for internet-restricted, or faster, access. Platform Automation will not create this resource for you. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 - name : platform-automation-pivnet type : pivnet source : api_token : ((pivnet_token)) product_slug : platform-automation product_version : 2\\.(.*) sort_by : semver - name : platform-automation-tasks type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : platform-automation-tasks-(.*).zip - name : platform-automation-image type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : platform-automation-image-(.*).tgz Configured Resources You will need to add your download-product configuration configuration files to your configurations repo. Platform Automation will not create these resources for you. For more details, see the Inputs and Outputs section. 1 2 3 4 5 6 7 8 - name : config type : git source : private_key : ((configuration.private_key)) uri : ((configuration.uri)) branch : master submodules : all depth : 1 Trigger Resources 1 2 3 4 - name : daily type : time source : interval : 24h Credhub Interpolate Job ((foundation)) is a value intended to be replaced by the filepath of your foundation directory structure in github (if you are not using multi-foundation, this value can be removed). ((credhub-*)) are values for accessing your Concourse Credhub. These are set when fly -ing your pipeline. For more information on how to fly your pipeline and use ((foundation)) , please reference our How To Guides for your specific workflow. Platform Automation will not create your Credhub or store values into your Credhub for you. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # This task is used in multiple jobs # The yaml anchor \"*credhub-interpolate\" is used in its place credhub-interpolate : &credhub-interpolate image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : '/pipeline/vsphere' INTERPOLATION_PATHS : \"download-product-configs\" input_mapping : files : config output_mapping : interpolated-files : config Jobs Each job corresponds to a \"box\" on the visual representation of your Concourse pipeline. These jobs consume resources defined above. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 jobs : - name : fetch-healthwatch plan : - aggregate : - get : daily trigger : true - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : config - task : credhub-interpolate << : *credhub-interpolate - task : download-healthwatch-product-and-stemcell image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : download-product-configs/healthwatch.yml output_mapping : { downloaded-stemcell : healthwatch-stemcell } - aggregate : - put : healthwatch-product params : file : downloaded-product/*.pivotal - put : healthwatch-stemcell params : file : healthwatch-stemcell/*.tgz - name : fetch-opsman plan : - aggregate : - get : daily trigger : true - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : config - task : credhub-interpolate << : *credhub-interpolate - task : download-opsman-image image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : download-product-configs/opsman.yml - aggregate : - put : opsman-product params : file : downloaded-product/* - name : fetch-pas plan : - aggregate : - get : daily trigger : true - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : config - task : credhub-interpolate << : *credhub-interpolate - task : download-pas-product-and-stemcell image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : download-product-configs/pas.yml output_mapping : { downloaded-stemcell : pas-stemcell } - aggregate : - put : pas-product params : file : downloaded-product/*.pivotal - put : pas-stemcell params : file : pas-stemcell/*.tgz - name : fetch-pas-windows plan : - aggregate : - get : daily trigger : true - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : config - task : credhub-interpolate << : *credhub-interpolate - task : download-pas-windows-product image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : download-product-configs/pas-windows.yml - aggregate : - put : pas-windows-product params : file : downloaded-product/*.pivotal - name : fetch-pks plan : - aggregate : - get : daily trigger : true - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : config - task : credhub-interpolate << : *credhub-interpolate - task : download-pks-product-and-stemcell image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : download-product-configs/pks.yml output_mapping : { downloaded-stemcell : pks-stemcell } - aggregate : - put : pks-product params : file : downloaded-product/*.pivotal - put : pks-stemcell params : file : pks-stemcell/*.tgz - name : fetch-platform-automation # We use the pivnet resource to bootstrap the pipeline, # and because this product is part of the pipeline, not the foundation plan : - get : platform-automation-pivnet trigger : true - aggregate : - put : platform-automation-tasks params : file : platform-automation-pivnet/*tasks*.zip - put : platform-automation-image params : file : platform-automation-pivnet/*image*.tgz","title":"Retrieving External Dependencies"},{"location":"pipeline/resources.html#prerequisites","text":"Deployed Concourse Info Platform Automation for PCF is based on Concourse CI. We recommend that you have some familiarity with Concourse before getting started. If you are new to Concourse, Concourse CI Tutorials would be a good place to start. Persisted datastore that can be accessed by Concourse resource (e.g. s3, gcs, minio) Pivnet access to Platform Automation A set of valid download-product-config files: Each product has a configuration YAML of what version to download from Pivotal Network.","title":"Prerequisites"},{"location":"pipeline/resources.html#retrieval-from-pivotal-network","text":"Ops Manager 2.5 The filename for the artifact downloaded from Ops Manager is changed! If your resources or pipelines have a regex for the Ops Manager filename, you may be affected. (Please see Ops Manager's official notice for more information) The pipeline downloads dependencies consumed by the tasks and places them into a trusted s3-like storage provider. This helps other concourse deployments without internet access retrieve task dependencies. S3 filename prefixing Note the unique regex format for blob names, for example: \\[p-healthwatch,(.*)\\]p-healthwatch-.*.pivotal . Pivnet filenames will not always contain the necessary metadata to accurately download files from S3. So, the product slug and version are prepended when using download-product . For more information on how this works, and what to expect when using download-product and download-product-s3 , refer to the download-product task reference. The pipeline requires configuration for the download-product task. Below are examples that can be used. Healthwatch 1 2 3 4 5 6 7 8 9 10 11 --- pivnet-api-token : ((pivnet-token)) pivnet-file-glob : \"*.pivotal\" pivnet-product-slug : p-healthwatch product-version : 1.4.4 stemcell-iaas : vsphere s3-access-key-id : ((s3_access_key_id)) s3-secret-access-key : ((s3_secret_access_key)) s3-bucket : ((s3_pivnet_products_bucket)) s3-region-name : ((s3_region_name)) PAS 1 2 3 4 5 6 7 8 9 10 11 --- pivnet-api-token : ((pivnet-token)) pivnet-file-glob : \"*cf*.pivotal\" pivnet-product-slug : elastic-runtime product-version-regex : ^2\\.[56]\\.0.*$ stemcell-iaas : vsphere s3-access-key-id : ((s3_access_key_id)) s3-secret-access-key : ((s3_secret_access_key)) s3-bucket : ((s3_pivnet_products_bucket)) s3-region-name : ((s3_region_name)) PAS Windows 1 2 3 4 5 6 7 8 9 10 --- pivnet-api-token : ((pivnet-token)) pivnet-file-glob : \"*.pivotal\" pivnet-product-slug : pas-windows product-version-regex : ^2\\.[56]\\.0.*$ s3-access-key-id : ((s3_access_key_id)) s3-secret-access-key : ((s3_secret_access_key)) s3-bucket : ((s3_pivnet_products_bucket)) s3-region-name : ((s3_region_name)) OpsMan 1 2 3 4 5 6 7 8 9 10 --- pivnet-api-token : ((pivnet-token)) pivnet-file-glob : \"ops-manager*.ova\" pivnet-product-slug : ops-manager product-version-regex : ^2\\.[56]\\.\\d+.*$ s3-access-key-id : ((s3_access_key_id)) s3-secret-access-key : ((s3_secret_access_key)) s3-bucket : ((s3_pivnet_products_bucket)) s3-region-name : ((s3_region_name)) PKS 1 2 3 4 5 6 7 8 9 10 11 --- pivnet-api-token : ((pivnet-token)) pivnet-file-glob : \"pivotal-container-service*.pivotal\" pivnet-product-slug : pivotal-container-service product-version-regex : ^1\\.3\\..*$ stemcell-iaas : vsphere s3-access-key-id : ((s3_access_key_id)) s3-secret-access-key : ((s3_secret_access_key)) s3-bucket : ((s3_pivnet_products_bucket)) s3-region-name : ((s3_region_name))","title":"Retrieval from Pivotal Network"},{"location":"pipeline/resources.html#pipeline-components","text":"","title":"Pipeline Components"},{"location":"pipeline/resources.html#resource-types","text":"This custom resource type uses the pivnet resource to pull down and separate both pieces of the Platform Automation product (tasks and image) so they can be stored separately in S3. 1 2 3 4 5 6 resource_types : - name : pivnet type : docker-image source : repository : pivotalcf/pivnet-resource tag : latest-final","title":"Resource Types"},{"location":"pipeline/resources.html#product-resources","text":"S3 resources where Platform Automation download-product outputs will be stored. Each product/stemcell needs a separate resource defined. Platform Automation will not create these resources for you. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 resources : - name : healthwatch-product type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : \\[p-healthwatch,(.*)\\]p-healthwatch-.*.pivotal - name : healthwatch-stemcell type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : healthwatch-stemcell/\\[stemcells-ubuntu-xenial,(.*)\\]bosh-stemcell-.*-vsphere.*\\.tgz - name : opsman-product type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : \\[ops-manager,(.*)\\].*.ova - name : pas-product type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : \\[elastic-runtime,(.*)\\]cf-.*.pivotal - name : pas-stemcell type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : pas-stemcell/\\[stemcells-ubuntu-xenial,(.*)\\]bosh-stemcell-.*-vsphere.*\\.tgz - name : pks-product type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : \\[pivotal-container-service,(.*)\\]pivotal-container-service-.*.pivotal - name : pks-stemcell type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : pks-stemcell/\\[stemcells-ubuntu-xenial,(.*)\\]bosh-stemcell-.*-vsphere.*\\.tgz - name : pas-windows-product type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : \\[pas-windows,(.*)\\]pas-windows-.*.pivotal","title":"Product Resources"},{"location":"pipeline/resources.html#platform-automation-resources","text":"platform-automation-pivnet is downloaded directly from Pivnet and will be used to download all other products from Pivnet. platform-automation-tasks and platform-automation-image are S3 resources that will be stored for internet-restricted, or faster, access. Platform Automation will not create this resource for you. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 - name : platform-automation-pivnet type : pivnet source : api_token : ((pivnet_token)) product_slug : platform-automation product_version : 2\\.(.*) sort_by : semver - name : platform-automation-tasks type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : platform-automation-tasks-(.*).zip - name : platform-automation-image type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : platform-automation-image-(.*).tgz","title":"Platform Automation Resources"},{"location":"pipeline/resources.html#configured-resources","text":"You will need to add your download-product configuration configuration files to your configurations repo. Platform Automation will not create these resources for you. For more details, see the Inputs and Outputs section. 1 2 3 4 5 6 7 8 - name : config type : git source : private_key : ((configuration.private_key)) uri : ((configuration.uri)) branch : master submodules : all depth : 1","title":"Configured Resources"},{"location":"pipeline/resources.html#trigger-resources","text":"1 2 3 4 - name : daily type : time source : interval : 24h","title":"Trigger Resources"},{"location":"pipeline/resources.html#credhub-interpolate-job","text":"((foundation)) is a value intended to be replaced by the filepath of your foundation directory structure in github (if you are not using multi-foundation, this value can be removed). ((credhub-*)) are values for accessing your Concourse Credhub. These are set when fly -ing your pipeline. For more information on how to fly your pipeline and use ((foundation)) , please reference our How To Guides for your specific workflow. Platform Automation will not create your Credhub or store values into your Credhub for you. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # This task is used in multiple jobs # The yaml anchor \"*credhub-interpolate\" is used in its place credhub-interpolate : &credhub-interpolate image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : '/pipeline/vsphere' INTERPOLATION_PATHS : \"download-product-configs\" input_mapping : files : config output_mapping : interpolated-files : config","title":"Credhub Interpolate Job"},{"location":"pipeline/resources.html#jobs","text":"Each job corresponds to a \"box\" on the visual representation of your Concourse pipeline. These jobs consume resources defined above. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 jobs : - name : fetch-healthwatch plan : - aggregate : - get : daily trigger : true - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : config - task : credhub-interpolate << : *credhub-interpolate - task : download-healthwatch-product-and-stemcell image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : download-product-configs/healthwatch.yml output_mapping : { downloaded-stemcell : healthwatch-stemcell } - aggregate : - put : healthwatch-product params : file : downloaded-product/*.pivotal - put : healthwatch-stemcell params : file : healthwatch-stemcell/*.tgz - name : fetch-opsman plan : - aggregate : - get : daily trigger : true - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : config - task : credhub-interpolate << : *credhub-interpolate - task : download-opsman-image image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : download-product-configs/opsman.yml - aggregate : - put : opsman-product params : file : downloaded-product/* - name : fetch-pas plan : - aggregate : - get : daily trigger : true - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : config - task : credhub-interpolate << : *credhub-interpolate - task : download-pas-product-and-stemcell image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : download-product-configs/pas.yml output_mapping : { downloaded-stemcell : pas-stemcell } - aggregate : - put : pas-product params : file : downloaded-product/*.pivotal - put : pas-stemcell params : file : pas-stemcell/*.tgz - name : fetch-pas-windows plan : - aggregate : - get : daily trigger : true - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : config - task : credhub-interpolate << : *credhub-interpolate - task : download-pas-windows-product image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : download-product-configs/pas-windows.yml - aggregate : - put : pas-windows-product params : file : downloaded-product/*.pivotal - name : fetch-pks plan : - aggregate : - get : daily trigger : true - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : config - task : credhub-interpolate << : *credhub-interpolate - task : download-pks-product-and-stemcell image : platform-automation-image file : platform-automation-tasks/tasks/download-product.yml params : CONFIG_FILE : download-product-configs/pks.yml output_mapping : { downloaded-stemcell : pks-stemcell } - aggregate : - put : pks-product params : file : downloaded-product/*.pivotal - put : pks-stemcell params : file : pks-stemcell/*.tgz - name : fetch-platform-automation # We use the pivnet resource to bootstrap the pipeline, # and because this product is part of the pipeline, not the foundation plan : - get : platform-automation-pivnet trigger : true - aggregate : - put : platform-automation-tasks params : file : platform-automation-pivnet/*tasks*.zip - put : platform-automation-image params : file : platform-automation-pivnet/*image*.tgz","title":"Jobs"},{"location":"pipeline/single-product.html","text":"Below you will find a reference pipeline that illustrates the tasks and provides an example of a basic pipeline design. You know your environment and constraints and we don't - we recommend you look at the tasks that make up the pipeline, and see how they can be arranged for your specific automation needs. For a deeper dive into each task see the Task Reference. These Concourse pipelines are examples on how to use the tasks . If you use a different CI/CD platform, you can use these Concourse files as examples of the inputs, outputs, and arguments used in each step in the workflow. Prerequisites Deployed Concourse Info Platform Automation for PCF is based on Concourse CI. We recommend that you have some familiarity with Concourse before getting started. If you are new to Concourse, Concourse CI Tutorials would be a good place to start. Persisted datastore that can be accessed by Concourse resource (e.g. s3, gcs, minio) A valid generating-env-file : this file will contain credentials necessary to login to Ops Manager using the om CLI. It is used by every task within Platform Automation for PCF A valid auth-file : this file will contain the credentials necessary to create the Ops Manager login the first time the VM is created. The choices for this file are simple or saml authentication. Info There will be some crossover between the auth file and the env file due to how om is setup and how the system works. It is highly recommended to parameterize these values, and let a credential management system (such as Credhub) fill in these values for you in order to maintain consistency across files. An [opsman-configuration] file: This file is required to connect to an IAAS, and control the lifecycle management of the Ops Manager VM A director-configuration file: Each Ops Manager needs its own configuration, but it is retrieved differently from a product configuration. This config is used to deploy a new Ops Manager director, or update an existing one. A set of valid product-configuration files: Each product configuration is a yaml file that contains the properties necessary to configure an Ops Manager product using the om tool. This can be used during install or update. (Optional) A working credhub setup with its own UAA client and secret. Retrieving products from Pivnet Please ensure products have been procured from Pivotal Network using the reference-resources . Installing Ops Manager and a single product The pipeline shows how to compose the tasks to install Ops Manager and the PKS product. Its dependencies are coming from a trusted git repository and S3, which can be retrieved using this pipeline . Pipeline Components S3 Resources These can either be uploaded manually or from the reference resources pipeline . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 resources : - name : platform-automation-tasks type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : .*tasks-(.*).zip - name : platform-automation-image type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : .*image-(.*).tgz - name : opsman-image type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : \\[ops-manager,(.*)\\].*.ova # vsphere ex: ops-manager-(.*).ova - name : pks-stemcell type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : pks-stemcell/\\[stemcells-ubuntu-xenial,(.*)\\]bosh-stemcell-.*-vsphere.*\\.tgz - name : pks-product type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : \\[pivotal-container-service,(.*)\\]pivotal-container-service-.*.pivotal Exported Installation Resource Warning It is recommended to persist the zip file exported from export-installation to an external file store (eg S3) on a regular basis. The exported installation can restore the Ops Manager to a working state if it is non-functional. 1 2 3 4 5 6 7 8 - name : installation type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.installation)) regexp : installation-(.*).zip Configured Resources These contain values for opsman vm creation, director, product, foundation-specific vars, auth, and env files. For more details, see the Inputs and Outputs section. Platform Automation will not create these resources for you. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # configurations - name : configuration type : git source : private_key : ((configuration.private_key)) uri : ((configuration.uri)) branch : master - name : variable type : git source : private_key : ((variable.private_key)) uri : ((variable.uri)) branch : master Trigger Resources 1 2 3 4 5 6 7 8 9 10 # triggers used to have jobs do something in a timely manner - name : one-time-trigger type : time source : interval : 999999h - name : daily-trigger type : time source : interval : 24h Credhub Interpolate Job ((foundation)) is a value intended to be replaced by the filepath of your foundation directory structure in github (if you are not using multi-foundation, this value can be removed). ((credhub-*)) are values for accessing your Concourse Credhub. These are set when fly -ing your pipeline. For more information on how to fly your pipeline and use ((foundation)) , please reference our How To Guides for your specific workflow. Platform Automation will not create your Credhub or store values into your Credhub for you. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # This task is used in multiple jobs # The yaml anchor \"*interpolate-creds\" is used in its place interpolate-creds : &interpolate-creds image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : '/pipeline/vsphere-pks' INTERPOLATION_PATHS : ((foundation))/config ((foundation))/env SKIP_MISSING : true input_mapping : files : configuration output_mapping : interpolated-files : interpolated-creds Jobs Each job corresponds to a \"box\" on the visual representation of your Concourse pipeline. These jobs consume resources defined above. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 jobs : - name : install-opsman serial : true serial_groups : [ install ] plan : - aggregate : - get : platform-automation-image params : unpack : true - get : one-time-trigger trigger : true - get : platform-automation-tasks params : unpack : true - get : configuration - task : interpolate-download-opsman-image-creds image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : '/pipeline/vsphere' INTERPOLATION_PATHS : \"download-product-configs\" SKIP_MISSING : true input_mapping : files : configuration output_mapping : interpolated-files : product-configs - task : download-opsman-image image : platform-automation-image file : platform-automation-tasks/tasks/download-product-s3.yml params : CONFIG_FILE : download-product-configs/opsman.yml input_mapping : config : product-configs output_mapping : downloaded-product : opsman-image - get : variable - task : interpolate-creds << : *interpolate-creds - task : create-vm image : platform-automation-image file : platform-automation-tasks/tasks/create-vm.yml input_mapping : image : opsman-image state : configuration config : interpolated-creds vars : variable params : VARS_FILES : vars/((foundation))/vars/opsman-vars.yml STATE_FILE : ((foundation))/state/state.yml OPSMAN_CONFIG_FILE : ((foundation))/config/opsman.yml ensure : &make-state-commit do : - task : make-commit image : platform-automation-image file : platform-automation-tasks/tasks/make-git-commit.yml input_mapping : repository : configuration file-source : generated-state output_mapping : repository-commit : configuration-commit params : FILE_SOURCE_PATH : state.yml FILE_DESTINATION_PATH : ((foundation))/state/state.yml GIT_AUTHOR_EMAIL : \"pcf-pipeline-bot@example.com\" GIT_AUTHOR_NAME : \"Platform Automation Bot\" COMMIT_MESSAGE : 'Update state file' - put : configuration params : repository : configuration-commit merge : true - task : configure-authentication image : platform-automation-image file : platform-automation-tasks/tasks/configure-authentication.yml attempts : 10 input_mapping : env : configuration config : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml AUTH_CONFIG_FILE : ((foundation))/config/auth.yml - task : configure-director image : platform-automation-image file : platform-automation-tasks/tasks/configure-director.yml input_mapping : config : interpolated-creds env : interpolated-creds vars : variable params : VARS_FILES : vars/((foundation))/vars/director-vars.yml ENV_FILE : ((foundation))/env/env.yml DIRECTOR_CONFIG_FILE : ((foundation))/config/director.yml - task : apply-director-changes image : platform-automation-image file : platform-automation-tasks/tasks/apply-director-changes.yml input_mapping : env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml - name : export-installation serial : true plan : - aggregate : - get : daily-trigger trigger : true - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : export-installation image : platform-automation-image file : platform-automation-tasks/tasks/export-installation.yml input_mapping : env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml INSTALLATION_FILE : installation-$timestamp.zip - put : installation params : file : installation/installation*.zip - name : upgrade-opsman serial : true serial_groups : [ install ] plan : - aggregate : - get : one-time-trigger passed : [ install-opsman ] - get : platform-automation-image params : unpack : true trigger : true - get : platform-automation-tasks params : unpack : true - get : opsman-image - get : installation passed : [ export-installation ] - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : upgrade-opsman image : platform-automation-image file : platform-automation-tasks/tasks/upgrade-opsman.yml input_mapping : image : opsman-image state : configuration config : interpolated-creds env : interpolated-creds vars : variable params : VARS_FILES : vars/((foundation))/vars/opsman-vars.yml ENV_FILE : ((foundation))/env/env.yml OPSMAN_CONFIG_FILE : ((foundation))/config/opsman.yml STATE_FILE : ((foundation))/state/state.yml ensure : *make-state-commit - task : apply-director-changes image : platform-automation-image file : platform-automation-tasks/tasks/apply-director-changes.yml input_mapping : env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml - name : upload-stemcells serial : true plan : - aggregate : - get : platform-automation-image params : unpack : true passed : - upgrade-opsman trigger : true - get : platform-automation-tasks params : unpack : true - get : pks-stemcell trigger : true - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : upload-pks-stemcell image : platform-automation-image file : platform-automation-tasks/tasks/upload-stemcell.yml input_mapping : env : interpolated-creds stemcell : pks-stemcell params : ENV_FILE : ((foundation))/env/env.yml - name : upload-and-stage-pks serial : true plan : - aggregate : - get : platform-automation-image params : unpack : true trigger : true passed : [ \"upgrade-opsman\" ] - get : platform-automation-tasks params : unpack : true - get : pks-product - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : upload-and-stage-product image : platform-automation-image file : platform-automation-tasks/tasks/upload-and-stage-product.yml input_mapping : product : pks-product env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml - name : configure-pks serial : true plan : - aggregate : - get : platform-automation-image params : unpack : true passed : - upload-and-stage-pks trigger : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : configure-pks image : platform-automation-image file : platform-automation-tasks/tasks/configure-product.yml input_mapping : config : interpolated-creds env : interpolated-creds vars : variable params : CONFIG_FILE : ((foundation))/config/pks.yml ENV_FILE : ((foundation))/env/env.yml - name : apply-product-changes serial : true plan : - aggregate : - get : platform-automation-image params : unpack : true passed : - configure-pks trigger : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : apply-product-changes image : platform-automation-image file : platform-automation-tasks/tasks/apply-changes.yml input_mapping : env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml - name : staged-pks-config plan : - aggregate : - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : staged-config image : platform-automation-image file : platform-automation-tasks/tasks/staged-config.yml input_mapping : env : interpolated-creds params : PRODUCT_NAME : pivotal-container-service ENV_FILE : ((foundation))/env/env.yml","title":"Ops Manager + a product"},{"location":"pipeline/single-product.html#prerequisites","text":"Deployed Concourse Info Platform Automation for PCF is based on Concourse CI. We recommend that you have some familiarity with Concourse before getting started. If you are new to Concourse, Concourse CI Tutorials would be a good place to start. Persisted datastore that can be accessed by Concourse resource (e.g. s3, gcs, minio) A valid generating-env-file : this file will contain credentials necessary to login to Ops Manager using the om CLI. It is used by every task within Platform Automation for PCF A valid auth-file : this file will contain the credentials necessary to create the Ops Manager login the first time the VM is created. The choices for this file are simple or saml authentication. Info There will be some crossover between the auth file and the env file due to how om is setup and how the system works. It is highly recommended to parameterize these values, and let a credential management system (such as Credhub) fill in these values for you in order to maintain consistency across files. An [opsman-configuration] file: This file is required to connect to an IAAS, and control the lifecycle management of the Ops Manager VM A director-configuration file: Each Ops Manager needs its own configuration, but it is retrieved differently from a product configuration. This config is used to deploy a new Ops Manager director, or update an existing one. A set of valid product-configuration files: Each product configuration is a yaml file that contains the properties necessary to configure an Ops Manager product using the om tool. This can be used during install or update. (Optional) A working credhub setup with its own UAA client and secret. Retrieving products from Pivnet Please ensure products have been procured from Pivotal Network using the reference-resources .","title":"Prerequisites"},{"location":"pipeline/single-product.html#installing-ops-manager-and-a-single-product","text":"The pipeline shows how to compose the tasks to install Ops Manager and the PKS product. Its dependencies are coming from a trusted git repository and S3, which can be retrieved using this pipeline .","title":"Installing Ops Manager and a single product"},{"location":"pipeline/single-product.html#pipeline-components","text":"","title":"Pipeline Components"},{"location":"pipeline/single-product.html#s3-resources","text":"These can either be uploaded manually or from the reference resources pipeline . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 resources : - name : platform-automation-tasks type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : .*tasks-(.*).zip - name : platform-automation-image type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : .*image-(.*).tgz - name : opsman-image type : s3 source : access_key_id : ((s3.access_key_id)) bucket : ((s3.buckets.pivnet_products)) region_name : ((s3.region_name)) secret_access_key : ((s3.secret_access_key)) regexp : \\[ops-manager,(.*)\\].*.ova # vsphere ex: ops-manager-(.*).ova - name : pks-stemcell type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : pks-stemcell/\\[stemcells-ubuntu-xenial,(.*)\\]bosh-stemcell-.*-vsphere.*\\.tgz - name : pks-product type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.pivnet_products)) regexp : \\[pivotal-container-service,(.*)\\]pivotal-container-service-.*.pivotal","title":"S3 Resources"},{"location":"pipeline/single-product.html#exported-installation-resource","text":"Warning It is recommended to persist the zip file exported from export-installation to an external file store (eg S3) on a regular basis. The exported installation can restore the Ops Manager to a working state if it is non-functional. 1 2 3 4 5 6 7 8 - name : installation type : s3 source : access_key_id : ((s3.access_key_id)) secret_access_key : ((s3.secret_access_key)) region_name : ((s3.region_name)) bucket : ((s3.buckets.installation)) regexp : installation-(.*).zip","title":"Exported Installation Resource"},{"location":"pipeline/single-product.html#configured-resources","text":"These contain values for opsman vm creation, director, product, foundation-specific vars, auth, and env files. For more details, see the Inputs and Outputs section. Platform Automation will not create these resources for you. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # configurations - name : configuration type : git source : private_key : ((configuration.private_key)) uri : ((configuration.uri)) branch : master - name : variable type : git source : private_key : ((variable.private_key)) uri : ((variable.uri)) branch : master","title":"Configured Resources"},{"location":"pipeline/single-product.html#trigger-resources","text":"1 2 3 4 5 6 7 8 9 10 # triggers used to have jobs do something in a timely manner - name : one-time-trigger type : time source : interval : 999999h - name : daily-trigger type : time source : interval : 24h","title":"Trigger Resources"},{"location":"pipeline/single-product.html#credhub-interpolate-job","text":"((foundation)) is a value intended to be replaced by the filepath of your foundation directory structure in github (if you are not using multi-foundation, this value can be removed). ((credhub-*)) are values for accessing your Concourse Credhub. These are set when fly -ing your pipeline. For more information on how to fly your pipeline and use ((foundation)) , please reference our How To Guides for your specific workflow. Platform Automation will not create your Credhub or store values into your Credhub for you. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # This task is used in multiple jobs # The yaml anchor \"*interpolate-creds\" is used in its place interpolate-creds : &interpolate-creds image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : '/pipeline/vsphere-pks' INTERPOLATION_PATHS : ((foundation))/config ((foundation))/env SKIP_MISSING : true input_mapping : files : configuration output_mapping : interpolated-files : interpolated-creds","title":"Credhub Interpolate Job"},{"location":"pipeline/single-product.html#jobs","text":"Each job corresponds to a \"box\" on the visual representation of your Concourse pipeline. These jobs consume resources defined above. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 jobs : - name : install-opsman serial : true serial_groups : [ install ] plan : - aggregate : - get : platform-automation-image params : unpack : true - get : one-time-trigger trigger : true - get : platform-automation-tasks params : unpack : true - get : configuration - task : interpolate-download-opsman-image-creds image : platform-automation-image file : platform-automation-tasks/tasks/credhub-interpolate.yml params : CREDHUB_CLIENT : ((credhub-client)) CREDHUB_SECRET : ((credhub-secret)) CREDHUB_SERVER : ((credhub-server)) PREFIX : '/pipeline/vsphere' INTERPOLATION_PATHS : \"download-product-configs\" SKIP_MISSING : true input_mapping : files : configuration output_mapping : interpolated-files : product-configs - task : download-opsman-image image : platform-automation-image file : platform-automation-tasks/tasks/download-product-s3.yml params : CONFIG_FILE : download-product-configs/opsman.yml input_mapping : config : product-configs output_mapping : downloaded-product : opsman-image - get : variable - task : interpolate-creds << : *interpolate-creds - task : create-vm image : platform-automation-image file : platform-automation-tasks/tasks/create-vm.yml input_mapping : image : opsman-image state : configuration config : interpolated-creds vars : variable params : VARS_FILES : vars/((foundation))/vars/opsman-vars.yml STATE_FILE : ((foundation))/state/state.yml OPSMAN_CONFIG_FILE : ((foundation))/config/opsman.yml ensure : &make-state-commit do : - task : make-commit image : platform-automation-image file : platform-automation-tasks/tasks/make-git-commit.yml input_mapping : repository : configuration file-source : generated-state output_mapping : repository-commit : configuration-commit params : FILE_SOURCE_PATH : state.yml FILE_DESTINATION_PATH : ((foundation))/state/state.yml GIT_AUTHOR_EMAIL : \"pcf-pipeline-bot@example.com\" GIT_AUTHOR_NAME : \"Platform Automation Bot\" COMMIT_MESSAGE : 'Update state file' - put : configuration params : repository : configuration-commit merge : true - task : configure-authentication image : platform-automation-image file : platform-automation-tasks/tasks/configure-authentication.yml attempts : 10 input_mapping : env : configuration config : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml AUTH_CONFIG_FILE : ((foundation))/config/auth.yml - task : configure-director image : platform-automation-image file : platform-automation-tasks/tasks/configure-director.yml input_mapping : config : interpolated-creds env : interpolated-creds vars : variable params : VARS_FILES : vars/((foundation))/vars/director-vars.yml ENV_FILE : ((foundation))/env/env.yml DIRECTOR_CONFIG_FILE : ((foundation))/config/director.yml - task : apply-director-changes image : platform-automation-image file : platform-automation-tasks/tasks/apply-director-changes.yml input_mapping : env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml - name : export-installation serial : true plan : - aggregate : - get : daily-trigger trigger : true - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : export-installation image : platform-automation-image file : platform-automation-tasks/tasks/export-installation.yml input_mapping : env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml INSTALLATION_FILE : installation-$timestamp.zip - put : installation params : file : installation/installation*.zip - name : upgrade-opsman serial : true serial_groups : [ install ] plan : - aggregate : - get : one-time-trigger passed : [ install-opsman ] - get : platform-automation-image params : unpack : true trigger : true - get : platform-automation-tasks params : unpack : true - get : opsman-image - get : installation passed : [ export-installation ] - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : upgrade-opsman image : platform-automation-image file : platform-automation-tasks/tasks/upgrade-opsman.yml input_mapping : image : opsman-image state : configuration config : interpolated-creds env : interpolated-creds vars : variable params : VARS_FILES : vars/((foundation))/vars/opsman-vars.yml ENV_FILE : ((foundation))/env/env.yml OPSMAN_CONFIG_FILE : ((foundation))/config/opsman.yml STATE_FILE : ((foundation))/state/state.yml ensure : *make-state-commit - task : apply-director-changes image : platform-automation-image file : platform-automation-tasks/tasks/apply-director-changes.yml input_mapping : env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml - name : upload-stemcells serial : true plan : - aggregate : - get : platform-automation-image params : unpack : true passed : - upgrade-opsman trigger : true - get : platform-automation-tasks params : unpack : true - get : pks-stemcell trigger : true - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : upload-pks-stemcell image : platform-automation-image file : platform-automation-tasks/tasks/upload-stemcell.yml input_mapping : env : interpolated-creds stemcell : pks-stemcell params : ENV_FILE : ((foundation))/env/env.yml - name : upload-and-stage-pks serial : true plan : - aggregate : - get : platform-automation-image params : unpack : true trigger : true passed : [ \"upgrade-opsman\" ] - get : platform-automation-tasks params : unpack : true - get : pks-product - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : upload-and-stage-product image : platform-automation-image file : platform-automation-tasks/tasks/upload-and-stage-product.yml input_mapping : product : pks-product env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml - name : configure-pks serial : true plan : - aggregate : - get : platform-automation-image params : unpack : true passed : - upload-and-stage-pks trigger : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : configure-pks image : platform-automation-image file : platform-automation-tasks/tasks/configure-product.yml input_mapping : config : interpolated-creds env : interpolated-creds vars : variable params : CONFIG_FILE : ((foundation))/config/pks.yml ENV_FILE : ((foundation))/env/env.yml - name : apply-product-changes serial : true plan : - aggregate : - get : platform-automation-image params : unpack : true passed : - configure-pks trigger : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : apply-product-changes image : platform-automation-image file : platform-automation-tasks/tasks/apply-changes.yml input_mapping : env : interpolated-creds params : ENV_FILE : ((foundation))/env/env.yml - name : staged-pks-config plan : - aggregate : - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - get : configuration - get : variable - task : interpolate-creds << : *interpolate-creds - task : staged-config image : platform-automation-image file : platform-automation-tasks/tasks/staged-config.yml input_mapping : env : interpolated-creds params : PRODUCT_NAME : pivotal-container-service ENV_FILE : ((foundation))/env/env.yml","title":"Jobs"},{"location":"pipeline-design/stemcell-handling.html","text":"Stemcell Handling What is Stemcell Handling? In Ops Manager, every product uploaded and staged needs to be given a stemcell in order to operate. By default, every stemcell uploaded to Ops Manager will automatically associate with any new or existing products. Using the automation tasks, this default can be overridden to not have a stemcell associate with any products, and can be manually assigned as deemed necessary by the user. Why do your Stemcell Handling Manually? Unless there is a specific need to manually handle the stemcells in Ops Manager, it is recommended to use the default. A common use case for manual stemcell handling is updating the product stemcells one at a time to minimize downtime during apply changes. This is particularly beneficial in environments with large numbers of tiles that share the same stemcell. How to use the Stemcell Handling Tasks in Automation Platform Automation has tasks that will assist in the manual handling of stemcells within Ops Manager. These tasks, in order, are: download-product upload-product stage-product upload-stemcell assign-stemcell download-product : Create a config.yml for this task using the example provided After running the task, a file named assign-stemcell.yml is outputted. The task will put a config file with two values, product and stemcell into the assign-stemcell-config output directory. This can be used with assign-stemcell to ensure the latest stemcell is used with that product. Run the upload-product and stage-product tasks to get the resources into Ops Manager. Run the upload-stemcell task. To upload the stemcell to Ops Manager without associating it with any product, the upload-stemcell task will need to be executed with the FLOATING_STEMCELL: false flag set. 1 An example of this , in a pipeline : 1 2 3 4 5 6 7 8 9 - task : upload-stemcell image : platform-automation-image file : platform-automation-tasks/tasks/upload-stemcell.yml input_mapping : env : configuration stemcell : downloaded-stemcell params : ENV_FILE : ((foundation))/env/env.yml FLOATING_STEMCELL : false Warning upload-stemcell should not be run until after the stage-product has completed. When the two tasks are run in the opposite order, the stemcell will still auto-associate with the product. Run the assign-stemcell task to associate the stemcell with the staged product. If using the download-product task before doing this within the same job, you must assign the config using the input_mapping key to assign the outputted config to the config that assign-stemcell is expecting. Upon successful completion, the stemcell specified in the config will be associated with the product specified in the config, and no other product will be associated with that stemcell. An example of this, in a pipeline: 1 2 3 4 5 6 7 8 - task : assign-stemcell image : platform-automation-image file : platform-automation-tasks/tasks/assign-stemcell.yml input_mapping : env : configuration config : assign-stemcell-config params : ENV_FILE : ((foundation))/env/env.yml Configure Product and Apply Changes can then be run on the product as normal.","title":"Stemcell Handling"},{"location":"pipeline-design/stemcell-handling.html#stemcell-handling","text":"","title":"Stemcell Handling"},{"location":"pipeline-design/stemcell-handling.html#what-is-stemcell-handling","text":"In Ops Manager, every product uploaded and staged needs to be given a stemcell in order to operate. By default, every stemcell uploaded to Ops Manager will automatically associate with any new or existing products. Using the automation tasks, this default can be overridden to not have a stemcell associate with any products, and can be manually assigned as deemed necessary by the user.","title":"What is Stemcell Handling?"},{"location":"pipeline-design/stemcell-handling.html#why-do-your-stemcell-handling-manually","text":"Unless there is a specific need to manually handle the stemcells in Ops Manager, it is recommended to use the default. A common use case for manual stemcell handling is updating the product stemcells one at a time to minimize downtime during apply changes. This is particularly beneficial in environments with large numbers of tiles that share the same stemcell.","title":"Why do your Stemcell Handling Manually?"},{"location":"pipeline-design/stemcell-handling.html#how-to-use-the-stemcell-handling-tasks-in-automation","text":"Platform Automation has tasks that will assist in the manual handling of stemcells within Ops Manager. These tasks, in order, are: download-product upload-product stage-product upload-stemcell assign-stemcell download-product : Create a config.yml for this task using the example provided After running the task, a file named assign-stemcell.yml is outputted. The task will put a config file with two values, product and stemcell into the assign-stemcell-config output directory. This can be used with assign-stemcell to ensure the latest stemcell is used with that product. Run the upload-product and stage-product tasks to get the resources into Ops Manager. Run the upload-stemcell task. To upload the stemcell to Ops Manager without associating it with any product, the upload-stemcell task will need to be executed with the FLOATING_STEMCELL: false flag set. 1 An example of this , in a pipeline : 1 2 3 4 5 6 7 8 9 - task : upload-stemcell image : platform-automation-image file : platform-automation-tasks/tasks/upload-stemcell.yml input_mapping : env : configuration stemcell : downloaded-stemcell params : ENV_FILE : ((foundation))/env/env.yml FLOATING_STEMCELL : false Warning upload-stemcell should not be run until after the stage-product has completed. When the two tasks are run in the opposite order, the stemcell will still auto-associate with the product. Run the assign-stemcell task to associate the stemcell with the staged product. If using the download-product task before doing this within the same job, you must assign the config using the input_mapping key to assign the outputted config to the config that assign-stemcell is expecting. Upon successful completion, the stemcell specified in the config will be associated with the product specified in the config, and no other product will be associated with that stemcell. An example of this, in a pipeline: 1 2 3 4 5 6 7 8 - task : assign-stemcell image : platform-automation-image file : platform-automation-tasks/tasks/assign-stemcell.yml input_mapping : env : configuration config : assign-stemcell-config params : ENV_FILE : ((foundation))/env/env.yml Configure Product and Apply Changes can then be run on the product as normal.","title":"How to use the Stemcell Handling Tasks in Automation"},{"location":"pipeline-design/variables.html","text":"Variables What are Platform Automation variables? Variables provide a way to define parameters for a YAML document. Each variable has a value and can be referenced in one or more locations. Variables are used in the Platform Automation tasks . One example usage is in configure director . Why use variables? It's typically necessary to separate passwords, certificates, S3 bucket names etc. from YAML documents for security or multi-foundation purposes. Even though the structure of a YAML document (manifest) does not change, these values are typically different. Variables require special syntax in the configuration files which need them. The resulting config file is then a parametrized template for use. Using variables In the Platform Automation task, you can choose to parametrize the specific entries in the configuration file, by using the ((parametrized-value)) syntax, and then defining the parametrized-value in a separate variable file. For example, to add two variables to a YAML document (base.yml): 1 2 s3_bucket_name : ((foundation_one_bucket)) domain_name : ((foundation_one_domain_name)) In your vars.yml file, define the parametrized values (vars.yml): 1 2 foundation_one_bucket : aws-bucket-one foundation_one_domain_name : foundation.one.domain.com To check the base.yml has the variables defined in vars.yml, you can run: om interpolate --config base.yml --vars-file vars.yml If everything works as expected, you should see the following output: 1 2 s3_bucket_name : aws-bucket-one domain_name : foundation.one.domain.com Otherwise you will receive an error message indicating missing variables: 1 could not execute \"interpolate\" : Expected to find variables : (( missing - value )) Info If you are using an additional secrets manager, such as credhub, you can add the flag --skip-missing to your om interpolate call to allow parametrized variables to still be present in your config after interpolation, to be later filled in by interpolating with your secrets manager. See the Secrets Handling page for a more detailed explanation. Why use variables if you're already using a secrets manager? Secrets Handling is a secure way to store sensitive information about your foundation, such as access keys, passwords, ssh keys, etc. The following flowchart gives an example workflow on how you might use a mix of a secrets manager and vars files across multiple foundations with a single shared base_vars_template , that can be used to generate the interpolated_vars unique to a particular foundation, and passed into the relevant tasks. A separate var_template.yml could be used for every foundation to give unique credentials to those foundations. More common shared settings could be included in the vars_file.yml . graph TD; vars_1[#vars_file.yml var_a: content]; secret_tmpl[\"#var_template.yml var_b: ((credhub_var))\"]; credhub[#Stored Creds in Credhub credhub_var: secret-var]; vars_2[\"#credhub_vars.yml var_b: secret-var\"] base[\"#base_vars_template.yml a: ((var_a)) b: ((var_b))\"]; generated[\"#interpolated_vars.yml a:content b:secret-var\"]; credhub -- Credhub Interpolate --> secret_tmpl; secret_tmpl -- Generates --> vars_2; vars_2 -- om Interpolate --> base; vars_1 -- om Interpolate --> base; base -- Generates --> generated; Alternatively, you can keep all of your vars in the same file for a foundation and mix parametrized and unparametrized values. The interpolated vars file can be used directly in any task that allows for them. The trade-off for this method is the mixed vars file would be tied to a single foundation, rather than have a single base_vars_template.yml shared across foundations. graph TD; secret_tmpl[\"#var_template.yml var_a: non-secret-var var_b: ((credhub_var))\"]; credhub[#Stored Creds in Credhub credhub_var: something]; vars_2[\"#interpolated_vars.yml var_a: non-secret-var var_b: secret-var\"] credhub -- Credhub Interpolate --> secret_tmpl; secret_tmpl -- Generates --> vars_2; Using variables in the Platform Automation Tasks Some Platform Automation tasks have an optional vars input. Using the flow described above, these files can be plugged in to the tasks. An Example Task has been provided to allow pipeline testing before installing Ops Manager and PCF. An example pipeline for this is below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 jobs : - name : test-interpolate plan : - get : <the-resource-contain-base-config-file> - get : <the-resource-contain-vars-files> - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - task : interpolate image : platform-automation-image file : platform-automation-tasks/tasks/test-interpolate.yml input_mapping : config : <the-resource-contain-base-config-file> vars : <the-resource-contain-vars-file> params : VARS_FILES : vars/vars.yml # vars/vars2.yml CONFIG_FILE : base.yml SKIP_MISSING : true # false to enable strict interpolation","title":"Variables"},{"location":"pipeline-design/variables.html#variables","text":"","title":"Variables"},{"location":"pipeline-design/variables.html#what-are-platform-automation-variables","text":"Variables provide a way to define parameters for a YAML document. Each variable has a value and can be referenced in one or more locations. Variables are used in the Platform Automation tasks . One example usage is in configure director .","title":"What are Platform Automation variables?"},{"location":"pipeline-design/variables.html#why-use-variables","text":"It's typically necessary to separate passwords, certificates, S3 bucket names etc. from YAML documents for security or multi-foundation purposes. Even though the structure of a YAML document (manifest) does not change, these values are typically different. Variables require special syntax in the configuration files which need them. The resulting config file is then a parametrized template for use.","title":"Why use variables?"},{"location":"pipeline-design/variables.html#using-variables","text":"In the Platform Automation task, you can choose to parametrize the specific entries in the configuration file, by using the ((parametrized-value)) syntax, and then defining the parametrized-value in a separate variable file. For example, to add two variables to a YAML document (base.yml): 1 2 s3_bucket_name : ((foundation_one_bucket)) domain_name : ((foundation_one_domain_name)) In your vars.yml file, define the parametrized values (vars.yml): 1 2 foundation_one_bucket : aws-bucket-one foundation_one_domain_name : foundation.one.domain.com To check the base.yml has the variables defined in vars.yml, you can run: om interpolate --config base.yml --vars-file vars.yml If everything works as expected, you should see the following output: 1 2 s3_bucket_name : aws-bucket-one domain_name : foundation.one.domain.com Otherwise you will receive an error message indicating missing variables: 1 could not execute \"interpolate\" : Expected to find variables : (( missing - value )) Info If you are using an additional secrets manager, such as credhub, you can add the flag --skip-missing to your om interpolate call to allow parametrized variables to still be present in your config after interpolation, to be later filled in by interpolating with your secrets manager. See the Secrets Handling page for a more detailed explanation.","title":"Using variables"},{"location":"pipeline-design/variables.html#why-use-variables-if-youre-already-using-a-secrets-manager","text":"Secrets Handling is a secure way to store sensitive information about your foundation, such as access keys, passwords, ssh keys, etc. The following flowchart gives an example workflow on how you might use a mix of a secrets manager and vars files across multiple foundations with a single shared base_vars_template , that can be used to generate the interpolated_vars unique to a particular foundation, and passed into the relevant tasks. A separate var_template.yml could be used for every foundation to give unique credentials to those foundations. More common shared settings could be included in the vars_file.yml . graph TD; vars_1[#vars_file.yml var_a: content]; secret_tmpl[\"#var_template.yml var_b: ((credhub_var))\"]; credhub[#Stored Creds in Credhub credhub_var: secret-var]; vars_2[\"#credhub_vars.yml var_b: secret-var\"] base[\"#base_vars_template.yml a: ((var_a)) b: ((var_b))\"]; generated[\"#interpolated_vars.yml a:content b:secret-var\"]; credhub -- Credhub Interpolate --> secret_tmpl; secret_tmpl -- Generates --> vars_2; vars_2 -- om Interpolate --> base; vars_1 -- om Interpolate --> base; base -- Generates --> generated; Alternatively, you can keep all of your vars in the same file for a foundation and mix parametrized and unparametrized values. The interpolated vars file can be used directly in any task that allows for them. The trade-off for this method is the mixed vars file would be tied to a single foundation, rather than have a single base_vars_template.yml shared across foundations. graph TD; secret_tmpl[\"#var_template.yml var_a: non-secret-var var_b: ((credhub_var))\"]; credhub[#Stored Creds in Credhub credhub_var: something]; vars_2[\"#interpolated_vars.yml var_a: non-secret-var var_b: secret-var\"] credhub -- Credhub Interpolate --> secret_tmpl; secret_tmpl -- Generates --> vars_2;","title":"Why use variables if you're already using a secrets manager?"},{"location":"pipeline-design/variables.html#using-variables-in-the-platform-automation-tasks","text":"Some Platform Automation tasks have an optional vars input. Using the flow described above, these files can be plugged in to the tasks. An Example Task has been provided to allow pipeline testing before installing Ops Manager and PCF. An example pipeline for this is below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 jobs : - name : test-interpolate plan : - get : <the-resource-contain-base-config-file> - get : <the-resource-contain-vars-files> - get : platform-automation-image params : unpack : true - get : platform-automation-tasks params : unpack : true - task : interpolate image : platform-automation-image file : platform-automation-tasks/tasks/test-interpolate.yml input_mapping : config : <the-resource-contain-base-config-file> vars : <the-resource-contain-vars-file> params : VARS_FILES : vars/vars.yml # vars/vars2.yml CONFIG_FILE : base.yml SKIP_MISSING : true # false to enable strict interpolation","title":"Using variables in the Platform Automation Tasks"},{"location":"reference/inputs-outputs.html","text":"Inputs These are the inputs that can be provided to the tasks. Each task can only take a specific set, indicated under the inputs property of the YAML. env The env input for a task expects to have a env.yml file. This file contains properties for targeting and logging into the Ops Manager API. basic authentication 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- target : https://pcf.example.com connect-timeout : 30 # default 5 request-timeout : 1800 # default 1800 skip-ssl-validation : false # default false username : username password : password # decryption-passphrase is optional, # except for use with `import-installation`. # OpsMan depends on the passphrase # to decrypt the imported installation. # For other commands, providing this key allows # decryption of the OpsMan VM after reboot, # which would otherwise need to be done manually. decryption-passphrase : passphrase uaa authentication 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- target : https://pcf.example.com connect-timeout : 30 # default 5 request-timeout : 1800 # default 1800 skip-ssl-validation : false # default false client-id : client_id client-secret : client_secret # decryption-passphrase is optional, # except for use with `import-installation`. # OpsMan depends on the passphrase # to decrypt the imported installation. # For other commands, providing this key allows # decryption of the OpsMan VM after reboot, # which would otherwise need to be done manually. decryption-passphrase : passphrase Getting the client-id and client-secret Ops Manager will by preference use Client ID and Client Secret if provided. To create a Client ID and Client Secret uaac target https://YOUR_OPSMANAGER/uaa uaac token sso get if using SAML or uaac token owner get if using basic auth. Specify the Client ID as opsman and leave Client Secret blank. Generate a client ID and secret 1 2 3 4 5 6 7 8 9 10 11 12 uaac client add -i Client ID: NEW_CLIENT_NAME New client secret: DESIRED_PASSWORD Verify new client secret: DESIRED_PASSWORD scope ( list ) : opsman.admin authorized grant types ( list ) : client_credentials authorities ( list ) : opsman.admin access token validity ( seconds ) : 43200 refresh token validity ( seconds ) : 43200 redirect uri ( list ) : autoapprove ( list ) : signup redirect url ( url ) : auth There are two different authentication methods that Ops Manager supports. basic authentication The configuration for authentication has a dependency on username/password. This configuration format matches the configuration for setting up authentication. See the task for the configure-authentication for details. 1 2 3 4 --- username : username password : password decryption-passphrase : decryption-passphrase Info basic authentication supports both basic env and uaa env formats saml authentication The configuration for authentication has a dependency on SAML. This configuration format matches the configuration for setting up authentication. See the task for the configure-saml-authentication for details. 1 2 3 4 5 6 --- decryption-passphrase : decryption-passphrase saml-idp-metadata : https://saml.example.com:8080 saml-bosh-idp-metadata : https://bosh-saml.example.com:8080 saml-rbac-admin-group : opsman.full_control saml-rbac-groups-attribute : myenterprise Info saml authentication requires the uaa env format The saml-configuration properties configures the SAML provider. The Ops Manager API has more information about the values Ops Manager config The config for an Ops Manager described IAAS specific information for creating the VM -- i.e. VM flavor (size), IP addresses The config input for opsman task expects to have a opsman.yml file. The configuration of the opsman.yml is IAAS specific. Specific examples for each IaaS are as follows: AWS These required properties are adapted from the instructions outlined in Launching an Ops Manager Director Instance on AWS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 --- opsman-configuration : aws : region : us-west-2 vm_name : ops-manager-vm # defaults Ops Manager-vm boot_disk_size : 100 # default 200 vpc_subnet_id : subnet-0292bc845215c2cbf security_group_id : sg-0354f804ba7c4bc41 key_pair_name : ops-manager-key iam_instance_profile_name : ops-manager-iam instance_type : m5.large # At least one IP address (public or private) # needs to be assigned to the VM. # It is also permissable to assign both. public_ip : 1.2.3.4 private_ip : 10.0.0.2 # Required if use_instance_profile is false # omit if using Instance Profiles access_key_id : sample-access-id secret_access_key : sample-secret-access-key # If using Instance Profiles (omit if using AWS Credentials) use_instance_profile : true # default false # Optional, necessary if a role is needed to authorize the instance profile assume_role : arn:aws:iam::123456789:role/test Info At least one IP address (public or private) must be assigned to the Ops Manager VM. Both can be assigned, too. Azure These required properties are adapted from the instructions outlined in Launching an Ops Manager Director Instance on Azure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 --- opsman-configuration : azure : subscription_id : 90f35f10-ea9e-4e80-aac4-d6778b995532 resource_group : res-group tenant_id : 3e52862f-a01e-4b97-98d5-f31a409df682 client_id : 5782deb6-9195-4827-83ae-a13fda90aa0d client_secret : 6Iaue71Lqxfq location : westus container : opsmanagerimage # container for opsman image network_security_group : ops-manager-security-group # Note that there are several environment-specific details in this path vpc_subnet : /subscriptions/<MY_SUBSCRIPTION_ID>/resourceGroups/<MY_RESOURCE_GROUP>/providers/Microsoft.Network/virtualNetworks/<MY_VNET>/subnets/<MY_SUBNET> storage_account : opsman # account name of container # Optional # only needed if your client doesn't have the needed storage permissions storage_key : pEuXDaDK/WWo... ssh_public_key : ssh-rsa AAAAB3NzaC1yc2EAZ... # ssh key to access VM vm_name : ops-manager-vm # default : Ops Manager-vm boot_disk_size : 100 # default : 200 cloud_name : AzureCloud # default : AzureCloud # This flag is only respected by the create-vm & upgrade-opsman commands # set to true if you want to create the new opsman vm with unmanaged disk # delete-vm discovers the disk type from the VM use_unmanaged_disk : false # At least one IP address (public or private) # needs to be assigned to the VM. # It is also permissable to assign both. public_ip : 1.2.3.4 private_ip : 10.0.0.3 Info At least one IP address (public or private) must be assigned to the Ops Manager VM. Both can be assigned, too. GCP These required properties are adapted from the instructions outlined in Launching an Ops Manager Director Instance on GCP 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 --- opsman-configuration : gcp : gcp_service_account : | { \"type\": \"service_account\", \"project_id\": \"project-id\", \"private_key_id\": \"af719b1ca48f7b6ac67ca9c5319cb175\", \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n...\\n-----END PRIVATE KEY-----\\n\", \"client_email\": \"user@project-id.iam.gserviceaccount.com\", \"client_id\": \"1234567890\", \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\", \"token_uri\": \"https://accounts.google.com/o/oauth2/token\", \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\", \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/user%40project-id.iam.gserviceaccount.com\" } project : project-id region : us-central1 zone : us-central1-b vm_name : ops-manager-vm # default : Ops Manager-vm # For SharedVPC: projects/[HOST_PROJECT_ID]/regions/[REGION]/subnetworks/[SUBNET] vpc_subnet : infrastructure-subnet tags : ops-manager # This CPU, Memory and disk size demonstrated here # match the defaults, and needn't be included if these are the desired values custom_cpu : 2 custom_memory : 8 boot_disk_size : 100 # At least one IP address (public or private) needs to be assigned to the VM. public_ip : 1.2.3.4. private_ip : 10.0.0.2 Info At least one IP address (public or private) must be assigned to the Ops Manager VM. Both can be assigned, too. Support for Shared VPC is done via configuring the vpc_subnet path to include the host project id, region of the subnet, and the subnet name. For example: projects/[HOST_PROJECT_ID]/regions/[REGION]/subnetworks/[SUBNET] Openstack These required properties are adapted from the instructions outlined in Launching an Ops Manager Director Instance on Openstack 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 --- opsman-configuration : openstack : auth_url : http://os.example.com:5000/v2.0 project_name : project net_id : 26a13112-b6c2-11e8-96f8-529269fb1459 username : admin password : password key_pair_name : opsman-keypair security_group_name : opsman-sec-group vm_name : ops-manager-vm # default : Ops Manager-vm # At least one IP address (public or private) needs to be assigned to the VM. public_ip : 1.2.3.4 private_ip : 10.0.0.3 flavor : m1.xlarge # default : m1.xlarge project_domain_name : default user_domain_name : default identity_api_version : 3 # default : 2 insecure : true # default : false availability_zone : zone-01 Info At least one IP address (public or private) must be assigned to the Ops Manager VM. Both can be assigned, too. vSphere These required properties are adapted from the instructions outlined in Deploying BOSH and Ops Manager to vSphere 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 --- opsman-configuration : vsphere : vcenter : url : vcenter.example.com username : admin password : password datastore : exmple-ds-1 ca_cert : certificate host : example-host # vCenter host to deploy Ops Manager in datacenter : example-dc resource_pool : /example-dc/host/example-host/Resources/ResPool # or /<Data Center Name>/host/<Cluster Name> folder : /example-dc/vm/Folder insecure : 1 # default : 0 (secure); 1 (insecure) disk_type : thin # example : thin|thick private_ip : 10.0.0.2 dns : 8.8.8.8 ntp : ntp.example.com # example : ntp.ubuntu.com ssh_password : password ssh_public_key : ssh-rsa ...... # for Ops Manager >= 2.3 hostname : pcf.example.com network : virtual-network # vcenter network to deploy to netmask : 255.255.255.192 gateway : 192.168.10.1 vm_name : Ops_Manager # default : Ops_Manager memory : 8 # default : 8 GB cpu : 1 # default : 1 director config The config director will set the bosh tile (director) on Ops Manager. The config input for a director task expects to have a director.yml file. The configuration of the director.yml is IAAS specific for some properties -- i.e. networking. There are two ways to build a director config. Using an already deployed Ops Manager, you can extract the config using staged-director-config . Deploying a brand new Ops Manager requires more effort for a director.yml . The configuration of director is variables based on the features enabled. For brevity, this director.yml is a basic example for vsphere. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 --- az-configuration : - clusters : - cluster : cluster-name resource_pool : resource-pool-name name : AZ01 properties-configuration : iaas_configuration : vcenter_host : vcenter.example.com vcenter_password : admin vcenter_username : password ...... director_configuration : blobstore_type : local bosh_recreate_on_next_deploy : false custom_ssh_banner : null ...... security_configuration : generate_vm_passwords : true trusted_certificates : syslog_configuration : enabled : false network-assignment : network : name : INFRASTRUCTURE other_availability_zones : [] singleton_availability_zone : name : AZ01 networks-configuration : icmp_checks_enabled : false networks : - name : NETWORK-NAME ...... resource-configuration : compilation : instance_type : id : automatic instances : automatic ...... The IAAS specific configuration can be found in the Ops Manager API documentation. Included below is a list of properties that can be set in the director.yml and a link to the API documentation explaining any IAAS specific properties. az-configuration - a list of availability zones Ops Manager API network-assignment - the network the bosh director is deployed to Ops Manager API networks-configuration - a list of named networks Ops Manager API properties-configuration iaas_configuration - configuration for the bosh IAAS CPI Ops Manager API director_configuration - properties for the bosh director Ops Manager API security_configuration - security properties for the bosh director Ops Manager API syslog_configuration - configure the syslog sinks for the bosh director Ops Manager API resource-configuration - IAAS VM flavor for the bosh director Ops Manager API vmextensions-configuration - create/update/delete vm extensions Ops Manager API GCP Shared VPC Support for Shared VPC is done via configuring the iaas_identifier path for the infrastructure subnet , which includes the host project id, region of the subnet, and the subnet name. For example: [HOST_PROJECT_ID]/[NETWORK]/[SUBNET]/[REGION] product config There are two ways to build a product config. Using an already deployed product (tile), you can extract the config using staged-config . Use an example and fill in the values based on the meta information from the tile. For brevity, this product.yml is a basic example for healthwatch . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 --- product-properties : .healthwatch-forwarder.bosh_taskcheck_username : value : admin .healthwatch-forwarder.boshhealth_instance_count : value : 1 .healthwatch-forwarder.boshtasks_instance_count : value : 2 .healthwatch-forwarder.canary_instance_count : value : 2 .healthwatch-forwarder.cli_instance_count : value : 2 .healthwatch-forwarder.health_check_az : value : AZ01 .healthwatch-forwarder.ingestor_instance_count : value : 4 .healthwatch-forwarder.opsman_instance_count : value : 2 .healthwatch-forwarder.publish_to_eva : value : true .healthwatch-forwarder.worker_instance_count : value : 4 .mysql.skip_name_resolve : value : true .properties.opsman : value : enable .properties.opsman.enable.url : value : https://pcf.example.com/ network-properties : network : name : DEPLOYMENT other_availability_zones : - name : AZ01 - name : AZ02 service_network : name : SERVICES singleton_availability_zone : name : AZ01 resource-config : healthwatch-forwarder : instances : automatic persistent_disk : size_mb : automatic instance_type : id : automatic migrate-v1.1-v1.2 : instances : automatic instance_type : id : automatic mysql : instances : automatic persistent_disk : size_mb : automatic instance_type : id : automatic redis : instances : automatic persistent_disk : size_mb : automatic instance_type : id : automatic Included below is a list of properties that can be set in the product.yml and a link to the API documentation explaining the properties. product-properties - properties for the tile Ops Manager API network-properties - a list of named networks to deploy the VMs to Ops Manager API resource-config - for the jobs of the tile Ops Manager API state This file contains that meta-information needed to manage the Ops Manager VM. The state input for a opsman VM task expects to have a state.yml file. 1 2 3 --- iaas : gcp vm_id : ops-manager-vm The file contains two properties: iaas is the iaas the ops manager vm is hosted on. ( gcp , vsphere , aws , azure , openstack ) vm_id is the VM unique identifier for the VM. For some IAAS, the vm ID is the VM name. opsman image This file is an artifact from Pivnet , which contains the VM image on an IAAS. For vsphere and openstack, it is a full disk image. For AWS, GCP, and Azure, it is the YAML file of the image locations. An example on how to pull the AWS image resource using the Pivnet Concourse Resource . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 resource_types : - name : pivnet type : docker-image source : repository : pivotalcf/pivnet-resource tag : latest-final resources : - name : opsman-image type : pivnet source : api_token : ((pivnet_token)) product_slug : ops-manager product_version : 2.* sort_by : semver jobs : - name : get-the-resource plan : - get : opsman-image params : globs : [ \"*AWS*.yml\" ] installation The file contains the information to restore an Ops Manager VM. The installation input for a opsman VM task expects to have a installation.zip file. This file can be exported from an Ops Manager VM using the export-installation . This file can be imported to an Ops Manager VM using the import-installation . Warning This file cannot be manually created. It is a file that must be generated via the export function of Ops Manager. stemcell This stemcell input requires the stemcell tarball ( .tgz ) as downloaded from Pivnet. It must be in the original filename as that is used by Ops Manager to parse metadata. The filename could look like bosh-stemcell-3541.48-vsphere-esxi-ubuntu-trusty-go_agent.tgz . Warning This file cannot be manually created. It is a file that must retrieved from Pivnet. An example on how to pull the vSphere stemcell using the Pivnet Concourse Resource . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 resource_types : - name : pivnet type : docker-image source : repository : pivotalcf/pivnet-resource tag : latest-final resources : - name : stemcell type : pivnet source : api_token : ((pivnet_token)) product_slug : stemcells product_version : 3541.* sort_by : semver jobs : - name : get-the-resource plan : - get : stemcell params : globs : [ \"*vsphere*.tgz\" ] product The product input requires a single tile file ( .pivotal ) as downloaded from Pivnet. An example on how to pull the PAS tile using the Pivnet Concourse Resource . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 resource_types : - name : pivnet type : docker-image source : repository : pivotalcf/pivnet-resource tag : latest-final resources : - name : stemcell type : pivnet source : api_token : ((pivnet_token)) product_slug : elastic-runtime product_version : 2.* sort_by : semver jobs : - name : get-the-resource plan : - get : product params : globs : [ \"*cf*.pivotal\" ] Warning This file cannot be manually created. It is a file that must retrieved from Pivnet. download-product-config The config input for a download product task expects to have a download-config.yml file. The configuration of the download-config.yml looks like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 --- pivnet-api-token : token ## Note that file globs must be quoted if they start with *; ## otherwise they'll be interpreted as a YAML anchor. pivnet-file-glob : \"*.pivotal\" pivnet-product-slug : product-slugs ## defaults to false and should be excluded if not set to true # pivnet-disable-ssl: true ## Either product-version OR product-version-regex is required # product-version: 1.2.3 ## Note that the regex mustn't be quoted, ## as escape characters for the regex will confuse yaml parsers. # product-version-regex: ^1\\.2\\..*$ stemcell-iaas : google ## The following are required only if using download-product-s3. ## Any key marked required above is still required when using S3. ## If s3-bucket is set, ## downloaded product files will have their slug and version prepended. s3-bucket : s3-bucket s3-region-name : us-west-1 # required; sufficient for AWS s3-endpoint : s3.endpoint.com # if not using AWS, this is required ## Required unless `s3-auth-method` is `iam` s3-access-key-id : aws-or-minio-key-id s3-secret-access-key : aws-or-minio-secret-key ## Optional paths for both the product and the associated stemcell ## defaults to the root path of the specified bucket # s3-product-path: /path/to/product # s3-stemcell-path: /path/to/stemcell ## defaults to false and should be excluded if not set to true # s3-disable-ssl: true ## defaults to false; ## made available only because sometimes necessary for compatibility # s3-enable-v2-signing: true ## defaults to accesskey; ## allows use of AWS instance IAM creds, if available # s3-auth-method: iam","title":"Inputs/Outputs"},{"location":"reference/inputs-outputs.html#inputs","text":"These are the inputs that can be provided to the tasks. Each task can only take a specific set, indicated under the inputs property of the YAML.","title":"Inputs"},{"location":"reference/inputs-outputs.html#env","text":"The env input for a task expects to have a env.yml file. This file contains properties for targeting and logging into the Ops Manager API.","title":"env"},{"location":"reference/inputs-outputs.html#basic-authentication","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- target : https://pcf.example.com connect-timeout : 30 # default 5 request-timeout : 1800 # default 1800 skip-ssl-validation : false # default false username : username password : password # decryption-passphrase is optional, # except for use with `import-installation`. # OpsMan depends on the passphrase # to decrypt the imported installation. # For other commands, providing this key allows # decryption of the OpsMan VM after reboot, # which would otherwise need to be done manually. decryption-passphrase : passphrase","title":"basic authentication"},{"location":"reference/inputs-outputs.html#uaa-authentication","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- target : https://pcf.example.com connect-timeout : 30 # default 5 request-timeout : 1800 # default 1800 skip-ssl-validation : false # default false client-id : client_id client-secret : client_secret # decryption-passphrase is optional, # except for use with `import-installation`. # OpsMan depends on the passphrase # to decrypt the imported installation. # For other commands, providing this key allows # decryption of the OpsMan VM after reboot, # which would otherwise need to be done manually. decryption-passphrase : passphrase","title":"uaa authentication"},{"location":"reference/inputs-outputs.html#getting-the-client-id-and-client-secret","text":"Ops Manager will by preference use Client ID and Client Secret if provided. To create a Client ID and Client Secret uaac target https://YOUR_OPSMANAGER/uaa uaac token sso get if using SAML or uaac token owner get if using basic auth. Specify the Client ID as opsman and leave Client Secret blank. Generate a client ID and secret 1 2 3 4 5 6 7 8 9 10 11 12 uaac client add -i Client ID: NEW_CLIENT_NAME New client secret: DESIRED_PASSWORD Verify new client secret: DESIRED_PASSWORD scope ( list ) : opsman.admin authorized grant types ( list ) : client_credentials authorities ( list ) : opsman.admin access token validity ( seconds ) : 43200 refresh token validity ( seconds ) : 43200 redirect uri ( list ) : autoapprove ( list ) : signup redirect url ( url ) :","title":"Getting the client-id and client-secret"},{"location":"reference/inputs-outputs.html#auth","text":"There are two different authentication methods that Ops Manager supports.","title":"auth"},{"location":"reference/inputs-outputs.html#basic-authentication_1","text":"The configuration for authentication has a dependency on username/password. This configuration format matches the configuration for setting up authentication. See the task for the configure-authentication for details. 1 2 3 4 --- username : username password : password decryption-passphrase : decryption-passphrase Info basic authentication supports both basic env and uaa env formats","title":"basic authentication"},{"location":"reference/inputs-outputs.html#saml-authentication","text":"The configuration for authentication has a dependency on SAML. This configuration format matches the configuration for setting up authentication. See the task for the configure-saml-authentication for details. 1 2 3 4 5 6 --- decryption-passphrase : decryption-passphrase saml-idp-metadata : https://saml.example.com:8080 saml-bosh-idp-metadata : https://bosh-saml.example.com:8080 saml-rbac-admin-group : opsman.full_control saml-rbac-groups-attribute : myenterprise Info saml authentication requires the uaa env format The saml-configuration properties configures the SAML provider. The Ops Manager API has more information about the values","title":"saml authentication"},{"location":"reference/inputs-outputs.html#ops-manager-config","text":"The config for an Ops Manager described IAAS specific information for creating the VM -- i.e. VM flavor (size), IP addresses The config input for opsman task expects to have a opsman.yml file. The configuration of the opsman.yml is IAAS specific. Specific examples for each IaaS are as follows:","title":"Ops Manager config"},{"location":"reference/inputs-outputs.html#aws","text":"These required properties are adapted from the instructions outlined in Launching an Ops Manager Director Instance on AWS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 --- opsman-configuration : aws : region : us-west-2 vm_name : ops-manager-vm # defaults Ops Manager-vm boot_disk_size : 100 # default 200 vpc_subnet_id : subnet-0292bc845215c2cbf security_group_id : sg-0354f804ba7c4bc41 key_pair_name : ops-manager-key iam_instance_profile_name : ops-manager-iam instance_type : m5.large # At least one IP address (public or private) # needs to be assigned to the VM. # It is also permissable to assign both. public_ip : 1.2.3.4 private_ip : 10.0.0.2 # Required if use_instance_profile is false # omit if using Instance Profiles access_key_id : sample-access-id secret_access_key : sample-secret-access-key # If using Instance Profiles (omit if using AWS Credentials) use_instance_profile : true # default false # Optional, necessary if a role is needed to authorize the instance profile assume_role : arn:aws:iam::123456789:role/test Info At least one IP address (public or private) must be assigned to the Ops Manager VM. Both can be assigned, too.","title":"AWS"},{"location":"reference/inputs-outputs.html#azure","text":"These required properties are adapted from the instructions outlined in Launching an Ops Manager Director Instance on Azure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 --- opsman-configuration : azure : subscription_id : 90f35f10-ea9e-4e80-aac4-d6778b995532 resource_group : res-group tenant_id : 3e52862f-a01e-4b97-98d5-f31a409df682 client_id : 5782deb6-9195-4827-83ae-a13fda90aa0d client_secret : 6Iaue71Lqxfq location : westus container : opsmanagerimage # container for opsman image network_security_group : ops-manager-security-group # Note that there are several environment-specific details in this path vpc_subnet : /subscriptions/<MY_SUBSCRIPTION_ID>/resourceGroups/<MY_RESOURCE_GROUP>/providers/Microsoft.Network/virtualNetworks/<MY_VNET>/subnets/<MY_SUBNET> storage_account : opsman # account name of container # Optional # only needed if your client doesn't have the needed storage permissions storage_key : pEuXDaDK/WWo... ssh_public_key : ssh-rsa AAAAB3NzaC1yc2EAZ... # ssh key to access VM vm_name : ops-manager-vm # default : Ops Manager-vm boot_disk_size : 100 # default : 200 cloud_name : AzureCloud # default : AzureCloud # This flag is only respected by the create-vm & upgrade-opsman commands # set to true if you want to create the new opsman vm with unmanaged disk # delete-vm discovers the disk type from the VM use_unmanaged_disk : false # At least one IP address (public or private) # needs to be assigned to the VM. # It is also permissable to assign both. public_ip : 1.2.3.4 private_ip : 10.0.0.3 Info At least one IP address (public or private) must be assigned to the Ops Manager VM. Both can be assigned, too.","title":"Azure"},{"location":"reference/inputs-outputs.html#gcp","text":"These required properties are adapted from the instructions outlined in Launching an Ops Manager Director Instance on GCP 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 --- opsman-configuration : gcp : gcp_service_account : | { \"type\": \"service_account\", \"project_id\": \"project-id\", \"private_key_id\": \"af719b1ca48f7b6ac67ca9c5319cb175\", \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n...\\n-----END PRIVATE KEY-----\\n\", \"client_email\": \"user@project-id.iam.gserviceaccount.com\", \"client_id\": \"1234567890\", \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\", \"token_uri\": \"https://accounts.google.com/o/oauth2/token\", \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\", \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/user%40project-id.iam.gserviceaccount.com\" } project : project-id region : us-central1 zone : us-central1-b vm_name : ops-manager-vm # default : Ops Manager-vm # For SharedVPC: projects/[HOST_PROJECT_ID]/regions/[REGION]/subnetworks/[SUBNET] vpc_subnet : infrastructure-subnet tags : ops-manager # This CPU, Memory and disk size demonstrated here # match the defaults, and needn't be included if these are the desired values custom_cpu : 2 custom_memory : 8 boot_disk_size : 100 # At least one IP address (public or private) needs to be assigned to the VM. public_ip : 1.2.3.4. private_ip : 10.0.0.2 Info At least one IP address (public or private) must be assigned to the Ops Manager VM. Both can be assigned, too. Support for Shared VPC is done via configuring the vpc_subnet path to include the host project id, region of the subnet, and the subnet name. For example: projects/[HOST_PROJECT_ID]/regions/[REGION]/subnetworks/[SUBNET]","title":"GCP"},{"location":"reference/inputs-outputs.html#openstack","text":"These required properties are adapted from the instructions outlined in Launching an Ops Manager Director Instance on Openstack 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 --- opsman-configuration : openstack : auth_url : http://os.example.com:5000/v2.0 project_name : project net_id : 26a13112-b6c2-11e8-96f8-529269fb1459 username : admin password : password key_pair_name : opsman-keypair security_group_name : opsman-sec-group vm_name : ops-manager-vm # default : Ops Manager-vm # At least one IP address (public or private) needs to be assigned to the VM. public_ip : 1.2.3.4 private_ip : 10.0.0.3 flavor : m1.xlarge # default : m1.xlarge project_domain_name : default user_domain_name : default identity_api_version : 3 # default : 2 insecure : true # default : false availability_zone : zone-01 Info At least one IP address (public or private) must be assigned to the Ops Manager VM. Both can be assigned, too.","title":"Openstack"},{"location":"reference/inputs-outputs.html#vsphere","text":"These required properties are adapted from the instructions outlined in Deploying BOSH and Ops Manager to vSphere 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 --- opsman-configuration : vsphere : vcenter : url : vcenter.example.com username : admin password : password datastore : exmple-ds-1 ca_cert : certificate host : example-host # vCenter host to deploy Ops Manager in datacenter : example-dc resource_pool : /example-dc/host/example-host/Resources/ResPool # or /<Data Center Name>/host/<Cluster Name> folder : /example-dc/vm/Folder insecure : 1 # default : 0 (secure); 1 (insecure) disk_type : thin # example : thin|thick private_ip : 10.0.0.2 dns : 8.8.8.8 ntp : ntp.example.com # example : ntp.ubuntu.com ssh_password : password ssh_public_key : ssh-rsa ...... # for Ops Manager >= 2.3 hostname : pcf.example.com network : virtual-network # vcenter network to deploy to netmask : 255.255.255.192 gateway : 192.168.10.1 vm_name : Ops_Manager # default : Ops_Manager memory : 8 # default : 8 GB cpu : 1 # default : 1","title":"vSphere"},{"location":"reference/inputs-outputs.html#director-config","text":"The config director will set the bosh tile (director) on Ops Manager. The config input for a director task expects to have a director.yml file. The configuration of the director.yml is IAAS specific for some properties -- i.e. networking. There are two ways to build a director config. Using an already deployed Ops Manager, you can extract the config using staged-director-config . Deploying a brand new Ops Manager requires more effort for a director.yml . The configuration of director is variables based on the features enabled. For brevity, this director.yml is a basic example for vsphere. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 --- az-configuration : - clusters : - cluster : cluster-name resource_pool : resource-pool-name name : AZ01 properties-configuration : iaas_configuration : vcenter_host : vcenter.example.com vcenter_password : admin vcenter_username : password ...... director_configuration : blobstore_type : local bosh_recreate_on_next_deploy : false custom_ssh_banner : null ...... security_configuration : generate_vm_passwords : true trusted_certificates : syslog_configuration : enabled : false network-assignment : network : name : INFRASTRUCTURE other_availability_zones : [] singleton_availability_zone : name : AZ01 networks-configuration : icmp_checks_enabled : false networks : - name : NETWORK-NAME ...... resource-configuration : compilation : instance_type : id : automatic instances : automatic ...... The IAAS specific configuration can be found in the Ops Manager API documentation. Included below is a list of properties that can be set in the director.yml and a link to the API documentation explaining any IAAS specific properties. az-configuration - a list of availability zones Ops Manager API network-assignment - the network the bosh director is deployed to Ops Manager API networks-configuration - a list of named networks Ops Manager API properties-configuration iaas_configuration - configuration for the bosh IAAS CPI Ops Manager API director_configuration - properties for the bosh director Ops Manager API security_configuration - security properties for the bosh director Ops Manager API syslog_configuration - configure the syslog sinks for the bosh director Ops Manager API resource-configuration - IAAS VM flavor for the bosh director Ops Manager API vmextensions-configuration - create/update/delete vm extensions Ops Manager API","title":"director config"},{"location":"reference/inputs-outputs.html#gcp-shared-vpc","text":"Support for Shared VPC is done via configuring the iaas_identifier path for the infrastructure subnet , which includes the host project id, region of the subnet, and the subnet name. For example: [HOST_PROJECT_ID]/[NETWORK]/[SUBNET]/[REGION]","title":"GCP Shared VPC"},{"location":"reference/inputs-outputs.html#product-config","text":"There are two ways to build a product config. Using an already deployed product (tile), you can extract the config using staged-config . Use an example and fill in the values based on the meta information from the tile. For brevity, this product.yml is a basic example for healthwatch . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 --- product-properties : .healthwatch-forwarder.bosh_taskcheck_username : value : admin .healthwatch-forwarder.boshhealth_instance_count : value : 1 .healthwatch-forwarder.boshtasks_instance_count : value : 2 .healthwatch-forwarder.canary_instance_count : value : 2 .healthwatch-forwarder.cli_instance_count : value : 2 .healthwatch-forwarder.health_check_az : value : AZ01 .healthwatch-forwarder.ingestor_instance_count : value : 4 .healthwatch-forwarder.opsman_instance_count : value : 2 .healthwatch-forwarder.publish_to_eva : value : true .healthwatch-forwarder.worker_instance_count : value : 4 .mysql.skip_name_resolve : value : true .properties.opsman : value : enable .properties.opsman.enable.url : value : https://pcf.example.com/ network-properties : network : name : DEPLOYMENT other_availability_zones : - name : AZ01 - name : AZ02 service_network : name : SERVICES singleton_availability_zone : name : AZ01 resource-config : healthwatch-forwarder : instances : automatic persistent_disk : size_mb : automatic instance_type : id : automatic migrate-v1.1-v1.2 : instances : automatic instance_type : id : automatic mysql : instances : automatic persistent_disk : size_mb : automatic instance_type : id : automatic redis : instances : automatic persistent_disk : size_mb : automatic instance_type : id : automatic Included below is a list of properties that can be set in the product.yml and a link to the API documentation explaining the properties. product-properties - properties for the tile Ops Manager API network-properties - a list of named networks to deploy the VMs to Ops Manager API resource-config - for the jobs of the tile Ops Manager API","title":"product config"},{"location":"reference/inputs-outputs.html#state","text":"This file contains that meta-information needed to manage the Ops Manager VM. The state input for a opsman VM task expects to have a state.yml file. 1 2 3 --- iaas : gcp vm_id : ops-manager-vm The file contains two properties: iaas is the iaas the ops manager vm is hosted on. ( gcp , vsphere , aws , azure , openstack ) vm_id is the VM unique identifier for the VM. For some IAAS, the vm ID is the VM name.","title":"state"},{"location":"reference/inputs-outputs.html#opsman-image","text":"This file is an artifact from Pivnet , which contains the VM image on an IAAS. For vsphere and openstack, it is a full disk image. For AWS, GCP, and Azure, it is the YAML file of the image locations. An example on how to pull the AWS image resource using the Pivnet Concourse Resource . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 resource_types : - name : pivnet type : docker-image source : repository : pivotalcf/pivnet-resource tag : latest-final resources : - name : opsman-image type : pivnet source : api_token : ((pivnet_token)) product_slug : ops-manager product_version : 2.* sort_by : semver jobs : - name : get-the-resource plan : - get : opsman-image params : globs : [ \"*AWS*.yml\" ]","title":"opsman image"},{"location":"reference/inputs-outputs.html#installation","text":"The file contains the information to restore an Ops Manager VM. The installation input for a opsman VM task expects to have a installation.zip file. This file can be exported from an Ops Manager VM using the export-installation . This file can be imported to an Ops Manager VM using the import-installation . Warning This file cannot be manually created. It is a file that must be generated via the export function of Ops Manager.","title":"installation"},{"location":"reference/inputs-outputs.html#stemcell","text":"This stemcell input requires the stemcell tarball ( .tgz ) as downloaded from Pivnet. It must be in the original filename as that is used by Ops Manager to parse metadata. The filename could look like bosh-stemcell-3541.48-vsphere-esxi-ubuntu-trusty-go_agent.tgz . Warning This file cannot be manually created. It is a file that must retrieved from Pivnet. An example on how to pull the vSphere stemcell using the Pivnet Concourse Resource . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 resource_types : - name : pivnet type : docker-image source : repository : pivotalcf/pivnet-resource tag : latest-final resources : - name : stemcell type : pivnet source : api_token : ((pivnet_token)) product_slug : stemcells product_version : 3541.* sort_by : semver jobs : - name : get-the-resource plan : - get : stemcell params : globs : [ \"*vsphere*.tgz\" ]","title":"stemcell"},{"location":"reference/inputs-outputs.html#product","text":"The product input requires a single tile file ( .pivotal ) as downloaded from Pivnet. An example on how to pull the PAS tile using the Pivnet Concourse Resource . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 resource_types : - name : pivnet type : docker-image source : repository : pivotalcf/pivnet-resource tag : latest-final resources : - name : stemcell type : pivnet source : api_token : ((pivnet_token)) product_slug : elastic-runtime product_version : 2.* sort_by : semver jobs : - name : get-the-resource plan : - get : product params : globs : [ \"*cf*.pivotal\" ] Warning This file cannot be manually created. It is a file that must retrieved from Pivnet.","title":"product"},{"location":"reference/inputs-outputs.html#download-product-config","text":"The config input for a download product task expects to have a download-config.yml file. The configuration of the download-config.yml looks like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 --- pivnet-api-token : token ## Note that file globs must be quoted if they start with *; ## otherwise they'll be interpreted as a YAML anchor. pivnet-file-glob : \"*.pivotal\" pivnet-product-slug : product-slugs ## defaults to false and should be excluded if not set to true # pivnet-disable-ssl: true ## Either product-version OR product-version-regex is required # product-version: 1.2.3 ## Note that the regex mustn't be quoted, ## as escape characters for the regex will confuse yaml parsers. # product-version-regex: ^1\\.2\\..*$ stemcell-iaas : google ## The following are required only if using download-product-s3. ## Any key marked required above is still required when using S3. ## If s3-bucket is set, ## downloaded product files will have their slug and version prepended. s3-bucket : s3-bucket s3-region-name : us-west-1 # required; sufficient for AWS s3-endpoint : s3.endpoint.com # if not using AWS, this is required ## Required unless `s3-auth-method` is `iam` s3-access-key-id : aws-or-minio-key-id s3-secret-access-key : aws-or-minio-secret-key ## Optional paths for both the product and the associated stemcell ## defaults to the root path of the specified bucket # s3-product-path: /path/to/product # s3-stemcell-path: /path/to/stemcell ## defaults to false and should be excluded if not set to true # s3-disable-ssl: true ## defaults to false; ## made available only because sometimes necessary for compatibility # s3-enable-v2-signing: true ## defaults to accesskey; ## allows use of AWS instance IAM creds, if available # s3-auth-method: iam","title":"download-product-config"},{"location":"reference/running-commands-locally.html","text":"This topic describes how to execute commands locally with Docker. If you wish to use the underlying om and p-automator CLI tools from your local workstation, we recommend using docker to execute commands. With p-automator in particular, using Docker is necessary, as the IaaS CLIs upon which we depend can be tricky to install. With om it's more a matter of convenience - you can just as easily download the binary if it's available for your system. Executing Commands To execute commands in Docker: First import the image: 1 docker import ${ PLATFORM_AUTOMATION_IMAGE_TGZ } platform-automation-image Where ${PLATFORM_AUTOMATION_IMAGE_TGZ} is the image file downloaded from Pivnet. Then, you can use docker run to pass it arbitrary commands. Here, we're running the p-automator CLI to see what commands are available: 1 2 docker run -it --rm -v $PWD :/workspace -w /workspace platform-automation-image \\ p-automator -h Note: that this will have access read and write files in your current working directory. If you need to mount other directories as well, you can add additional -v arguments. Useful Commands It can be very useful to pull configuration for the director and tiles locally using Docker. To get the staged config for a product: 1 2 docker run -it --rm -v $PWD :/workspace -w /workspace platform-automation-image \\ om --env ${ ENV_FILE } staged-config --product-name ${ PRODUCT_SLUG } --include-placeholders ${ENV_FILE} is the [generating-env-file] required for all tasks. ${PRODUCT_SLUG} is the name of the product downloaded from [pivnet]. The resulting file can then be parameterized, saved, and committed to a config repo. To get the director configuration: 1 2 docker run -it --rm -v $PWD :/workspace -w /workspace platform-automation-image \\ om --env ${ ENV_FILE } staged-director-config --include-placeholders","title":"Running Commands Locally"},{"location":"reference/running-commands-locally.html#executing-commands","text":"To execute commands in Docker: First import the image: 1 docker import ${ PLATFORM_AUTOMATION_IMAGE_TGZ } platform-automation-image Where ${PLATFORM_AUTOMATION_IMAGE_TGZ} is the image file downloaded from Pivnet. Then, you can use docker run to pass it arbitrary commands. Here, we're running the p-automator CLI to see what commands are available: 1 2 docker run -it --rm -v $PWD :/workspace -w /workspace platform-automation-image \\ p-automator -h Note: that this will have access read and write files in your current working directory. If you need to mount other directories as well, you can add additional -v arguments.","title":"Executing Commands"},{"location":"reference/running-commands-locally.html#useful-commands","text":"It can be very useful to pull configuration for the director and tiles locally using Docker. To get the staged config for a product: 1 2 docker run -it --rm -v $PWD :/workspace -w /workspace platform-automation-image \\ om --env ${ ENV_FILE } staged-config --product-name ${ PRODUCT_SLUG } --include-placeholders ${ENV_FILE} is the [generating-env-file] required for all tasks. ${PRODUCT_SLUG} is the name of the product downloaded from [pivnet]. The resulting file can then be parameterized, saved, and committed to a config repo. To get the director configuration: 1 2 docker run -it --rm -v $PWD :/workspace -w /workspace platform-automation-image \\ om --env ${ ENV_FILE } staged-director-config --include-placeholders","title":"Useful Commands"},{"location":"reference/task.html","text":"Platform Automation for PCF Tasks This document lists each Platform Automation for PCF task, and provides information about their intentions, inputs, and outputs. The tasks are presented, in their entirety, as they are found in the product. The docker image can be used to invoke the commands in each task locally. Use --help for more information. To learn more see the running-commands-locally section. apply-changes Triggers an install on the Ops Manager described by the auth file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 --- platform : linux inputs : - name : env # contains the env file with target OpsMan Information params : ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux om --env env/\"${ENV_FILE}\" apply-changes apply-director-changes apply-changes can also be used to trigger an install for just the BOSH Director with the --skip-deploy-products / -sdp flag. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 --- platform : linux inputs : - name : env # contains the env file with target OpsMan Information params : ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux om --env env/\"${ENV_FILE}\" apply-changes \\ --skip-deploy-products assign-multi-stemcell assign-multi-stemcell assigns multiple stemcells to a provided product. This feature is only available in OpsMan 2.6+. For more information on how to utilize this workflow, check out the Stemcell Handling topic. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # This feature is only available in OpsMan 2.6+. --- platform : linux inputs : - name : env # contains the env file with target OpsMan Information - name : config # contains the configuration file for assign-multi-stemcell command # - Example config: # --- # product: cf # stemcell: # - ubuntu-trusty:1234.6 # - ubuntu-xenial:latest params : ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input CONFIG_FILE : config.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `assign-multi-stemcell-config` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux om --env env/\"${ENV_FILE}\" assign-multi-stemcell \\ --config config/\"$CONFIG_FILE\" assign-stemcell assign-stemcell assigns a stemcell to a provided product. For more information on how to utilize this workflow, check out the Stemcell Handling topic. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 --- platform : linux inputs : - name : env # contains the env file with target OpsMan Information - name : config # contains the configuration file for assign-stemcell command # - Can consume the output of `download-product` task directly # - Example config: # --- # product: cf # stemcell: 3468.86 params : ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input CONFIG_FILE : config.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `assign-stemcell-config` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux om --env env/\"${ENV_FILE}\" assign-stemcell \\ --config config/\"$CONFIG_FILE\" configure-authentication Configures Ops Manager with an internal userstore and admin user account. See configure-saml-authentication to configure an external SAML user store, and configure-ldap-authentication to configure with LDAP. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 --- platform : linux inputs : - name : env # contains the env file with target OpsMan Information - name : config # contains the auth configuration params : ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input AUTH_CONFIG_FILE : auth.yml # - Required # - Filepath of the authorization config YAML # - The path is relative to root of the `config` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux om --env env/\"${ENV_FILE}\" configure-authentication \\ --config config/\"${AUTH_CONFIG_FILE}\" For details on the config file expected in the config input, please see Generating an Auth File . configure-director Configures the BOSH Director with settings from a config file. See staged-director-config , which can extract a config file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 --- platform : linux inputs : - name : config # contains the director configuration file - name : env # contains the env file with target OpsMan Information - name : vars # variable files to be made available optional : true - name : secrets # secret files to be made available # separate from vars, so they can be store securely optional : true - name : ops-files # operations files to custom configure the product optional : true params : VARS_FILES : # - Optional # - Filepath to the Ops Manager vars yaml file # - The path is relative to root of the task build, # so `vars` and `secrets` can be used. OPS_FILES : # - Optional # - Filepath to the Ops Manager operations yaml files # - The path is relative to root of the task build ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input DIRECTOR_CONFIG_FILE : director.yml # - Required # - Filepath to the director configuration yaml file # - The path is relative to the root of the `config` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux vars_files_args=(\"\") for vf in ${VARS_FILES} do vars_files_args+=(\"--vars-file ${vf}\") done ops_files_args=(\"\") for of in ${OPS_FILES} do ops_files_args+=(\"--ops-file ${of}\") done # ${vars_files_args[@] needs to be globbed to pass through properly # ${ops_files_args[@] needs to be globbed to pass through properly # shellcheck disable=SC2068 om --env env/\"${ENV_FILE}\" configure-director \\ --config \"config/${DIRECTOR_CONFIG_FILE}\" \\ ${vars_files_args[@]} \\ ${ops_files_args[@]} GCP with service account For GCP, if service account is used, the property associated_service_account has to be set explicitly in the iaas_configuration section. configure-ldap-authentication Configures Ops Manager with an external LDAP user store and admin user account. See configure-authentication to configure an internal user store, and configure-saml-authentication to configure with SAML. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 --- platform : linux inputs : - name : env # contains the env file with target OpsMan Information - name : config # contains the auth configuration params : ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input AUTH_CONFIG_FILE : auth.yml # - Required # - Filepath of the authorization config YAML # - The path is relative to root of the `config` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux om --env env/\"${ENV_FILE}\" configure-ldap-authentication \\ --config config/\"${AUTH_CONFIG_FILE}\" For more details on using LDAP, please refer to the Ops Manager documentation . For details on the config file expected in the config input, please see Generating an Auth File . configure-product Configures an individual, staged product with settings from a config file. Not to be confused with Ops Manager's built-in import , which reads all deployed products and configurations from a single opaque file, intended for import as part of backup/restore and upgrade lifecycle processes. See staged-config , which can extract a config file, and upload-and-stage-product , which can stage a product that's been uploaded. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 --- platform : linux inputs : - name : config # contains the product configuration file - name : env # contains the env file with target OpsMan Information - name : vars # variable files to be made available optional : true - name : secrets # secret files to be made available # separate from vars, so they can be store securely optional : true - name : ops-files # operations files to custom configure the product optional : true params : CONFIG_FILE : # - Required # - Filepath to the product configuration yaml file # - The path is relative to the root of the `config` input VARS_FILES : # - Optional # - Filepath to the product configuration vars yaml file # - The path is relative to root of the task build, # so `vars` and `secrets` can be used. OPS_FILES : # - Optional # - Filepath to the product configuration operations yaml files # - The path is relative to root of the task build ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux vars_files_args=(\"\") for vf in ${VARS_FILES} do vars_files_args+=(\"--vars-file ${vf}\") done ops_files_args=(\"\") for of in ${OPS_FILES} do ops_files_args+=(\"--ops-file ${of}\") done # ${vars_files_args[@] needs to be globbed to pass through properly # ${ops_files_args[@] needs to be globbed to pass through properly # shellcheck disable=SC2068 om --env env/\"${ENV_FILE}\" configure-product \\ --config \"config/${CONFIG_FILE}\" \\ ${vars_files_args[@]} \\ ${ops_files_args[@]} configure-saml-authentication Configures Ops Manager with an external SAML user store and admin user account. See configure-authentication to configure an internal user store, and configure-ldap-authentication to configure with LDAP. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 --- platform : linux inputs : - name : env # contains the env file with target OpsMan Information - name : config # contains the auth configuration params : ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input AUTH_CONFIG_FILE : auth.yml # - Required # - Filepath of the authorization config YAML # - The path is relative to root of the `config` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux om --env env/\"${ENV_FILE}\" configure-saml-authentication \\ --config config/\"${AUTH_CONFIG_FILE}\" Configuring SAML has two different auth flows for the UI and the task. The UI will have a browser based login flow. The CLI will require client-id and client-secret as it cannot do a browser login flow. For more details on using SAML, please refer to the Ops Manager documentation For details on the config file expected in the config input, please see Generating an Auth File . create-vm Creates an unconfigured Ops Manager VM. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 --- platform : linux inputs : - name : state # contains the state for the vm - name : config # contains the product configuration file - name : image # contains the image file to be installed - name : vars # variable files to be made available optional : true - name : secrets # secret files to be made available # separate from vars, so they can be store securely optional : true outputs : - name : generated-state #contains the updated state file params : VARS_FILES : # - Optional # - Filepath to the Ops Manager vars yaml file # - The path is relative to root of the task build, # so `vars` and `secrets` can be used. OPSMAN_CONFIG_FILE : opsman.yml # - Required # - Filepath of the opsman config YAML # - The path is relative to root of the `config` input STATE_FILE : state.yml # - Required # - Filepath of the state yaml file # - The path is relative to root of the `state` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux vars_files_args=(\"\") for vf in ${VARS_FILES} do vars_files_args+=(\"--vars-file ${vf}\") done generated_state_path=\"generated-state/$(basename \"$STATE_FILE\")\" if [ -e \"state/$STATE_FILE\" ]; then cp \"state/$STATE_FILE\" \"$generated_state_path\" fi export IMAGE_FILE IMAGE_FILE=\"$(find image/*.{yml,ova,raw} 2>/dev/null | head -n1)\" if [ -z \"$IMAGE_FILE\" ]; then echo \"No image file found in image input.\" echo \"Contents of image input:\" ls -al image exit 1 fi # ${vars_files_args[@] needs to be globbed to split properly # shellcheck disable=SC2068 p-automator create-vm \\ --config config/\"${OPSMAN_CONFIG_FILE}\" \\ --image-file \"${IMAGE_FILE}\" \\ --state-file \"$generated_state_path\" \\ ${vars_files_args[@]} This task requires a config file specific to the IaaS being deployed to. Please see the configuration page for more specific examples. The task does specific CLI commands for the creation of the Ops Manager VM on each IAAS. See below for more information: AWS Requires the image YAML file from Pivnet Validates the existence of the VM if defined in the statefile, if so do nothing Chooses the correct ami to use based on the provided image YAML file from Pivnet Creates the vm configured via opsman config and the image YAML. This only attaches existing infrastructure to a newly created VM. This does not create any new resources The public IP address, if provided, is assigned after successful creation Azure Requires the image YAML file from Pivnet Validates the existence of the VM if defined in the statefile, if so do nothing Copies the image (of the OpsMan VM from the specified region) as a blob into the specified storage account Creates the Ops Manager image Creates a VM from the image. This will use unmanaged disk (if specified), and assign a public and/or private IP. This only attaches existing infrastructure to a newly createdVM. This does not create any new resources. GCP Requires the image YAML file from Pivnet Validates the existence of the VM if defined in the statefile, if so do nothing Creates a compute image based on the region specific Ops Manager source URI in the specified Ops Manager account Creates a VM from the image. This will assign a public and/or private IP address, VM sizing, and tags. This does not create any new resources. Openstack Requires the image YAML file from Pivnet Validates the existence of the VM if defined in the statefile, if so do nothing Recreates the image in openstack if it already exists to validate we are using the correct version of the image Creates a VM from the image. This does not create any new resources The public IP address, if provided, is assigned after successful creation Vsphere Requires the OVA image from Pivnet Validates the existence of the VM if defined in the statefile, if so do nothing Build ipath from the provided datacenter, folder, and vmname provided in the config file. The created VM is stored on the generated path. If folder is not provided, the vm will be placed in the datacenter. Creates a VM from the image provided to the create-vm command. This does not create any new resources credhub-interpolate Interpolate credhub entries into configuration files 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 --- platform : linux inputs : - name : files # contains YAML files with extension `.yml`. # Each one of these files will have their values interpolated from credhub. # For examples, run: `credhub interpolate --help` # (minimum version >= 2.1.0 required) outputs : - name : interpolated-files # Contains only yaml files found and interpolated by this task. # Maintains the filestructure of the `files` input. # all params are required to be filled out params : CREDHUB_CLIENT : CREDHUB_SECRET : CREDHUB_SERVER : # - Required # - Credentials to talk to credhub server CREDHUB_CA_CERT : # - Optional # - This is only necessary if your Concourse worker # is not already configured to trust the CA used for Credhub PREFIX : # - Required # - Prefix flag used by credhub interpolate INTERPOLATION_PATHS : '.' # - Required # - Path the contains the files to read from # - This is a space separated list of directories # the paths are all evaluated relative to files/ SKIP_MISSING : true # Optional # Change to false to have strict interpolation # and fail if params are missing from vars run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -euo pipefail # NOTE: The credhub cli does not ignore empty/null environment variables. # https://github.com/cloudfoundry-incubator/credhub-cli/issues/68 if [ -z \"$CREDHUB_CA_CERT\" ]; then unset CREDHUB_CA_CERT fi credhub --version if [ -z \"$PREFIX\" ]; then echo \"Please specify a PREFIX. It is required.\" exit 1 fi # $INTERPOLATION_PATHS needs to be globbed to read multiple files # shellcheck disable=SC2086 files=$(cd files && find $INTERPOLATION_PATHS -type f -name '*.yml' -follow) if [ \"$SKIP_MISSING\" ]; then export SKIP_MISSING=\"--skip-missing\" else export SKIP_MISSING=\"\" fi for file in $files; do echo \"interpolating files/$file\" mkdir -p interpolated-files/\"$(dirname \"$file\")\" credhub interpolate --prefix \"$PREFIX\" \\ --file files/\"$file\" \"$SKIP_MISSING\" \\ > interpolated-files/\"$file\" done This task requires a valid credhub with UAA client and secret. For information on how to set this up, see Secrets Handling delete-installation Delete the Ops Manager Installation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 --- platform : linux inputs : - name : env # contains the env file with target OpsMan Information params : ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux om --env env/\"${ENV_FILE}\" delete-installation --force delete-vm Deletes the Ops Manager VM instantiated by create-vm . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 --- platform : linux inputs : - name : state # contains the state for the vm - name : config # contains the product configuration file - name : vars # variable files to be made available optional : true - name : secrets # secret files to be made available # separate from vars, so they can be store securely optional : true outputs : - name : generated-state #contains the updated state file params : VARS_FILES : # - Optional # - Filepath to the Ops Manager vars yaml file # - The path is relative to root of the task build, # so `vars` and `secrets` can be used. STATE_FILE : state.yml # - Required # - Filepath of the state yaml file # - The path is relative to root of the `state` input OPSMAN_CONFIG_FILE : opsman.yml # - Required # - Filepath of the opsman config YAML # - The path is relative to root of the `config` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux vars_files_args=(\"\") for vf in ${VARS_FILES} do vars_files_args+=(\"--vars-file ${vf}\") done generated_state_path=\"generated-state/$(basename \"$STATE_FILE\")\" cp \"state/$STATE_FILE\" \"$generated_state_path\" # ${vars_files_args[@] needs to be globbed to split properly # shellcheck disable=SC2068 p-automator delete-vm \\ --state-file \"$generated_state_path\" \\ --config config/\"$OPSMAN_CONFIG_FILE\" \\ ${vars_files_args[@]} This task requires the state file generated create-vm . The task does specific CLI commands for the deletion of the Ops Manager VM and resources on each IAAS. See below for more information: AWS Deletes the VM Azure Deletes the VM Attempts to delete the associated disk Attempts to delete the associated nic Attempts to delete the associated image GCP Deletes the VM Attempts to delete the associated image Openstack Deletes the VM Attempts to delete the associated image vSphere Deletes the VM download-product Ops Manager 2.5 The filename for the artifact downloaded from Ops Manager is changed! If your resources or pipelines have a regex for the Ops Manager filename, you may be affected. (Please see Ops Manager's official notice for more information) Downloads a product specified in a config file from Pivnet. Optionally, also downloads the latest stemcell for that product. Downloads are cached, so files are not re-downloaded each time. Outputs can be persisted to an S3-compatible blobstore using a put to an appropriate resource for later use with the download-product-s3 , or used directly as inputs to upload-and-stage-product and upload-stemcell tasks. This task requires a download-product config file . If S3 configuration is present in the download-product config file , the slug and version of the downloaded product file will be prepended in brackets to the filename. For example: original-pivnet-filenames: 1 2 ops - manager - aws - 2 . 5 . 0 - build . 123 . yml cf - 2 . 5 . 0 - build . 45 . pivotal download-product-filenames if S3 configuration is present: 1 2 [ ops - manager , 2 . 5 . 0 ] ops - manager - aws - 2 . 5 . 0 - build . 123 . yml [ elastic - runtime , 2 . 5 . 0 ] cf - 2 . 5 . 0 - build . 45 . pivotal This is to allow the same config parameters that let us select a file from Pivnet select it again when pulling from S3. Note that the filename will be unchanged if S3 keys are not present in the configuration file. This avoids breaking current pipelines. When using the s3 resource in concourse If you are using a regexp in your s3 resource definition that explicitly requires the pivnet filename to be the start of the regex, (i.e., the pattern starts with ^ ) this won't work when using S3 config. The new file format preserves the original filename, so it is still possible to match on that - but if you need to match from the beginning of the filename, that will have been replaced by the prefix described above. When specifying PAS-Windows This task will automatically download and inject the winfs for pas-windows. When specifying PAS-Windows on Vsphere This task cannot download the stemcell for pas-windows on vSphere. To build this stemcell manually, please reference the Creating a vSphere Windows Stemcell guide in Pivotal Documentation. When only downloading from Pivnet When the download product config only has Pivnet credentials, it will not add the prefix to the downloaded product. For example, example-product.pivotal from Pivnet will be outputed as example-product.pivotal . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 --- platform : linux inputs : - name : config # contains download-file config file - name : vars # variable files to be made available optional : true - name : secrets # secret files to be made available # separate from vars, so they can be store securely optional : true outputs : - name : downloaded-product - name : downloaded-stemcell - name : assign-stemcell-config caches : - path : downloaded-files params : CONFIG_FILE : download-config.yml # - Required # - Filepath to the product configuration yaml file # - The path is relative to the root of the `config` input VARS_FILES : # - Optional # - Filepath to the product configuration vars yaml file # - The path is relative to root of the task build, # so `vars` and `secrets` can be used. run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux vars_files_args=(\"\") for vf in ${VARS_FILES} do vars_files_args+=(\"--vars-file ${vf}\") done # ${vars_files_args[@] needs to be globbed to pass through properly # shellcheck disable=SC2068 om download-product \\ --config config/\"${CONFIG_FILE}\" ${vars_files_args[@]} \\ --output-directory downloaded-files { printf \"\\nReading product details...\"; } 2> /dev/null # shellcheck disable=SC2068 product_slug=$(om interpolate \\ --config config/\"${CONFIG_FILE}\" ${vars_files_args[@]} \\ --path /pivnet-product-slug) product_file=$(om interpolate \\ --config downloaded-files/download-file.json \\ --path /product_path) stemcell_file=$(om interpolate \\ --config downloaded-files/download-file.json \\ --path /stemcell_path?) { printf \"\\nChecking if product needs winfs injected...\"; } 2> /dev/null if [ \"$product_slug\" == \"pas-windows\" ]; then TILE_FILENAME=\"$(basename \"$product_file\")\" # The winfs-injector determines the necessary windows image, # and uses the CF-foundation dockerhub repo # to pull the appropriate Microsoft-hosted foreign layer. winfs-injector \\ --input-tile \"$product_file\" \\ --output-tile \"downloaded-product/${TILE_FILENAME}\" else cp \"$product_file\" downloaded-product fi if [ -e \"$stemcell_file\" ]; then cp \"$stemcell_file\" downloaded-stemcell fi if [ -e downloaded-files/assign-stemcell.yml ]; then cp downloaded-files/assign-stemcell.yml assign-stemcell-config/config.yml fi download-product-s3 Downloads a product specified in a config file from an S3-compatible blobstore. This is useful when retrieving assets in an offline environment. Downloads are cached, so files are not re-downloaded each time. This is intended to be used with files downloaded from Pivnet by download-product and then persisted to a blobstore using a put step. Outputs can be used directly as an input to upload-and-stage-product and upload-stemcell tasks. This task requires a download-product config file . The same configuration file should be used with both this task and download-product . This ensures that the same file is being captured with both tasks. The product files uploaded to s3 for download with this task require a specific prefix: [product-slug,semantic-version] . This prefix is added by the download-product task when S3 keys are present in the configuration file. This is the meta information about the product from Pivnet, which is not guaranteed to be in the original filename. This tasks uses the meta information to be able to perform consistent downloads from s3 as defined in the provided download config. For example: original-pivnet-filenames: 1 2 ops - manager - aws - 2 . 5 . 0 - build . 123 . yml cf - 2 . 5 . 0 - build . 45 . pivotal filenames expected by download-product-s3 in a bucket: 1 2 [ ops - manager , 2 . 5 . 0 ] ops - manager - aws - 2 . 5 . 0 - build . 123 . yml [ elastic - runtime , 2 . 5 . 0 ] cf - 2 . 5 . 0 - build . 45 . pivotal When only downloading from Pivnet When the download product config only has Pivnet credentials, it will not add the prefix to the downloaded product. For example, example-product.pivotal from Pivnet will be outputed as example-product.pivotal . Info It's possible to use IAM instance credentials instead of providing S3 creds in the config file. See download-product config file for details. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 --- platform : linux inputs : - name : config # contains download-file config file - name : vars # variable files to be made available optional : true - name : secrets # secret files to be made available # separate from vars, so they can be store securely optional : true outputs : - name : downloaded-product - name : downloaded-stemcell - name : assign-stemcell-config caches : - path : downloaded-files params : CONFIG_FILE : download-config.yml # - Required # - Filepath to the product configuration yaml file # - The path is relative to the root of the `config` input VARS_FILES : # - Optional # - Filepath to the product configuration vars yaml file # - The path is relative to root of the task build, # so `vars` and `secrets` can be used. run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux vars_files_args=(\"\") for vf in ${VARS_FILES} do vars_files_args+=(\"--vars-file ${vf}\") done # ${vars_files_args[@] needs to be globbed to pass through properly # shellcheck disable=SC2068 om download-product \\ --config config/\"${CONFIG_FILE}\" ${vars_files_args[@]} \\ --output-directory downloaded-files \\ --source s3 product_file=$(om interpolate \\ --config downloaded-files/download-file.json \\ --path /product_path) stemcell_file=$(om interpolate \\ --config downloaded-files/download-file.json \\ --path /stemcell_path?) cp \"$product_file\" downloaded-product if [ -e \"$stemcell_file\" ]; then cp \"$stemcell_file\" downloaded-stemcell fi if [ -e downloaded-files/assign-stemcell.yml ]; then cp downloaded-files/assign-stemcell.yml assign-stemcell-config/config.yml fi export-installation Exports an existing Ops Manager to a file. This is the first part of the backup/restore and upgrade lifecycle processes. This task is used on a fully installed and healthy Ops Manager to export settings to an upgraded version of Ops Manager. To use with non-versioned blobstore, you can override INSTALLATION_FILE param to include $timestamp , then the generated installation file will include a sortable timestamp in the filename. example: 1 2 params : INSTALLATION_FILE : installation-$timestamp.zip Info The timestamp is generated using the time on concourse worker. If the time is different on different workers, the generated timestamp may fail to sort correctly. Changing the time or timezone on workers might interfere with ordering. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 --- platform : linux inputs : - name : env # contains the env file with target OpsMan Information outputs : - name : installation # will contain the exported installation params : ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input INSTALLATION_FILE : installation-$timestamp.zip # - Required # - Filepath of the installation ZIP file # - The path is relative to root of the `installation` output # - if the filename includes \"$timestamp\", # for example \"installation-$timestamp.zip\", # the final filename will include the current timestamp. # - this is necessary if using an \"S3 compatible\" blobstore # that doesn't support versioned blobs # - timestamped filenames will need to be represented # with a glob-style wildcard in the `upgrade-opsman` task configuration # (the default will work with the example provided above). run : path : bash args : - -c - | cat /var/version && echo \"\" set -eux timestamp=\"$(date '+%Y%m%d.%-H%M.%S+%Z')\" export timestamp # '$timestamp' must be a literal, because envsubst uses it as a filter # this allows us to avoid accidentally interpolating anything else. # shellcheck disable=SC2016 OUTPUT_FILE_NAME=\"$(echo \"$INSTALLATION_FILE\" | envsubst '$timestamp')\" om --env env/\"${ENV_FILE}\" export-installation \\ --output-file installation/\"$OUTPUT_FILE_NAME\" Warning It is recommended to persist the zip file exported from export-installation to an external file store (eg S3) on a regular basis. The exported installation can restore the Ops Manager to a working state if it is non-functional. import-installation Imports a previously exported installation to Ops Manager. This is a part of the backup/restore and upgrade lifecycle processes. This task is used after an installation has been exported and a new Ops Manager has been deployed, but before the new Ops Manager is configured. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 --- platform : linux inputs : - name : env # contains the environment information about the OpsMan - name : installation # contains the installation to be imported params : ENV_FILE : env.yml # - Required # - Filepath of the environment config YAML # - The path is relative to root of the `env` input # - The env file _must_ contain the `decryption-passphrase` # while it's optional for other tasks, this one requires it. INSTALLATION_FILE : installation*.zip # - Required # - Filepath of the installation ZIP file # - The filepath provided can be wildcard expanded. # - The path is relative to root of the `installation` input run : path : bash args : - -c - | cat /var/version && echo \"\" set -eux # INSTALLATION_FILE needs to be globbed # shellcheck disable=SC2086 om --env env/\"${ENV_FILE}\" import-installation \\ --installation installation/$INSTALLATION_FILE make-git-commit Copies a single file into a repo and makes a commit. Useful for persisting the state output of tasks that manage the vm, such as: create-vm upgrade-opsman delete-vm Also useful for persisting the configuration output from: staged-config staged-director-config Info This commits all changes present in the repo used for the repository input, in addition to copying in a single file. Info This does not perform a git push ! You will need to put the output of this task to a git resource to persist it. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 --- platform : linux inputs : - name : repository # - This must be an initialized git repository. # - Note that any changes present in this input # will be committed along with the file copied in # by this task. - name : file-source # - This is the input folder containing the file to be committed. # Typically, this will from some other task # with an output that needs to be persisted. outputs : - name : repository-commit params : FILE_SOURCE_PATH : # - Required # - Filepath to be copied into the git repo # before a commit is created # - Relative to the root of the `file-source` input FILE_DESTINATION_PATH : # - Required # - Filepath to write the file specified by FILE_SOURCE_PATH # - Relative to the root of the `repository` input GIT_AUTHOR_NAME : # - Required # - Used to configure the human-readable # name in the `author` field of the commit GIT_AUTHOR_EMAIL : # - Required # - Used to configure the email address # in the `author` field of the commit COMMIT_MESSAGE : # - Required # - Specify a commit message to be used # for all commits made by this task. run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eu git config --global user.email \"$GIT_AUTHOR_EMAIL\" git config --global user.name \"$GIT_AUTHOR_NAME\" git clone repository repository-commit if [ ! -d \"$(dirname \"$FILE_DESTINATION_PATH\")\" ]; then echo \"Directory $(dirname \"$FILE_DESTINATION_PATH\") does not exist in repository, creating it...\" mkdir -p \"$(dirname \"$FILE_DESTINATION_PATH\")\"; fi; cp file-source/\"$FILE_SOURCE_PATH\" \\ repository-commit/\"$FILE_DESTINATION_PATH\" cd repository-commit git add -A git commit -m \"$COMMIT_MESSAGE\" --allow-empty pre-deploy-check Checks if the Ops Manager director is configured properly and validates the configuration. Additionally, checks each of the staged products and validates they are configured correctly. This task can be run at any time and can be used a a pre-check for apply-changes . The checks that this task executes are: is configuration complete and valid is the network assigned is the availability zone assigned is the stemcell assigned what stemcell type/version is required are there any unset/invalid properties did any ops manager verifiers fail If any of the above checks fail the task will fail. The failed task will provide a list of errors that need to be fixed before an apply-changes could start. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 --- platform : linux inputs : - name : env # contains the env file with target OpsMan Information params : ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux om --env env/\"${ENV_FILE}\" pre-deploy-check stage-product Staged a product to the Ops Manager specified in the config file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 --- platform : linux inputs : - name : product # contains the product file to be uploaded and staged - name : env # contains the env file with target OpsMan Information params : ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux product_name=\"$(om tile-metadata \\ --product-path product/*.pivotal \\ --product-name)\" product_version=\"$(om tile-metadata \\ --product-path product/*.pivotal \\ --product-version)\" om --env env/\"${ENV_FILE}\" stage-product \\ --product-name \"$product_name\" \\ --product-version \"$product_version\" staged-config Downloads the configuration for a product from Ops Manager. Not to be confused with Ops Manager's built-in export , which puts all deployed products and configurations into a single file, intended for import as part of backup/restore and upgrade lifecycle processes. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 --- platform : linux inputs : - name : env # contains the env file with target OpsMan Information outputs : - name : generated-config # will contain the staged product config params : PRODUCT_NAME : # - Required # - The name of the product config to be exported ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input SUBSTITUTE_CREDENTIALS_WITH_PLACEHOLDERS : true # - Optional # - Replace credentials with interpolatable variable names # - If set to false, **literal credentials** will be included in the output run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux flag=$(if \"$SUBSTITUTE_CREDENTIALS_WITH_PLACEHOLDERS\"; then echo '--include-placeholders'; else echo '--include-credentials'; fi ) om --env env/\"${ENV_FILE}\" staged-config \\ --product-name \"$PRODUCT_NAME\" \\ \"$flag\" > generated-config/\"$PRODUCT_NAME\".yml staged-director-config Ops Manager 2.5 The filename for the artifact downloaded from Ops Manager is changed! If your resources or pipelines have a regex for the Ops Manager filename, you may be affected. (Please see Ops Manager's official notice for more information) Downloads configuration for the BOSH director from Ops Manager. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 --- platform : linux inputs : - name : env # contains the env file with target OpsMan Information outputs : - name : generated-config # will contain the staged product config params : ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input run : path : bash args : - -c - | cat /var/version && echo \"\" set -eux om --env env/\"${ENV_FILE}\" staged-director-config \\ --include-placeholders > generated-config/director.yml The configuration is exported to the generated-config output. It does not extract credentials from Ops Manager and replaced them all with YAML interpolation (()) placeholders. This is to ensure that credentials are never written to disk. The credentials need to be provided from an external configuration when invoking configure-director . Info staged-director-config will not be able to grab all sensitive fields in your Ops Manager installation (for example: vcenter_username and vcenter_password if using vsphere). To find these missing fields, please refer to the Ops Manager API Documentation test An example task to ensure the assets and docker image are setup correctly in your concourse pipeline. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 --- platform : linux run : path : bash args : - \"-c\" - | echo \"Platform Automation for PCF version:\" cat /var/version && echo \"\" printf \"\\\\np-automator version:\" p-automator -v printf \"\\\\nom version:\" om -v set -eux p-automator --help om --help { echo \"Successfully validated tasks and image!\"; } 2> /dev/null test-interpolate An example task to ensure that all required vars are present when interpolating into a base file. For more instruction on this topic, see the variables section 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 --- platform : linux inputs : - name : config # contains the base configuration file - name : vars # variable files to be made available optional : true params : VARS_FILES : # - Optional # - Filepath to the vars yaml file # - The path is relative to root of the task build, # so `vars` and `secrets` can be used. CONFIG_FILE : base.yml # - Required # - Filepath to the base yaml file to interpolate from # - The path is relative to root of the task build SKIP_MISSING : true # - Optional # - Change to false to have strict interpolation # and fail if params are missing from vars run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux vars_files_args=(\"\") for vf in ${VARS_FILES} do vars_files_args+=(\"--vars-file ${vf}\") done if [ \"$SKIP_MISSING\" ]; then export SKIP_MISSING=\"--skip-missing\" else export SKIP_MISSING=\"\" fi # ${vars_files_args[@] needs to be globbed to pass through properly # shellcheck disable=SC2068 om interpolate --config \"config/$CONFIG_FILE\" \"$SKIP_MISSING\" ${vars_files_args[@]} upgrade-opsman Upgrades an existing Ops Manager to a new given Ops Manager version 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 --- platform : linux inputs : - name : state # contains the state for the vm - name : config # contains the OpsMan configuration file - name : image # contains the image file to be installed - name : installation # contains the installation to be imported - name : env # contains the environment information for OpsMan - name : vars # variable files to be made available optional : true - name : secrets # secret files to be made available # separate from vars, so they can be stored securely optional : true outputs : - name : generated-state #contains the updated state file params : VARS_FILES : # - Optional # - space-seperated array of filepaths to YAML vars files # to be loaded with the OPSMAN_CONFIG_FILE # - relative to root of the task build, # so both `vars` and `secrets` can be used. ENV_FILE : env.yml # - Required # - filepath of the env config YAML # - relative to root of the `env` input OPSMAN_CONFIG_FILE : opsman.yml # - Required # - filepath of the opsman config YAML # - relative to root of the `config` input STATE_FILE : state.yml # - Required # - filepath of the state YAML file # - relative to root of the `state` input INSTALLATION_FILE : installation*.zip # - Required # - filepath of the installation ZIP file # - can be wildcard expanded # - relative to root of the `installation` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" p-automator -v set -eux vars_files_args=(\"\") for vf in ${VARS_FILES} do vars_files_args+=(\"--vars-file ${vf}\") done generated_state_path=\"generated-state/$(basename \"$STATE_FILE\")\" cp \"state/$STATE_FILE\" \"$generated_state_path\" # ${vars_files_args[@] needs to be globbed to split properly (SC2068) # INSTALLATION_FILE needs to be globbed (SC2086) # shellcheck disable=SC2068,SC2086 p-automator upgrade-opsman \\ --config config/\"${OPSMAN_CONFIG_FILE}\" \\ --env-file env/\"${ENV_FILE}\" \\ --image-file \"$(find image/*.{yml,ova,raw} | head -n1)\" \\ --state-file \"$generated_state_path\" \\ --installation installation/$INSTALLATION_FILE \\ ${vars_files_args[@]} For more information about this task and how it works, see the upgrade page. upload-and-stage-product Uploads and stages product to the Ops Manager specified in the config file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 --- platform : linux inputs : - name : product # contains the product file to be uploaded and staged - name : env # contains the env file with target OpsMan Information - name : config # contains the product configuration file optional : true params : CONFIG_FILE : # - Optional # - Path to the config file for the product # - Relative to the root of the config input # - If empty, no config will be used; version and sha256 will not be checked # - Example config: # --- # product-version: 1.2.3-build.4 # sha256: 6daededd8fb4c341d0cd437a669d732d2fde62cb89716498e6b16f34607a1498 ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux if [ -z \"$CONFIG_FILE\" ]; then om --env env/\"${ENV_FILE}\" upload-product \\ --product product/*.pivotal else om --env env/\"${ENV_FILE}\" upload-product \\ --product product/*.pivotal --config \"config/$CONFIG_FILE\" fi product_name=\"$(om tile-metadata \\ --product-path product/*.pivotal \\ --product-name)\" product_version=\"$(om tile-metadata \\ --product-path product/*.pivotal \\ --product-version)\" om --env env/\"${ENV_FILE}\" stage-product \\ --product-name \"$product_name\" \\ --product-version \"$product_version\" upload-product Uploads a product to the Ops Manager specified in the config file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 --- platform : linux inputs : - name : product # contains the product file to be uploaded and staged - name : env # contains the env file with target OpsMan Information - name : config # contains the product configuration file optional : true params : CONFIG_FILE : # - Optional # - Path to the config file for the product # - Relative to the root of the `config` input # - If empty, no config will be used; version and sha256 will not be checked # - Example config: # --- # product-version: 1.2.3-build.4 # shasum: 6daededd8fb4c341d0cd437a669d732d2fde62cb89716498e6b16f34607a1498 ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - Relative to root of the `env` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux export OPTIONAL_CONFIG_FLAG=\"\" if [ -n \"$CONFIG_FILE\" ]; then export OPTIONAL_CONFIG_FLAG=\"--config config/$CONFIG_FILE\" fi om --env env/\"${ENV_FILE}\" upload-product \\ --product product/*.pivotal \\ \"$OPTIONAL_CONFIG_FLAG\" upload-stemcell Uploads a stemcell to Ops Manager. Note that the filename of the stemcell must be exactly as downloaded from Pivnet. Ops Manager parses this filename to determine the version and OS of the stemcell. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 --- platform : linux inputs : - name : env # contains the env file with target OpsMan Information - name : stemcell # contains the stemcell tarball # - The stemcell filename is important and must be preserved. # if using the bosh.io concourse resource, # set `params.preserve_filename: true` on your GET. params : CONFIG_FILE : # - Optional # - Path to the config file for the product # - Relative to the root of the `config` input # - If empty, no config will be used; version and sha256 will not be checked # - Example config: # --- # shasum: 6daededd8fb4c341d0cd437a669d732d2fde62cb89716498e6b16f34607a1498 ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input FLOATING_STEMCELL : true # - Optional # - Assigns the stemcell to all compatible products # - If false, a user is required to run the assign-stemcell task run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux export OPTIONAL_CONFIG_FLAG=\"\" if [ -n \"$CONFIG_FILE\" ]; then export OPTIONAL_CONFIG_FLAG=\"--config config/$CONFIG_FILE\" fi om --env env/\"${ENV_FILE}\" upload-stemcell \\ --floating=\"$FLOATING_STEMCELL\" \\ --stemcell \"$PWD\"/stemcell/*.tgz \\ \"$OPTIONAL_CONFIG_FLAG\"","title":"Tasks"},{"location":"reference/task.html#platform-automation-for-pcf-tasks","text":"This document lists each Platform Automation for PCF task, and provides information about their intentions, inputs, and outputs. The tasks are presented, in their entirety, as they are found in the product. The docker image can be used to invoke the commands in each task locally. Use --help for more information. To learn more see the running-commands-locally section.","title":"Platform Automation for PCF Tasks"},{"location":"reference/task.html#apply-changes","text":"Triggers an install on the Ops Manager described by the auth file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 --- platform : linux inputs : - name : env # contains the env file with target OpsMan Information params : ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux om --env env/\"${ENV_FILE}\" apply-changes","title":"apply-changes"},{"location":"reference/task.html#apply-director-changes","text":"apply-changes can also be used to trigger an install for just the BOSH Director with the --skip-deploy-products / -sdp flag. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 --- platform : linux inputs : - name : env # contains the env file with target OpsMan Information params : ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux om --env env/\"${ENV_FILE}\" apply-changes \\ --skip-deploy-products","title":"apply-director-changes"},{"location":"reference/task.html#assign-multi-stemcell","text":"assign-multi-stemcell assigns multiple stemcells to a provided product. This feature is only available in OpsMan 2.6+. For more information on how to utilize this workflow, check out the Stemcell Handling topic. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # This feature is only available in OpsMan 2.6+. --- platform : linux inputs : - name : env # contains the env file with target OpsMan Information - name : config # contains the configuration file for assign-multi-stemcell command # - Example config: # --- # product: cf # stemcell: # - ubuntu-trusty:1234.6 # - ubuntu-xenial:latest params : ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input CONFIG_FILE : config.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `assign-multi-stemcell-config` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux om --env env/\"${ENV_FILE}\" assign-multi-stemcell \\ --config config/\"$CONFIG_FILE\"","title":"assign-multi-stemcell"},{"location":"reference/task.html#assign-stemcell","text":"assign-stemcell assigns a stemcell to a provided product. For more information on how to utilize this workflow, check out the Stemcell Handling topic. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 --- platform : linux inputs : - name : env # contains the env file with target OpsMan Information - name : config # contains the configuration file for assign-stemcell command # - Can consume the output of `download-product` task directly # - Example config: # --- # product: cf # stemcell: 3468.86 params : ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input CONFIG_FILE : config.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `assign-stemcell-config` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux om --env env/\"${ENV_FILE}\" assign-stemcell \\ --config config/\"$CONFIG_FILE\"","title":"assign-stemcell"},{"location":"reference/task.html#configure-authentication","text":"Configures Ops Manager with an internal userstore and admin user account. See configure-saml-authentication to configure an external SAML user store, and configure-ldap-authentication to configure with LDAP. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 --- platform : linux inputs : - name : env # contains the env file with target OpsMan Information - name : config # contains the auth configuration params : ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input AUTH_CONFIG_FILE : auth.yml # - Required # - Filepath of the authorization config YAML # - The path is relative to root of the `config` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux om --env env/\"${ENV_FILE}\" configure-authentication \\ --config config/\"${AUTH_CONFIG_FILE}\" For details on the config file expected in the config input, please see Generating an Auth File .","title":"configure-authentication"},{"location":"reference/task.html#configure-director","text":"Configures the BOSH Director with settings from a config file. See staged-director-config , which can extract a config file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 --- platform : linux inputs : - name : config # contains the director configuration file - name : env # contains the env file with target OpsMan Information - name : vars # variable files to be made available optional : true - name : secrets # secret files to be made available # separate from vars, so they can be store securely optional : true - name : ops-files # operations files to custom configure the product optional : true params : VARS_FILES : # - Optional # - Filepath to the Ops Manager vars yaml file # - The path is relative to root of the task build, # so `vars` and `secrets` can be used. OPS_FILES : # - Optional # - Filepath to the Ops Manager operations yaml files # - The path is relative to root of the task build ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input DIRECTOR_CONFIG_FILE : director.yml # - Required # - Filepath to the director configuration yaml file # - The path is relative to the root of the `config` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux vars_files_args=(\"\") for vf in ${VARS_FILES} do vars_files_args+=(\"--vars-file ${vf}\") done ops_files_args=(\"\") for of in ${OPS_FILES} do ops_files_args+=(\"--ops-file ${of}\") done # ${vars_files_args[@] needs to be globbed to pass through properly # ${ops_files_args[@] needs to be globbed to pass through properly # shellcheck disable=SC2068 om --env env/\"${ENV_FILE}\" configure-director \\ --config \"config/${DIRECTOR_CONFIG_FILE}\" \\ ${vars_files_args[@]} \\ ${ops_files_args[@]} GCP with service account For GCP, if service account is used, the property associated_service_account has to be set explicitly in the iaas_configuration section.","title":"configure-director"},{"location":"reference/task.html#configure-ldap-authentication","text":"Configures Ops Manager with an external LDAP user store and admin user account. See configure-authentication to configure an internal user store, and configure-saml-authentication to configure with SAML. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 --- platform : linux inputs : - name : env # contains the env file with target OpsMan Information - name : config # contains the auth configuration params : ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input AUTH_CONFIG_FILE : auth.yml # - Required # - Filepath of the authorization config YAML # - The path is relative to root of the `config` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux om --env env/\"${ENV_FILE}\" configure-ldap-authentication \\ --config config/\"${AUTH_CONFIG_FILE}\" For more details on using LDAP, please refer to the Ops Manager documentation . For details on the config file expected in the config input, please see Generating an Auth File .","title":"configure-ldap-authentication"},{"location":"reference/task.html#configure-product","text":"Configures an individual, staged product with settings from a config file. Not to be confused with Ops Manager's built-in import , which reads all deployed products and configurations from a single opaque file, intended for import as part of backup/restore and upgrade lifecycle processes. See staged-config , which can extract a config file, and upload-and-stage-product , which can stage a product that's been uploaded. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 --- platform : linux inputs : - name : config # contains the product configuration file - name : env # contains the env file with target OpsMan Information - name : vars # variable files to be made available optional : true - name : secrets # secret files to be made available # separate from vars, so they can be store securely optional : true - name : ops-files # operations files to custom configure the product optional : true params : CONFIG_FILE : # - Required # - Filepath to the product configuration yaml file # - The path is relative to the root of the `config` input VARS_FILES : # - Optional # - Filepath to the product configuration vars yaml file # - The path is relative to root of the task build, # so `vars` and `secrets` can be used. OPS_FILES : # - Optional # - Filepath to the product configuration operations yaml files # - The path is relative to root of the task build ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux vars_files_args=(\"\") for vf in ${VARS_FILES} do vars_files_args+=(\"--vars-file ${vf}\") done ops_files_args=(\"\") for of in ${OPS_FILES} do ops_files_args+=(\"--ops-file ${of}\") done # ${vars_files_args[@] needs to be globbed to pass through properly # ${ops_files_args[@] needs to be globbed to pass through properly # shellcheck disable=SC2068 om --env env/\"${ENV_FILE}\" configure-product \\ --config \"config/${CONFIG_FILE}\" \\ ${vars_files_args[@]} \\ ${ops_files_args[@]}","title":"configure-product"},{"location":"reference/task.html#configure-saml-authentication","text":"Configures Ops Manager with an external SAML user store and admin user account. See configure-authentication to configure an internal user store, and configure-ldap-authentication to configure with LDAP. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 --- platform : linux inputs : - name : env # contains the env file with target OpsMan Information - name : config # contains the auth configuration params : ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input AUTH_CONFIG_FILE : auth.yml # - Required # - Filepath of the authorization config YAML # - The path is relative to root of the `config` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux om --env env/\"${ENV_FILE}\" configure-saml-authentication \\ --config config/\"${AUTH_CONFIG_FILE}\" Configuring SAML has two different auth flows for the UI and the task. The UI will have a browser based login flow. The CLI will require client-id and client-secret as it cannot do a browser login flow. For more details on using SAML, please refer to the Ops Manager documentation For details on the config file expected in the config input, please see Generating an Auth File .","title":"configure-saml-authentication"},{"location":"reference/task.html#create-vm","text":"Creates an unconfigured Ops Manager VM. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 --- platform : linux inputs : - name : state # contains the state for the vm - name : config # contains the product configuration file - name : image # contains the image file to be installed - name : vars # variable files to be made available optional : true - name : secrets # secret files to be made available # separate from vars, so they can be store securely optional : true outputs : - name : generated-state #contains the updated state file params : VARS_FILES : # - Optional # - Filepath to the Ops Manager vars yaml file # - The path is relative to root of the task build, # so `vars` and `secrets` can be used. OPSMAN_CONFIG_FILE : opsman.yml # - Required # - Filepath of the opsman config YAML # - The path is relative to root of the `config` input STATE_FILE : state.yml # - Required # - Filepath of the state yaml file # - The path is relative to root of the `state` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux vars_files_args=(\"\") for vf in ${VARS_FILES} do vars_files_args+=(\"--vars-file ${vf}\") done generated_state_path=\"generated-state/$(basename \"$STATE_FILE\")\" if [ -e \"state/$STATE_FILE\" ]; then cp \"state/$STATE_FILE\" \"$generated_state_path\" fi export IMAGE_FILE IMAGE_FILE=\"$(find image/*.{yml,ova,raw} 2>/dev/null | head -n1)\" if [ -z \"$IMAGE_FILE\" ]; then echo \"No image file found in image input.\" echo \"Contents of image input:\" ls -al image exit 1 fi # ${vars_files_args[@] needs to be globbed to split properly # shellcheck disable=SC2068 p-automator create-vm \\ --config config/\"${OPSMAN_CONFIG_FILE}\" \\ --image-file \"${IMAGE_FILE}\" \\ --state-file \"$generated_state_path\" \\ ${vars_files_args[@]} This task requires a config file specific to the IaaS being deployed to. Please see the configuration page for more specific examples. The task does specific CLI commands for the creation of the Ops Manager VM on each IAAS. See below for more information: AWS Requires the image YAML file from Pivnet Validates the existence of the VM if defined in the statefile, if so do nothing Chooses the correct ami to use based on the provided image YAML file from Pivnet Creates the vm configured via opsman config and the image YAML. This only attaches existing infrastructure to a newly created VM. This does not create any new resources The public IP address, if provided, is assigned after successful creation Azure Requires the image YAML file from Pivnet Validates the existence of the VM if defined in the statefile, if so do nothing Copies the image (of the OpsMan VM from the specified region) as a blob into the specified storage account Creates the Ops Manager image Creates a VM from the image. This will use unmanaged disk (if specified), and assign a public and/or private IP. This only attaches existing infrastructure to a newly createdVM. This does not create any new resources. GCP Requires the image YAML file from Pivnet Validates the existence of the VM if defined in the statefile, if so do nothing Creates a compute image based on the region specific Ops Manager source URI in the specified Ops Manager account Creates a VM from the image. This will assign a public and/or private IP address, VM sizing, and tags. This does not create any new resources. Openstack Requires the image YAML file from Pivnet Validates the existence of the VM if defined in the statefile, if so do nothing Recreates the image in openstack if it already exists to validate we are using the correct version of the image Creates a VM from the image. This does not create any new resources The public IP address, if provided, is assigned after successful creation Vsphere Requires the OVA image from Pivnet Validates the existence of the VM if defined in the statefile, if so do nothing Build ipath from the provided datacenter, folder, and vmname provided in the config file. The created VM is stored on the generated path. If folder is not provided, the vm will be placed in the datacenter. Creates a VM from the image provided to the create-vm command. This does not create any new resources","title":"create-vm"},{"location":"reference/task.html#credhub-interpolate","text":"Interpolate credhub entries into configuration files 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 --- platform : linux inputs : - name : files # contains YAML files with extension `.yml`. # Each one of these files will have their values interpolated from credhub. # For examples, run: `credhub interpolate --help` # (minimum version >= 2.1.0 required) outputs : - name : interpolated-files # Contains only yaml files found and interpolated by this task. # Maintains the filestructure of the `files` input. # all params are required to be filled out params : CREDHUB_CLIENT : CREDHUB_SECRET : CREDHUB_SERVER : # - Required # - Credentials to talk to credhub server CREDHUB_CA_CERT : # - Optional # - This is only necessary if your Concourse worker # is not already configured to trust the CA used for Credhub PREFIX : # - Required # - Prefix flag used by credhub interpolate INTERPOLATION_PATHS : '.' # - Required # - Path the contains the files to read from # - This is a space separated list of directories # the paths are all evaluated relative to files/ SKIP_MISSING : true # Optional # Change to false to have strict interpolation # and fail if params are missing from vars run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -euo pipefail # NOTE: The credhub cli does not ignore empty/null environment variables. # https://github.com/cloudfoundry-incubator/credhub-cli/issues/68 if [ -z \"$CREDHUB_CA_CERT\" ]; then unset CREDHUB_CA_CERT fi credhub --version if [ -z \"$PREFIX\" ]; then echo \"Please specify a PREFIX. It is required.\" exit 1 fi # $INTERPOLATION_PATHS needs to be globbed to read multiple files # shellcheck disable=SC2086 files=$(cd files && find $INTERPOLATION_PATHS -type f -name '*.yml' -follow) if [ \"$SKIP_MISSING\" ]; then export SKIP_MISSING=\"--skip-missing\" else export SKIP_MISSING=\"\" fi for file in $files; do echo \"interpolating files/$file\" mkdir -p interpolated-files/\"$(dirname \"$file\")\" credhub interpolate --prefix \"$PREFIX\" \\ --file files/\"$file\" \"$SKIP_MISSING\" \\ > interpolated-files/\"$file\" done This task requires a valid credhub with UAA client and secret. For information on how to set this up, see Secrets Handling","title":"credhub-interpolate"},{"location":"reference/task.html#delete-installation","text":"Delete the Ops Manager Installation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 --- platform : linux inputs : - name : env # contains the env file with target OpsMan Information params : ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux om --env env/\"${ENV_FILE}\" delete-installation --force","title":"delete-installation"},{"location":"reference/task.html#delete-vm","text":"Deletes the Ops Manager VM instantiated by create-vm . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 --- platform : linux inputs : - name : state # contains the state for the vm - name : config # contains the product configuration file - name : vars # variable files to be made available optional : true - name : secrets # secret files to be made available # separate from vars, so they can be store securely optional : true outputs : - name : generated-state #contains the updated state file params : VARS_FILES : # - Optional # - Filepath to the Ops Manager vars yaml file # - The path is relative to root of the task build, # so `vars` and `secrets` can be used. STATE_FILE : state.yml # - Required # - Filepath of the state yaml file # - The path is relative to root of the `state` input OPSMAN_CONFIG_FILE : opsman.yml # - Required # - Filepath of the opsman config YAML # - The path is relative to root of the `config` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux vars_files_args=(\"\") for vf in ${VARS_FILES} do vars_files_args+=(\"--vars-file ${vf}\") done generated_state_path=\"generated-state/$(basename \"$STATE_FILE\")\" cp \"state/$STATE_FILE\" \"$generated_state_path\" # ${vars_files_args[@] needs to be globbed to split properly # shellcheck disable=SC2068 p-automator delete-vm \\ --state-file \"$generated_state_path\" \\ --config config/\"$OPSMAN_CONFIG_FILE\" \\ ${vars_files_args[@]} This task requires the state file generated create-vm . The task does specific CLI commands for the deletion of the Ops Manager VM and resources on each IAAS. See below for more information: AWS Deletes the VM Azure Deletes the VM Attempts to delete the associated disk Attempts to delete the associated nic Attempts to delete the associated image GCP Deletes the VM Attempts to delete the associated image Openstack Deletes the VM Attempts to delete the associated image vSphere Deletes the VM","title":"delete-vm"},{"location":"reference/task.html#download-product","text":"Ops Manager 2.5 The filename for the artifact downloaded from Ops Manager is changed! If your resources or pipelines have a regex for the Ops Manager filename, you may be affected. (Please see Ops Manager's official notice for more information) Downloads a product specified in a config file from Pivnet. Optionally, also downloads the latest stemcell for that product. Downloads are cached, so files are not re-downloaded each time. Outputs can be persisted to an S3-compatible blobstore using a put to an appropriate resource for later use with the download-product-s3 , or used directly as inputs to upload-and-stage-product and upload-stemcell tasks. This task requires a download-product config file . If S3 configuration is present in the download-product config file , the slug and version of the downloaded product file will be prepended in brackets to the filename. For example: original-pivnet-filenames: 1 2 ops - manager - aws - 2 . 5 . 0 - build . 123 . yml cf - 2 . 5 . 0 - build . 45 . pivotal download-product-filenames if S3 configuration is present: 1 2 [ ops - manager , 2 . 5 . 0 ] ops - manager - aws - 2 . 5 . 0 - build . 123 . yml [ elastic - runtime , 2 . 5 . 0 ] cf - 2 . 5 . 0 - build . 45 . pivotal This is to allow the same config parameters that let us select a file from Pivnet select it again when pulling from S3. Note that the filename will be unchanged if S3 keys are not present in the configuration file. This avoids breaking current pipelines. When using the s3 resource in concourse If you are using a regexp in your s3 resource definition that explicitly requires the pivnet filename to be the start of the regex, (i.e., the pattern starts with ^ ) this won't work when using S3 config. The new file format preserves the original filename, so it is still possible to match on that - but if you need to match from the beginning of the filename, that will have been replaced by the prefix described above. When specifying PAS-Windows This task will automatically download and inject the winfs for pas-windows. When specifying PAS-Windows on Vsphere This task cannot download the stemcell for pas-windows on vSphere. To build this stemcell manually, please reference the Creating a vSphere Windows Stemcell guide in Pivotal Documentation. When only downloading from Pivnet When the download product config only has Pivnet credentials, it will not add the prefix to the downloaded product. For example, example-product.pivotal from Pivnet will be outputed as example-product.pivotal . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 --- platform : linux inputs : - name : config # contains download-file config file - name : vars # variable files to be made available optional : true - name : secrets # secret files to be made available # separate from vars, so they can be store securely optional : true outputs : - name : downloaded-product - name : downloaded-stemcell - name : assign-stemcell-config caches : - path : downloaded-files params : CONFIG_FILE : download-config.yml # - Required # - Filepath to the product configuration yaml file # - The path is relative to the root of the `config` input VARS_FILES : # - Optional # - Filepath to the product configuration vars yaml file # - The path is relative to root of the task build, # so `vars` and `secrets` can be used. run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux vars_files_args=(\"\") for vf in ${VARS_FILES} do vars_files_args+=(\"--vars-file ${vf}\") done # ${vars_files_args[@] needs to be globbed to pass through properly # shellcheck disable=SC2068 om download-product \\ --config config/\"${CONFIG_FILE}\" ${vars_files_args[@]} \\ --output-directory downloaded-files { printf \"\\nReading product details...\"; } 2> /dev/null # shellcheck disable=SC2068 product_slug=$(om interpolate \\ --config config/\"${CONFIG_FILE}\" ${vars_files_args[@]} \\ --path /pivnet-product-slug) product_file=$(om interpolate \\ --config downloaded-files/download-file.json \\ --path /product_path) stemcell_file=$(om interpolate \\ --config downloaded-files/download-file.json \\ --path /stemcell_path?) { printf \"\\nChecking if product needs winfs injected...\"; } 2> /dev/null if [ \"$product_slug\" == \"pas-windows\" ]; then TILE_FILENAME=\"$(basename \"$product_file\")\" # The winfs-injector determines the necessary windows image, # and uses the CF-foundation dockerhub repo # to pull the appropriate Microsoft-hosted foreign layer. winfs-injector \\ --input-tile \"$product_file\" \\ --output-tile \"downloaded-product/${TILE_FILENAME}\" else cp \"$product_file\" downloaded-product fi if [ -e \"$stemcell_file\" ]; then cp \"$stemcell_file\" downloaded-stemcell fi if [ -e downloaded-files/assign-stemcell.yml ]; then cp downloaded-files/assign-stemcell.yml assign-stemcell-config/config.yml fi","title":"download-product"},{"location":"reference/task.html#download-product-s3","text":"Downloads a product specified in a config file from an S3-compatible blobstore. This is useful when retrieving assets in an offline environment. Downloads are cached, so files are not re-downloaded each time. This is intended to be used with files downloaded from Pivnet by download-product and then persisted to a blobstore using a put step. Outputs can be used directly as an input to upload-and-stage-product and upload-stemcell tasks. This task requires a download-product config file . The same configuration file should be used with both this task and download-product . This ensures that the same file is being captured with both tasks. The product files uploaded to s3 for download with this task require a specific prefix: [product-slug,semantic-version] . This prefix is added by the download-product task when S3 keys are present in the configuration file. This is the meta information about the product from Pivnet, which is not guaranteed to be in the original filename. This tasks uses the meta information to be able to perform consistent downloads from s3 as defined in the provided download config. For example: original-pivnet-filenames: 1 2 ops - manager - aws - 2 . 5 . 0 - build . 123 . yml cf - 2 . 5 . 0 - build . 45 . pivotal filenames expected by download-product-s3 in a bucket: 1 2 [ ops - manager , 2 . 5 . 0 ] ops - manager - aws - 2 . 5 . 0 - build . 123 . yml [ elastic - runtime , 2 . 5 . 0 ] cf - 2 . 5 . 0 - build . 45 . pivotal When only downloading from Pivnet When the download product config only has Pivnet credentials, it will not add the prefix to the downloaded product. For example, example-product.pivotal from Pivnet will be outputed as example-product.pivotal . Info It's possible to use IAM instance credentials instead of providing S3 creds in the config file. See download-product config file for details. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 --- platform : linux inputs : - name : config # contains download-file config file - name : vars # variable files to be made available optional : true - name : secrets # secret files to be made available # separate from vars, so they can be store securely optional : true outputs : - name : downloaded-product - name : downloaded-stemcell - name : assign-stemcell-config caches : - path : downloaded-files params : CONFIG_FILE : download-config.yml # - Required # - Filepath to the product configuration yaml file # - The path is relative to the root of the `config` input VARS_FILES : # - Optional # - Filepath to the product configuration vars yaml file # - The path is relative to root of the task build, # so `vars` and `secrets` can be used. run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux vars_files_args=(\"\") for vf in ${VARS_FILES} do vars_files_args+=(\"--vars-file ${vf}\") done # ${vars_files_args[@] needs to be globbed to pass through properly # shellcheck disable=SC2068 om download-product \\ --config config/\"${CONFIG_FILE}\" ${vars_files_args[@]} \\ --output-directory downloaded-files \\ --source s3 product_file=$(om interpolate \\ --config downloaded-files/download-file.json \\ --path /product_path) stemcell_file=$(om interpolate \\ --config downloaded-files/download-file.json \\ --path /stemcell_path?) cp \"$product_file\" downloaded-product if [ -e \"$stemcell_file\" ]; then cp \"$stemcell_file\" downloaded-stemcell fi if [ -e downloaded-files/assign-stemcell.yml ]; then cp downloaded-files/assign-stemcell.yml assign-stemcell-config/config.yml fi","title":"download-product-s3"},{"location":"reference/task.html#export-installation","text":"Exports an existing Ops Manager to a file. This is the first part of the backup/restore and upgrade lifecycle processes. This task is used on a fully installed and healthy Ops Manager to export settings to an upgraded version of Ops Manager. To use with non-versioned blobstore, you can override INSTALLATION_FILE param to include $timestamp , then the generated installation file will include a sortable timestamp in the filename. example: 1 2 params : INSTALLATION_FILE : installation-$timestamp.zip Info The timestamp is generated using the time on concourse worker. If the time is different on different workers, the generated timestamp may fail to sort correctly. Changing the time or timezone on workers might interfere with ordering. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 --- platform : linux inputs : - name : env # contains the env file with target OpsMan Information outputs : - name : installation # will contain the exported installation params : ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input INSTALLATION_FILE : installation-$timestamp.zip # - Required # - Filepath of the installation ZIP file # - The path is relative to root of the `installation` output # - if the filename includes \"$timestamp\", # for example \"installation-$timestamp.zip\", # the final filename will include the current timestamp. # - this is necessary if using an \"S3 compatible\" blobstore # that doesn't support versioned blobs # - timestamped filenames will need to be represented # with a glob-style wildcard in the `upgrade-opsman` task configuration # (the default will work with the example provided above). run : path : bash args : - -c - | cat /var/version && echo \"\" set -eux timestamp=\"$(date '+%Y%m%d.%-H%M.%S+%Z')\" export timestamp # '$timestamp' must be a literal, because envsubst uses it as a filter # this allows us to avoid accidentally interpolating anything else. # shellcheck disable=SC2016 OUTPUT_FILE_NAME=\"$(echo \"$INSTALLATION_FILE\" | envsubst '$timestamp')\" om --env env/\"${ENV_FILE}\" export-installation \\ --output-file installation/\"$OUTPUT_FILE_NAME\" Warning It is recommended to persist the zip file exported from export-installation to an external file store (eg S3) on a regular basis. The exported installation can restore the Ops Manager to a working state if it is non-functional.","title":"export-installation"},{"location":"reference/task.html#import-installation","text":"Imports a previously exported installation to Ops Manager. This is a part of the backup/restore and upgrade lifecycle processes. This task is used after an installation has been exported and a new Ops Manager has been deployed, but before the new Ops Manager is configured. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 --- platform : linux inputs : - name : env # contains the environment information about the OpsMan - name : installation # contains the installation to be imported params : ENV_FILE : env.yml # - Required # - Filepath of the environment config YAML # - The path is relative to root of the `env` input # - The env file _must_ contain the `decryption-passphrase` # while it's optional for other tasks, this one requires it. INSTALLATION_FILE : installation*.zip # - Required # - Filepath of the installation ZIP file # - The filepath provided can be wildcard expanded. # - The path is relative to root of the `installation` input run : path : bash args : - -c - | cat /var/version && echo \"\" set -eux # INSTALLATION_FILE needs to be globbed # shellcheck disable=SC2086 om --env env/\"${ENV_FILE}\" import-installation \\ --installation installation/$INSTALLATION_FILE","title":"import-installation"},{"location":"reference/task.html#make-git-commit","text":"Copies a single file into a repo and makes a commit. Useful for persisting the state output of tasks that manage the vm, such as: create-vm upgrade-opsman delete-vm Also useful for persisting the configuration output from: staged-config staged-director-config Info This commits all changes present in the repo used for the repository input, in addition to copying in a single file. Info This does not perform a git push ! You will need to put the output of this task to a git resource to persist it. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 --- platform : linux inputs : - name : repository # - This must be an initialized git repository. # - Note that any changes present in this input # will be committed along with the file copied in # by this task. - name : file-source # - This is the input folder containing the file to be committed. # Typically, this will from some other task # with an output that needs to be persisted. outputs : - name : repository-commit params : FILE_SOURCE_PATH : # - Required # - Filepath to be copied into the git repo # before a commit is created # - Relative to the root of the `file-source` input FILE_DESTINATION_PATH : # - Required # - Filepath to write the file specified by FILE_SOURCE_PATH # - Relative to the root of the `repository` input GIT_AUTHOR_NAME : # - Required # - Used to configure the human-readable # name in the `author` field of the commit GIT_AUTHOR_EMAIL : # - Required # - Used to configure the email address # in the `author` field of the commit COMMIT_MESSAGE : # - Required # - Specify a commit message to be used # for all commits made by this task. run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eu git config --global user.email \"$GIT_AUTHOR_EMAIL\" git config --global user.name \"$GIT_AUTHOR_NAME\" git clone repository repository-commit if [ ! -d \"$(dirname \"$FILE_DESTINATION_PATH\")\" ]; then echo \"Directory $(dirname \"$FILE_DESTINATION_PATH\") does not exist in repository, creating it...\" mkdir -p \"$(dirname \"$FILE_DESTINATION_PATH\")\"; fi; cp file-source/\"$FILE_SOURCE_PATH\" \\ repository-commit/\"$FILE_DESTINATION_PATH\" cd repository-commit git add -A git commit -m \"$COMMIT_MESSAGE\" --allow-empty","title":"make-git-commit"},{"location":"reference/task.html#pre-deploy-check","text":"Checks if the Ops Manager director is configured properly and validates the configuration. Additionally, checks each of the staged products and validates they are configured correctly. This task can be run at any time and can be used a a pre-check for apply-changes . The checks that this task executes are: is configuration complete and valid is the network assigned is the availability zone assigned is the stemcell assigned what stemcell type/version is required are there any unset/invalid properties did any ops manager verifiers fail If any of the above checks fail the task will fail. The failed task will provide a list of errors that need to be fixed before an apply-changes could start. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 --- platform : linux inputs : - name : env # contains the env file with target OpsMan Information params : ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux om --env env/\"${ENV_FILE}\" pre-deploy-check","title":"pre-deploy-check"},{"location":"reference/task.html#stage-product","text":"Staged a product to the Ops Manager specified in the config file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 --- platform : linux inputs : - name : product # contains the product file to be uploaded and staged - name : env # contains the env file with target OpsMan Information params : ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux product_name=\"$(om tile-metadata \\ --product-path product/*.pivotal \\ --product-name)\" product_version=\"$(om tile-metadata \\ --product-path product/*.pivotal \\ --product-version)\" om --env env/\"${ENV_FILE}\" stage-product \\ --product-name \"$product_name\" \\ --product-version \"$product_version\"","title":"stage-product"},{"location":"reference/task.html#staged-config","text":"Downloads the configuration for a product from Ops Manager. Not to be confused with Ops Manager's built-in export , which puts all deployed products and configurations into a single file, intended for import as part of backup/restore and upgrade lifecycle processes. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 --- platform : linux inputs : - name : env # contains the env file with target OpsMan Information outputs : - name : generated-config # will contain the staged product config params : PRODUCT_NAME : # - Required # - The name of the product config to be exported ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input SUBSTITUTE_CREDENTIALS_WITH_PLACEHOLDERS : true # - Optional # - Replace credentials with interpolatable variable names # - If set to false, **literal credentials** will be included in the output run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux flag=$(if \"$SUBSTITUTE_CREDENTIALS_WITH_PLACEHOLDERS\"; then echo '--include-placeholders'; else echo '--include-credentials'; fi ) om --env env/\"${ENV_FILE}\" staged-config \\ --product-name \"$PRODUCT_NAME\" \\ \"$flag\" > generated-config/\"$PRODUCT_NAME\".yml","title":"staged-config"},{"location":"reference/task.html#staged-director-config","text":"Ops Manager 2.5 The filename for the artifact downloaded from Ops Manager is changed! If your resources or pipelines have a regex for the Ops Manager filename, you may be affected. (Please see Ops Manager's official notice for more information) Downloads configuration for the BOSH director from Ops Manager. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 --- platform : linux inputs : - name : env # contains the env file with target OpsMan Information outputs : - name : generated-config # will contain the staged product config params : ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input run : path : bash args : - -c - | cat /var/version && echo \"\" set -eux om --env env/\"${ENV_FILE}\" staged-director-config \\ --include-placeholders > generated-config/director.yml The configuration is exported to the generated-config output. It does not extract credentials from Ops Manager and replaced them all with YAML interpolation (()) placeholders. This is to ensure that credentials are never written to disk. The credentials need to be provided from an external configuration when invoking configure-director . Info staged-director-config will not be able to grab all sensitive fields in your Ops Manager installation (for example: vcenter_username and vcenter_password if using vsphere). To find these missing fields, please refer to the Ops Manager API Documentation","title":"staged-director-config"},{"location":"reference/task.html#test","text":"An example task to ensure the assets and docker image are setup correctly in your concourse pipeline. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 --- platform : linux run : path : bash args : - \"-c\" - | echo \"Platform Automation for PCF version:\" cat /var/version && echo \"\" printf \"\\\\np-automator version:\" p-automator -v printf \"\\\\nom version:\" om -v set -eux p-automator --help om --help { echo \"Successfully validated tasks and image!\"; } 2> /dev/null","title":"test"},{"location":"reference/task.html#test-interpolate","text":"An example task to ensure that all required vars are present when interpolating into a base file. For more instruction on this topic, see the variables section 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 --- platform : linux inputs : - name : config # contains the base configuration file - name : vars # variable files to be made available optional : true params : VARS_FILES : # - Optional # - Filepath to the vars yaml file # - The path is relative to root of the task build, # so `vars` and `secrets` can be used. CONFIG_FILE : base.yml # - Required # - Filepath to the base yaml file to interpolate from # - The path is relative to root of the task build SKIP_MISSING : true # - Optional # - Change to false to have strict interpolation # and fail if params are missing from vars run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux vars_files_args=(\"\") for vf in ${VARS_FILES} do vars_files_args+=(\"--vars-file ${vf}\") done if [ \"$SKIP_MISSING\" ]; then export SKIP_MISSING=\"--skip-missing\" else export SKIP_MISSING=\"\" fi # ${vars_files_args[@] needs to be globbed to pass through properly # shellcheck disable=SC2068 om interpolate --config \"config/$CONFIG_FILE\" \"$SKIP_MISSING\" ${vars_files_args[@]}","title":"test-interpolate"},{"location":"reference/task.html#upgrade-opsman","text":"Upgrades an existing Ops Manager to a new given Ops Manager version 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 --- platform : linux inputs : - name : state # contains the state for the vm - name : config # contains the OpsMan configuration file - name : image # contains the image file to be installed - name : installation # contains the installation to be imported - name : env # contains the environment information for OpsMan - name : vars # variable files to be made available optional : true - name : secrets # secret files to be made available # separate from vars, so they can be stored securely optional : true outputs : - name : generated-state #contains the updated state file params : VARS_FILES : # - Optional # - space-seperated array of filepaths to YAML vars files # to be loaded with the OPSMAN_CONFIG_FILE # - relative to root of the task build, # so both `vars` and `secrets` can be used. ENV_FILE : env.yml # - Required # - filepath of the env config YAML # - relative to root of the `env` input OPSMAN_CONFIG_FILE : opsman.yml # - Required # - filepath of the opsman config YAML # - relative to root of the `config` input STATE_FILE : state.yml # - Required # - filepath of the state YAML file # - relative to root of the `state` input INSTALLATION_FILE : installation*.zip # - Required # - filepath of the installation ZIP file # - can be wildcard expanded # - relative to root of the `installation` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" p-automator -v set -eux vars_files_args=(\"\") for vf in ${VARS_FILES} do vars_files_args+=(\"--vars-file ${vf}\") done generated_state_path=\"generated-state/$(basename \"$STATE_FILE\")\" cp \"state/$STATE_FILE\" \"$generated_state_path\" # ${vars_files_args[@] needs to be globbed to split properly (SC2068) # INSTALLATION_FILE needs to be globbed (SC2086) # shellcheck disable=SC2068,SC2086 p-automator upgrade-opsman \\ --config config/\"${OPSMAN_CONFIG_FILE}\" \\ --env-file env/\"${ENV_FILE}\" \\ --image-file \"$(find image/*.{yml,ova,raw} | head -n1)\" \\ --state-file \"$generated_state_path\" \\ --installation installation/$INSTALLATION_FILE \\ ${vars_files_args[@]} For more information about this task and how it works, see the upgrade page.","title":"upgrade-opsman"},{"location":"reference/task.html#upload-and-stage-product","text":"Uploads and stages product to the Ops Manager specified in the config file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 --- platform : linux inputs : - name : product # contains the product file to be uploaded and staged - name : env # contains the env file with target OpsMan Information - name : config # contains the product configuration file optional : true params : CONFIG_FILE : # - Optional # - Path to the config file for the product # - Relative to the root of the config input # - If empty, no config will be used; version and sha256 will not be checked # - Example config: # --- # product-version: 1.2.3-build.4 # sha256: 6daededd8fb4c341d0cd437a669d732d2fde62cb89716498e6b16f34607a1498 ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux if [ -z \"$CONFIG_FILE\" ]; then om --env env/\"${ENV_FILE}\" upload-product \\ --product product/*.pivotal else om --env env/\"${ENV_FILE}\" upload-product \\ --product product/*.pivotal --config \"config/$CONFIG_FILE\" fi product_name=\"$(om tile-metadata \\ --product-path product/*.pivotal \\ --product-name)\" product_version=\"$(om tile-metadata \\ --product-path product/*.pivotal \\ --product-version)\" om --env env/\"${ENV_FILE}\" stage-product \\ --product-name \"$product_name\" \\ --product-version \"$product_version\"","title":"upload-and-stage-product"},{"location":"reference/task.html#upload-product","text":"Uploads a product to the Ops Manager specified in the config file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 --- platform : linux inputs : - name : product # contains the product file to be uploaded and staged - name : env # contains the env file with target OpsMan Information - name : config # contains the product configuration file optional : true params : CONFIG_FILE : # - Optional # - Path to the config file for the product # - Relative to the root of the `config` input # - If empty, no config will be used; version and sha256 will not be checked # - Example config: # --- # product-version: 1.2.3-build.4 # shasum: 6daededd8fb4c341d0cd437a669d732d2fde62cb89716498e6b16f34607a1498 ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - Relative to root of the `env` input run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux export OPTIONAL_CONFIG_FLAG=\"\" if [ -n \"$CONFIG_FILE\" ]; then export OPTIONAL_CONFIG_FLAG=\"--config config/$CONFIG_FILE\" fi om --env env/\"${ENV_FILE}\" upload-product \\ --product product/*.pivotal \\ \"$OPTIONAL_CONFIG_FLAG\"","title":"upload-product"},{"location":"reference/task.html#upload-stemcell","text":"Uploads a stemcell to Ops Manager. Note that the filename of the stemcell must be exactly as downloaded from Pivnet. Ops Manager parses this filename to determine the version and OS of the stemcell. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 --- platform : linux inputs : - name : env # contains the env file with target OpsMan Information - name : stemcell # contains the stemcell tarball # - The stemcell filename is important and must be preserved. # if using the bosh.io concourse resource, # set `params.preserve_filename: true` on your GET. params : CONFIG_FILE : # - Optional # - Path to the config file for the product # - Relative to the root of the `config` input # - If empty, no config will be used; version and sha256 will not be checked # - Example config: # --- # shasum: 6daededd8fb4c341d0cd437a669d732d2fde62cb89716498e6b16f34607a1498 ENV_FILE : env.yml # - Required # - Filepath of the env config YAML # - The path is relative to root of the `env` input FLOATING_STEMCELL : true # - Optional # - Assigns the stemcell to all compatible products # - If false, a user is required to run the assign-stemcell task run : path : bash args : - \"-c\" - | cat /var/version && echo \"\" set -eux export OPTIONAL_CONFIG_FLAG=\"\" if [ -n \"$CONFIG_FILE\" ]; then export OPTIONAL_CONFIG_FLAG=\"--config config/$CONFIG_FILE\" fi om --env env/\"${ENV_FILE}\" upload-stemcell \\ --floating=\"$FLOATING_STEMCELL\" \\ --stemcell \"$PWD\"/stemcell/*.tgz \\ \"$OPTIONAL_CONFIG_FLAG\"","title":"upload-stemcell"}]}